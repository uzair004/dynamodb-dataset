[
  {
    "title": "Contributors - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/contributors.html",
    "html": "Contributors\nPDF\nRSS\n\nContributors to this guide include:\n\nCamilo Gonzalez, Senior Data Architect, AWS\n\nMoinul Al-Mamun, Senior Big Data Architect, AWS\n\nSantiago Segura, Professional Services Consultant, AWS\n\nSatheish Kumar Chandraprakasam, Cloud Application Architect, AWS"
  },
  {
    "title": "Step 7: Validate the data model - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step7-hierarchical-model.html",
    "html": "Step 7: Validate the data model\nPDF\nRSS\n\nIn this step, the business user validates the query results and checks whether they satisfy business needs. You can use the following table to check the access patterns against the requirements of the user.\n\nQuestion\n\n\t\n\nBase table / GSI\n\n\t\n\nQuery\n\n\n\n\nAs a user, I want to retrieve all the immediate child components for a parent component ID.\n\n\t\n\nGSI1\n\n\t\n\nParentId = \"<ComponentId>\"\n\n(Find immediate children of a component.)\n\n\n\n\nAs a user, I want to retrieve a recursive list of all child components for a component ID.\n\n\t\n\nGSI1 or GSI2\n\n\t\n\nGSI1: ParentId = \"<ComponentId>\"\n\nor\n\nGSI2: GraphId = \"<TopLevelComponentId>#N\" AND BEGINS_WITH(\"Path\", \"<PATH_OF_Component>\")\n\n(Find all down-level child components using a top- level component. Find all down-level child components using a middle-level component.)\n\n\n\n\nAs a user, I want to see the ancestors of a component.\n\n\t\n\nBase table\n\n\t\n\nComponentId = \"<ComponentId>\", then select the Path attribute.\n\n(Find ancestors of a component.)\n\nYou can also implement a script (test) in any programming language to query DynamoDB directly and compare the results with the expected results."
  },
  {
    "title": "Data-modeling process steps - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/steps.html",
    "html": "Data-modeling process steps\nPDF\nRSS\n\nThis section details each step of the recommended data modeling process for Amazon DynamoDB.\n\nTopics\nStep 1. Identify the use cases and logical data model\nStep 2. Create a preliminary cost estimation\nStep 3. Identify your data access patterns\nStep 4. Identify the technical requirements\nStep 5. Create the DynamoDB data model\nStep 6. Create the data queries\nStep 7. Validate the data model\nStep 8. Review the cost estimation\nStep 9. Deploy the data model"
  },
  {
    "title": "Document history - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/doc-history.html",
    "html": "Document history\nPDF\nRSS\n\nThe following table describes significant changes to this guide. If you want to be notified about future updates, you can subscribe to an RSS feed.\n\nChange\tDescription\tDate\n\n\nAdded a Best practices section and an example for hierarchical data modeling.\n\n\t\n\nWe added a summary of DynamoDB best practices and a step-by-step example of designing and validating a hierarchical model.\n\n\t\n\nDecember 5, 2023\n\n\n\n\nInitial publication\n\n\t\n\n—\n\n\t\n\nOctober 26, 2020"
  },
  {
    "title": "Step 2: Create a preliminary cost estimation - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step2-hierarchical-model.html",
    "html": "Step 2: Create a preliminary cost estimation\nPDF\nRSS\n\nIt's important to calculate an estimation of the cost for all environments of your application so you can check if the solution is financially viable. A best practice is to make a high-level estimation and get approval from the business analyst before proceeding with the development and deployment.\n\nDatabase engineer creates the initial cost analysis using available information and the examples presented on the DynamoDB pricing page.\n\nCreate a cost estimate for on-demand capacity (see example).\n\nCreate a cost estimate for provisioned capacity (see example).\n\nFor the provisioned capacity model, get the estimated cost from the calculator, and apply the discount for reserved capacity.\n\nCompare the estimated costs of the two capacity models.\n\nCreate an estimation for all the environments (Dev, Prod, QA).\n\nBusiness analyst reviews and approves or rejects the preliminary cost estimate.\n\nUsing these reference values, you can create an estimated price to submit for approval. To create the budget, you can use the DynamoDB pricing page and AWS Pricing Calculator."
  },
  {
    "title": "Example of hierarchical data modeling - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/hierarchical-data-modeling.html",
    "html": "Example of hierarchical data modeling\nPDF\nRSS\n\nThe following sections use an example automotive company to show how you can use the data modeling process steps to design a multi-level component-management system in DynamoDB.\n\nTopics\nStep 1: Identify the use cases and logical data model\nStep 2: Create a preliminary cost estimation\nStep 3: Identify your data-access patterns\nStep 4: Identify the technical requirements\nStep 5: Create a DynamoDB data model\nStep 6: Create data queries\nStep 7: Validate the data model\nStep 8: Review the cost estimation\nStep 9: Deploy the data model"
  },
  {
    "title": "Step 3. Identify your data access patterns - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step3.html",
    "html": "Step 3. Identify your data access patterns\nPDF\nRSS\n\nAccess patterns or query patterns define how the users and the system access the data to satisfy business needs.\n\nObjective\n\nDocument the data access patterns.\n\nProcess\n\nDatabase engineer and business analyst interview the end users to identify how data will be queried using the data-access patterns matrix template.\n\nFor new applications, they review user stories about activities and objectives. They document the use cases and analyze the access patterns that the use cases require.\n\nFor existing applications, they analyze query logs to find out how people are currently using the system and to identify the key access patterns.\n\nDatabase engineer identifies the following properties of the access patterns:\n\nData size: Knowing how much data will be stored and requested at one time helps determine the most effective way to partition the data (see blog post).\n\nData shape: Instead of reshaping data when a query is processed (as an RDBMS system does), a NoSQL database organizes data so that its shape in the database corresponds with what will be queried. This is a key factor in increasing speed and scalability.\n\nData velocity: DynamoDB scales by increasing the number of physical partitions that are available to process queries, and by efficiently distributing data across those partitions. Knowing the peak query loads in advance might help determine how to partition data to best use I/O capacity.\n\nBusiness user prioritizes the access or query patterns.\n\nPriority queries usually are the most used or most relevant queries. It is also important to identify queries that require lower response latency.\n\nTools and resources\n\nAccess patterns matrix (see template)\n\nChoosing the Right DynamoDB Partition Key (AWS Database blog)\n\nNoSQL design for DynamoDB (DynamoDB documentation)\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nC\n\n\t\n\nA\n\n\t\n\nI\n\n\t\n\nR\n\n\t\n\n\t\n\nOutputs\n\nData-access patterns matrix\n\nExample\n\nAccess pattern\n\n\t\n\nPriority\n\n\t\n\nRead or write\n\n\t\n\nDescription\n\n\t\n\nType (single item, multiple items, or all)\n\n\t\n\nKey attribute\n\n\t\n\nFilters\n\n\t\n\nResult ordering\n\n\n\n\nCreate user profile\n\n\t\n\nHigh\n\n\t\n\nWrite\n\n\t\n\nUser creates a new profile\n\n\t\n\nSingle item\n\n\t\n\nUsername\n\n\t\n\nN/A\n\n\t\n\nN/A\n\n\n\n\nUpdate user profile\n\n\t\n\nMedium\n\n\t\n\nWrite\n\n\t\n\nUser updates their profile\n\n\t\n\nSingle item\n\n\t\n\nUsername\n\n\t\n\nUsername = current user\n\n\t\n\nN/A"
  },
  {
    "title": "Step 1. Identify the use cases and logical data model - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step1.html",
    "html": "Step 1. Identify the use cases and logical data model\nPDF\nRSS\nObjectives\n\nGather the business needs and use cases that require a NoSQL database.\n\nDefine the logical data model by using an entity-relationship (ER) diagram.\n\nProcess\n\nBusiness analysts interview business users to identify the use cases and the expected outcomes.\n\nDatabase engineer creates the conceptual data model.\n\nDatabase engineer creates the logical data model.\n\nDatabase engineer gathers information about item size, data volume, and expected read and write throughput.\n\nTools and resources\n\nBusiness requirements assessment (see template)\n\nAccess patterns matrix (see template)\n\nYour preferred tool for creating diagrams\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nC\n\n\t\n\nR/A\n\n\t\n\nI\n\n\t\n\nR\n\n\t\n\n\t\n\nOutputs\n\nDocumented use cases and business requirements\n\nLogical data model (ER diagram)"
  },
  {
    "title": "AWS Prescriptive Guidance glossary - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/apg-gloss.html",
    "html": "AWS Prescriptive Guidance glossary\nPDF\nRSS\n\nThe following are commonly used terms in strategies, guides, and patterns provided by AWS Prescriptive Guidance. To suggest entries, please use the Provide feedback link at the end of the glossary.\n\nNumbers\n7 Rs\n\nSeven common migration strategies for moving applications to the cloud. These strategies build upon the 5 Rs that Gartner identified in 2011 and consist of the following:\n\nRefactor/re-architect – Move an application and modify its architecture by taking full advantage of cloud-native features to improve agility, performance, and scalability. This typically involves porting the operating system and database. Example: Migrate your on-premises Oracle database to the Amazon Aurora PostgreSQL-Compatible Edition.\n\nReplatform (lift and reshape) – Move an application to the cloud, and introduce some level of optimization to take advantage of cloud capabilities. Example: Migrate your on-premises Oracle database to Amazon Relational Database Service (Amazon RDS) for Oracle in the AWS Cloud.\n\nRepurchase (drop and shop) – Switch to a different product, typically by moving from a traditional license to a SaaS model. Example: Migrate your customer relationship management (CRM) system to Salesforce.com.\n\nRehost (lift and shift) – Move an application to the cloud without making any changes to take advantage of cloud capabilities. Example: Migrate your on-premises Oracle database to Oracle on an EC2 instance in the AWS Cloud.\n\nRelocate (hypervisor-level lift and shift) – Move infrastructure to the cloud without purchasing new hardware, rewriting applications, or modifying your existing operations. This migration scenario is specific to VMware Cloud on AWS, which supports virtual machine (VM) compatibility and workload portability between your on-premises environment and AWS. You can use the VMware Cloud Foundation technologies from your on-premises data centers when you migrate your infrastructure to VMware Cloud on AWS. Example: Relocate the hypervisor hosting your Oracle database to VMware Cloud on AWS.\n\nRetain (revisit) – Keep applications in your source environment. These might include applications that require major refactoring, and you want to postpone that work until a later time, and legacy applications that you want to retain, because there’s no business justification for migrating them.\n\nRetire – Decommission or remove applications that are no longer needed in your source environment.\n\nA\nABAC\n\nSee attribute-based access control.\n\nabstracted services\n\nSee managed services.\n\nACID\n\nSee atomicity, consistency, isolation, durability.\n\nactive-active migration\n\nA database migration method in which the source and target databases are kept in sync (by using a bidirectional replication tool or dual write operations), and both databases handle transactions from connecting applications during migration. This method supports migration in small, controlled batches instead of requiring a one-time cutover. It’s more flexible but requires more work than active-passive migration.\n\nactive-passive migration\n\nA database migration method in which in which the source and target databases are kept in sync, but only the source database handles transactions from connecting applications while data is replicated to the target database. The target database doesn’t accept any transactions during migration.\n\naggregate function\n\nA SQL function that operates on a group of rows and calculates a single return value for the group. Examples of aggregate functions include SUM and MAX.\n\nAI\n\nSee artificial intelligence.\n\nAIOps\n\nSee artificial intelligence operations.\n\nanonymization\n\nThe process of permanently deleting personal information in a dataset. Anonymization can help protect personal privacy. Anonymized data is no longer considered to be personal data.\n\nanti-pattern\n\nA frequently used solution for a recurring issue where the solution is counter-productive, ineffective, or less effective than an alternative.\n\napplication control\n\nA security approach that allows the use of only approved applications in order to help protect a system from malware.\n\napplication portfolio\n\nA collection of detailed information about each application used by an organization, including the cost to build and maintain the application, and its business value. This information is key to the portfolio discovery and analysis process and helps identify and prioritize the applications to be migrated, modernized, and optimized.\n\nartificial intelligence (AI)\n\nThe field of computer science that is dedicated to using computing technologies to perform cognitive functions that are typically associated with humans, such as learning, solving problems, and recognizing patterns. For more information, see What is Artificial Intelligence?\n\nartificial intelligence operations (AIOps)\n\nThe process of using machine learning techniques to solve operational problems, reduce operational incidents and human intervention, and increase service quality. For more information about how AIOps is used in the AWS migration strategy, see the operations integration guide.\n\nasymmetric encryption\n\nAn encryption algorithm that uses a pair of keys, a public key for encryption and a private key for decryption. You can share the public key because it isn’t used for decryption, but access to the private key should be highly restricted.\n\natomicity, consistency, isolation, durability (ACID)\n\nA set of software properties that guarantee the data validity and operational reliability of a database, even in the case of errors, power failures, or other problems.\n\nattribute-based access control (ABAC)\n\nThe practice of creating fine-grained permissions based on user attributes, such as department, job role, and team name. For more information, see ABAC for AWS in the AWS Identity and Access Management (IAM) documentation.\n\nauthoritative data source\n\nA location where you store the primary version of data, which is considered to be the most reliable source of information. You can copy data from the authoritative data source to other locations for the purposes of processing or modifying the data, such as anonymizing, redacting, or pseudonymizing it.\n\nAvailability Zone\n\nA distinct location within an AWS Region that is insulated from failures in other Availability Zones and provides inexpensive, low-latency network connectivity to other Availability Zones in the same Region.\n\nAWS Cloud Adoption Framework (AWS CAF)\n\nA framework of guidelines and best practices from AWS to help organizations develop an efficient and effective plan to move successfully to the cloud. AWS CAF organizes guidance into six focus areas called perspectives: business, people, governance, platform, security, and operations. The business, people, and governance perspectives focus on business skills and processes; the platform, security, and operations perspectives focus on technical skills and processes. For example, the people perspective targets stakeholders who handle human resources (HR), staffing functions, and people management. For this perspective, AWS CAF provides guidance for people development, training, and communications to help ready the organization for successful cloud adoption. For more information, see the AWS CAF website and the AWS CAF whitepaper.\n\nAWS Workload Qualification Framework (AWS WQF)\n\nA tool that evaluates database migration workloads, recommends migration strategies, and provides work estimates. AWS WQF is included with AWS Schema Conversion Tool (AWS SCT). It analyzes database schemas and code objects, application code, dependencies, and performance characteristics, and provides assessment reports.\n\nB\nbad bot\n\nA bot that is intended to disrupt or cause harm to individuals or organizations.\n\nBCP\n\nSee business continuity planning.\n\nbehavior graph\n\nA unified, interactive view of resource behavior and interactions over time. You can use a behavior graph with Amazon Detective to examine failed logon attempts, suspicious API calls, and similar actions. For more information, see Data in a behavior graph in the Detective documentation.\n\nbig-endian system\n\nA system that stores the most significant byte first. See also endianness.\n\nbinary classification\n\nA process that predicts a binary outcome (one of two possible classes). For example, your ML model might need to predict problems such as “Is this email spam or not spam?\" or \"Is this product a book or a car?\"\n\nbloom filter\n\nA probabilistic, memory-efficient data structure that is used to test whether an element is a member of a set.\n\nblue/green deployment\n\nA deployment strategy where you create two separate but identical environments. You run the current application version in one environment (blue) and the new application version in the other environment (green). This strategy helps you quickly roll back with minimal impact.\n\nbot\n\nA software application that runs automated tasks over the internet and simulates human activity or interaction. Some bots are useful or beneficial, such as web crawlers that index information on the internet. Some other bots, known as bad bots, are intended to disrupt or cause harm to individuals or organizations.\n\nbotnet\n\nNetworks of bots that are infected by malware and are under the control of a single party, known as a bot herder or bot operator. Botnets are the best-known mechanism to scale bots and their impact.\n\nbranch\n\nA contained area of a code repository. The first branch created in a repository is the main branch. You can create a new branch from an existing branch, and you can then develop features or fix bugs in the new branch. A branch you create to build a feature is commonly referred to as a feature branch. When the feature is ready for release, you merge the feature branch back into the main branch. For more information, see About branches (GitHub documentation).\n\nbreak-glass access\n\nIn exceptional circumstances and through an approved process, a quick means for a user to gain access to an AWS account that they don't typically have permissions to access. For more information, see the Implement break-glass procedures indicator in the AWS Well-Architected guidance.\n\nbrownfield strategy\n\nThe existing infrastructure in your environment. When adopting a brownfield strategy for a system architecture, you design the architecture around the constraints of the current systems and infrastructure. If you are expanding the existing infrastructure, you might blend brownfield and greenfield strategies.\n\nbuffer cache\n\nThe memory area where the most frequently accessed data is stored.\n\nbusiness capability\n\nWhat a business does to generate value (for example, sales, customer service, or marketing). Microservices architectures and development decisions can be driven by business capabilities. For more information, see the Organized around business capabilities section of the Running containerized microservices on AWS whitepaper.\n\nbusiness continuity planning (BCP)\n\nA plan that addresses the potential impact of a disruptive event, such as a large-scale migration, on operations and enables a business to resume operations quickly.\n\nC\nCAF\n\nSee AWS Cloud Adoption Framework.\n\ncanary deployment\n\nThe slow and incremental release of a version to end users. When you are confident, you deploy the new version and replace the current version in its entirety.\n\nCCoE\n\nSee Cloud Center of Excellence.\n\nCDC\n\nSee change data capture.\n\nchange data capture (CDC)\n\nThe process of tracking changes to a data source, such as a database table, and recording metadata about the change. You can use CDC for various purposes, such as auditing or replicating changes in a target system to maintain synchronization.\n\nchaos engineering\n\nIntentionally introducing failures or disruptive events to test a system’s resilience. You can use AWS Fault Injection Service (AWS FIS) to perform experiments that stress your AWS workloads and evaluate their response.\n\nCI/CD\n\nSee continuous integration and continuous delivery.\n\nclassification\n\nA categorization process that helps generate predictions. ML models for classification problems predict a discrete value. Discrete values are always distinct from one another. For example, a model might need to evaluate whether or not there is a car in an image.\n\nclient-side encryption\n\nEncryption of data locally, before the target AWS service receives it.\n\nCloud Center of Excellence (CCoE)\n\nA multi-disciplinary team that drives cloud adoption efforts across an organization, including developing cloud best practices, mobilizing resources, establishing migration timelines, and leading the organization through large-scale transformations. For more information, see the CCoE posts on the AWS Cloud Enterprise Strategy Blog.\n\ncloud computing\n\nThe cloud technology that is typically used for remote data storage and IoT device management. Cloud computing is commonly connected to edge computing technology.\n\ncloud operating model\n\nIn an IT organization, the operating model that is used to build, mature, and optimize one or more cloud environments. For more information, see Building your Cloud Operating Model.\n\ncloud stages of adoption\n\nThe four phases that organizations typically go through when they migrate to the AWS Cloud:\n\nProject – Running a few cloud-related projects for proof of concept and learning purposes\n\nFoundation – Making foundational investments to scale your cloud adoption (e.g., creating a landing zone, defining a CCoE, establishing an operations model)\n\nMigration – Migrating individual applications\n\nRe-invention – Optimizing products and services, and innovating in the cloud\n\nThese stages were defined by Stephen Orban in the blog post The Journey Toward Cloud-First & the Stages of Adoption on the AWS Cloud Enterprise Strategy blog. For information about how they relate to the AWS migration strategy, see the migration readiness guide.\n\nCMDB\n\nSee configuration management database.\n\ncode repository\n\nA location where source code and other assets, such as documentation, samples, and scripts, are stored and updated through version control processes. Common cloud repositories include GitHub or AWS CodeCommit. Each version of the code is called a branch. In a microservice structure, each repository is devoted to a single piece of functionality. A single CI/CD pipeline can use multiple repositories.\n\ncold cache\n\nA buffer cache that is empty, not well populated, or contains stale or irrelevant data. This affects performance because the database instance must read from the main memory or disk, which is slower than reading from the buffer cache.\n\ncold data\n\nData that is rarely accessed and is typically historical. When querying this kind of data, slow queries are typically acceptable. Moving this data to lower-performing and less expensive storage tiers or classes can reduce costs.\n\ncomputer vision (CV)\n\nA field of AI that uses machine learning to analyze and extract information from visual formats such as digital images and videos. For example, AWS Panorama offers devices that add CV to on-premises camera networks, and Amazon SageMaker provides image processing algorithms for CV.\n\nconfiguration drift\n\nFor a workload, a configuration change from the expected state. It might cause the workload to become noncompliant, and it's typically gradual and unintentional.\n\nconfiguration management database (CMDB)\n\nA repository that stores and manages information about a database and its IT environment, including both hardware and software components and their configurations. You typically use data from a CMDB in the portfolio discovery and analysis stage of migration.\n\nconformance pack\n\nA collection of AWS Config rules and remediation actions that you can assemble to customize your compliance and security checks. You can deploy a conformance pack as a single entity in an AWS account and Region, or across an organization, by using a YAML template. For more information, see Conformance packs in the AWS Config documentation.\n\ncontinuous integration and continuous delivery (CI/CD)\n\nThe process of automating the source, build, test, staging, and production stages of the software release process. CI/CD is commonly described as a pipeline. CI/CD can help you automate processes, improve productivity, improve code quality, and deliver faster. For more information, see Benefits of continuous delivery. CD can also stand for continuous deployment. For more information, see Continuous Delivery vs. Continuous Deployment.\n\nCV\n\nSee computer vision.\n\nD\ndata at rest\n\nData that is stationary in your network, such as data that is in storage.\n\ndata classification\n\nA process for identifying and categorizing the data in your network based on its criticality and sensitivity. It is a critical component of any cybersecurity risk management strategy because it helps you determine the appropriate protection and retention controls for the data. Data classification is a component of the security pillar in the AWS Well-Architected Framework. For more information, see Data classification.\n\ndata drift\n\nA meaningful variation between the production data and the data that was used to train an ML model, or a meaningful change in the input data over time. Data drift can reduce the overall quality, accuracy, and fairness in ML model predictions.\n\ndata in transit\n\nData that is actively moving through your network, such as between network resources.\n\ndata mesh\n\nAn architectural framework that provides distributed, decentralized data ownership with centralized management and governance.\n\ndata minimization\n\nThe principle of collecting and processing only the data that is strictly necessary. Practicing data minimization in the AWS Cloud can reduce privacy risks, costs, and your analytics carbon footprint.\n\ndata perimeter\n\nA set of preventive guardrails in your AWS environment that help make sure that only trusted identities are accessing trusted resources from expected networks. For more information, see Building a data perimeter on AWS.\n\ndata preprocessing\n\nTo transform raw data into a format that is easily parsed by your ML model. Preprocessing data can mean removing certain columns or rows and addressing missing, inconsistent, or duplicate values.\n\ndata provenance\n\nThe process of tracking the origin and history of data throughout its lifecycle, such as how the data was generated, transmitted, and stored.\n\ndata subject\n\nAn individual whose data is being collected and processed.\n\ndata warehouse\n\nA data management system that supports business intelligence, such as analytics. Data warehouses commonly contain large amounts of historical data, and they are typically used for queries and analysis.\n\ndatabase definition language (DDL)\n\nStatements or commands for creating or modifying the structure of tables and objects in a database.\n\ndatabase manipulation language (DML)\n\nStatements or commands for modifying (inserting, updating, and deleting) information in a database.\n\nDDL\n\nSee database definition language.\n\ndeep ensemble\n\nTo combine multiple deep learning models for prediction. You can use deep ensembles to obtain a more accurate prediction or for estimating uncertainty in predictions.\n\ndeep learning\n\nAn ML subfield that uses multiple layers of artificial neural networks to identify mapping between input data and target variables of interest.\n\ndefense-in-depth\n\nAn information security approach in which a series of security mechanisms and controls are thoughtfully layered throughout a computer network to protect the confidentiality, integrity, and availability of the network and the data within. When you adopt this strategy on AWS, you add multiple controls at different layers of the AWS Organizations structure to help secure resources. For example, a defense-in-depth approach might combine multi-factor authentication, network segmentation, and encryption.\n\ndelegated administrator\n\nIn AWS Organizations, a compatible service can register an AWS member account to administer the organization’s accounts and manage permissions for that service. This account is called the delegated administrator for that service. For more information and a list of compatible services, see Services that work with AWS Organizations in the AWS Organizations documentation.\n\ndeployment\n\nThe process of making an application, new features, or code fixes available in the target environment. Deployment involves implementing changes in a code base and then building and running that code base in the application’s environments.\n\ndevelopment environment\n\nSee environment.\n\ndetective control\n\nA security control that is designed to detect, log, and alert after an event has occurred. These controls are a second line of defense, alerting you to security events that bypassed the preventative controls in place. For more information, see Detective controls in Implementing security controls on AWS.\n\ndevelopment value stream mapping (DVSM)\n\nA process used to identify and prioritize constraints that adversely affect speed and quality in a software development lifecycle. DVSM extends the value stream mapping process originally designed for lean manufacturing practices. It focuses on the steps and teams required to create and move value through the software development process.\n\ndigital twin\n\nA virtual representation of a real-world system, such as a building, factory, industrial equipment, or production line. Digital twins support predictive maintenance, remote monitoring, and production optimization.\n\ndimension table\n\nIn a star schema, a smaller table that contains data attributes about quantitative data in a fact table. Dimension table attributes are typically text fields or discrete numbers that behave like text. These attributes are commonly used for query constraining, filtering, and result set labeling.\n\ndisaster\n\nAn event that prevents a workload or system from fulfilling its business objectives in its primary deployed location. These events can be natural disasters, technical failures, or the result of human actions, such as unintentional misconfiguration or a malware attack.\n\ndisaster recovery (DR)\n\nThe strategy and process you use to minimize downtime and data loss caused by a disaster. For more information, see Disaster Recovery of Workloads on AWS: Recovery in the Cloud in the AWS Well-Architected Framework.\n\nDML\n\nSee database manipulation language.\n\ndomain-driven design\n\nAn approach to developing a complex software system by connecting its components to evolving domains, or core business goals, that each component serves. This concept was introduced by Eric Evans in his book, Domain-Driven Design: Tackling Complexity in the Heart of Software (Boston: Addison-Wesley Professional, 2003). For information about how you can use domain-driven design with the strangler fig pattern, see Modernizing legacy Microsoft ASP.NET (ASMX) web services incrementally by using containers and Amazon API Gateway.\n\nDR\n\nSee disaster recovery.\n\ndrift detection\n\nTracking deviations from a baselined configuration. For example, you can use AWS CloudFormation to detect drift in system resources, or you can use AWS Control Tower to detect changes in your landing zone that might affect compliance with governance requirements.\n\nDVSM\n\nSee development value stream mapping.\n\nE\nEDA\n\nSee exploratory data analysis.\n\nedge computing\n\nThe technology that increases the computing power for smart devices at the edges of an IoT network. When compared with cloud computing, edge computing can reduce communication latency and improve response time.\n\nencryption\n\nA computing process that transforms plaintext data, which is human-readable, into ciphertext.\n\nencryption key\n\nA cryptographic string of randomized bits that is generated by an encryption algorithm. Keys can vary in length, and each key is designed to be unpredictable and unique.\n\nendianness\n\nThe order in which bytes are stored in computer memory. Big-endian systems store the most significant byte first. Little-endian systems store the least significant byte first.\n\nendpoint\n\nSee service endpoint.\n\nendpoint service\n\nA service that you can host in a virtual private cloud (VPC) to share with other users. You can create an endpoint service with AWS PrivateLink and grant permissions to other AWS accounts or to AWS Identity and Access Management (IAM) principals. These accounts or principals can connect to your endpoint service privately by creating interface VPC endpoints. For more information, see Create an endpoint service in the Amazon Virtual Private Cloud (Amazon VPC) documentation.\n\nenterprise resource planning (ERP)\n\nA system that automates and manages key business processes (such as accounting, MES, and project management) for an enterprise.\n\nenvelope encryption\n\nThe process of encrypting an encryption key with another encryption key. For more information, see Envelope encryption in the AWS Key Management Service (AWS KMS) documentation.\n\nenvironment\n\nAn instance of a running application. The following are common types of environments in cloud computing:\n\ndevelopment environment – An instance of a running application that is available only to the core team responsible for maintaining the application. Development environments are used to test changes before promoting them to upper environments. This type of environment is sometimes referred to as a test environment.\n\nlower environments – All development environments for an application, such as those used for initial builds and tests.\n\nproduction environment – An instance of a running application that end users can access. In a CI/CD pipeline, the production environment is the last deployment environment.\n\nupper environments – All environments that can be accessed by users other than the core development team. This can include a production environment, preproduction environments, and environments for user acceptance testing.\n\nepic\n\nIn agile methodologies, functional categories that help organize and prioritize your work. Epics provide a high-level description of requirements and implementation tasks. For example, AWS CAF security epics include identity and access management, detective controls, infrastructure security, data protection, and incident response. For more information about epics in the AWS migration strategy, see the program implementation guide.\n\nERP\n\nSee enterprise resource planning.\n\nexploratory data analysis (EDA)\n\nThe process of analyzing a dataset to understand its main characteristics. You collect or aggregate data and then perform initial investigations to find patterns, detect anomalies, and check assumptions. EDA is performed by calculating summary statistics and creating data visualizations.\n\nF\nfact table\n\nThe central table in a star schema. It stores quantitative data about business operations. Typically, a fact table contains two types of columns: those that contain measures and those that contain a foreign key to a dimension table.\n\nfail fast\n\nA philosophy that uses frequent and incremental testing to reduce the development lifecycle. It is a critical part of an agile approach.\n\nfault isolation boundary\n\nIn the AWS Cloud, a boundary such as an Availability Zone, AWS Region, control plane, or data plane that limits the effect of a failure and helps improve the resilience of workloads. For more information, see AWS Fault Isolation Boundaries.\n\nfeature branch\n\nSee branch.\n\nfeatures\n\nThe input data that you use to make a prediction. For example, in a manufacturing context, features could be images that are periodically captured from the manufacturing line.\n\nfeature importance\n\nHow significant a feature is for a model’s predictions. This is usually expressed as a numerical score that can be calculated through various techniques, such as Shapley Additive Explanations (SHAP) and integrated gradients. For more information, see Machine learning model interpretability with :AWS.\n\nfeature transformation\n\nTo optimize data for the ML process, including enriching data with additional sources, scaling values, or extracting multiple sets of information from a single data field. This enables the ML model to benefit from the data. For example, if you break down the “2021-05-27 00:15:37” date into “2021”, “May”, “Thu”, and “15”, you can help the learning algorithm learn nuanced patterns associated with different data components.\n\nFGAC\n\nSee fine-grained access control.\n\nfine-grained access control (FGAC)\n\nThe use of multiple conditions to allow or deny an access request.\n\nflash-cut migration\n\nA database migration method that uses continuous data replication through change data capture to migrate data in the shortest time possible, instead of using a phased approach. The objective is to keep downtime to a minimum.\n\nG\ngeo blocking\n\nSee geographic restrictions.\n\ngeographic restrictions (geo blocking)\n\nIn Amazon CloudFront, an option to prevent users in specific countries from accessing content distributions. You can use an allow list or block list to specify approved and banned countries. For more information, see Restricting the geographic distribution of your content in the CloudFront documentation.\n\nGitflow workflow\n\nAn approach in which lower and upper environments use different branches in a source code repository. The Gitflow workflow is considered legacy, and the trunk-based workflow is the modern, preferred approach.\n\ngreenfield strategy\n\nThe absence of existing infrastructure in a new environment. When adopting a greenfield strategy for a system architecture, you can select all new technologies without the restriction of compatibility with existing infrastructure, also known as brownfield. If you are expanding the existing infrastructure, you might blend brownfield and greenfield strategies.\n\nguardrail\n\nA high-level rule that helps govern resources, policies, and compliance across organizational units (OUs). Preventive guardrails enforce policies to ensure alignment to compliance standards. They are implemented by using service control policies and IAM permissions boundaries. Detective guardrails detect policy violations and compliance issues, and generate alerts for remediation. They are implemented by using AWS Config, AWS Security Hub, Amazon GuardDuty, AWS Trusted Advisor, Amazon Inspector, and custom AWS Lambda checks.\n\nH\nHA\n\nSee high availability.\n\nheterogeneous database migration\n\nMigrating your source database to a target database that uses a different database engine (for example, Oracle to Amazon Aurora). Heterogeneous migration is typically part of a re-architecting effort, and converting the schema can be a complex task. AWS provides AWS SCT that helps with schema conversions.\n\nhigh availability (HA)\n\nThe ability of a workload to operate continuously, without intervention, in the event of challenges or disasters. HA systems are designed to automatically fail over, consistently deliver high-quality performance, and handle different loads and failures with minimal performance impact.\n\nhistorian modernization\n\nAn approach used to modernize and upgrade operational technology (OT) systems to better serve the needs of the manufacturing industry. A historian is a type of database that is used to collect and store data from various sources in a factory.\n\nhomogeneous database migration\n\nMigrating your source database to a target database that shares the same database engine (for example, Microsoft SQL Server to Amazon RDS for SQL Server). Homogeneous migration is typically part of a rehosting or replatforming effort. You can use native database utilities to migrate the schema.\n\nhot data\n\nData that is frequently accessed, such as real-time data or recent translational data. This data typically requires a high-performance storage tier or class to provide fast query responses.\n\nhotfix\n\nAn urgent fix for a critical issue in a production environment. Due to its urgency, a hotfix is usually made outside of the typical DevOps release workflow.\n\nhypercare period\n\nImmediately following cutover, the period of time when a migration team manages and monitors the migrated applications in the cloud in order to address any issues. Typically, this period is 1–4 days in length. At the end of the hypercare period, the migration team typically transfers responsibility for the applications to the cloud operations team.\n\nI\nIaC\n\nSee infrastructure as code.\n\nidentity-based policy\n\nA policy attached to one or more IAM principals that defines their permissions within the AWS Cloud environment.\n\nidle application\n\nAn application that has an average CPU and memory usage between 5 and 20 percent over a period of 90 days. In a migration project, it is common to retire these applications or retain them on premises.\n\nIIoT\n\nSee industrial Internet of Things.\n\nimmutable infrastructure\n\nA model that deploys new infrastructure for production workloads instead of updating, patching, or modifying the existing infrastructure. Immutable infrastructures are inherently more consistent, reliable, and predictable than mutable infrastructure. For more information, see the Deploy using immutable infrastructure best practice in the AWS Well-Architected Framework.\n\ninbound (ingress) VPC\n\nIn an AWS multi-account architecture, a VPC that accepts, inspects, and routes network connections from outside an application. The AWS Security Reference Architecture recommends setting up your Network account with inbound, outbound, and inspection VPCs to protect the two-way interface between your application and the broader internet.\n\nincremental migration\n\nA cutover strategy in which you migrate your application in small parts instead of performing a single, full cutover. For example, you might move only a few microservices or users to the new system initially. After you verify that everything is working properly, you can incrementally move additional microservices or users until you can decommission your legacy system. This strategy reduces the risks associated with large migrations.\n\nIndustry 4.0\n\nA term that was introduced by Klaus Schwab in 2016 to refer to the modernization of manufacturing processes through advances in connectivity, real-time data, automation, analytics, and AI/ML.\n\ninfrastructure\n\nAll of the resources and assets contained within an application’s environment.\n\ninfrastructure as code (IaC)\n\nThe process of provisioning and managing an application’s infrastructure through a set of configuration files. IaC is designed to help you centralize infrastructure management, standardize resources, and scale quickly so that new environments are repeatable, reliable, and consistent.\n\nindustrial Internet of Things (IIoT)\n\nThe use of internet-connected sensors and devices in the industrial sectors, such as manufacturing, energy, automotive, healthcare, life sciences, and agriculture. For more information, see Building an industrial Internet of Things (IIoT) digital transformation strategy.\n\ninspection VPC\n\nIn an AWS multi-account architecture, a centralized VPC that manages inspections of network traffic between VPCs (in the same or different AWS Regions), the internet, and on-premises networks. The AWS Security Reference Architecture recommends setting up your Network account with inbound, outbound, and inspection VPCs to protect the two-way interface between your application and the broader internet.\n\nInternet of Things (IoT)\n\nThe network of connected physical objects with embedded sensors or processors that communicate with other devices and systems through the internet or over a local communication network. For more information, see What is IoT?\n\ninterpretability\n\nA characteristic of a machine learning model that describes the degree to which a human can understand how the model’s predictions depend on its inputs. For more information, see Machine learning model interpretability with AWS.\n\nIoT\n\nSee Internet of Things.\n\nIT information library (ITIL)\n\nA set of best practices for delivering IT services and aligning these services with business requirements. ITIL provides the foundation for ITSM.\n\nIT service management (ITSM)\n\nActivities associated with designing, implementing, managing, and supporting IT services for an organization. For information about integrating cloud operations with ITSM tools, see the operations integration guide.\n\nITIL\n\nSee IT information library.\n\nITSM\n\nSee IT service management.\n\nL\nlabel-based access control (LBAC)\n\nAn implementation of mandatory access control (MAC) where the users and the data itself are each explicitly assigned a security label value. The intersection between the user security label and data security label determines which rows and columns can be seen by the user.\n\nlanding zone\n\nA landing zone is a well-architected, multi-account AWS environment that is scalable and secure. This is a starting point from which your organizations can quickly launch and deploy workloads and applications with confidence in their security and infrastructure environment. For more information about landing zones, see Setting up a secure and scalable multi-account AWS environment.\n\nlarge migration\n\nA migration of 300 or more servers.\n\nLBAC\n\nSee label-based access control.\n\nleast privilege\n\nThe security best practice of granting the minimum permissions required to perform a task. For more information, see Apply least-privilege permissions in the IAM documentation.\n\nlift and shift\n\nSee 7 Rs.\n\nlittle-endian system\n\nA system that stores the least significant byte first. See also endianness.\n\nlower environments\n\nSee environment.\n\nM\nmachine learning (ML)\n\nA type of artificial intelligence that uses algorithms and techniques for pattern recognition and learning. ML analyzes and learns from recorded data, such as Internet of Things (IoT) data, to generate a statistical model based on patterns. For more information, see Machine Learning.\n\nmain branch\n\nSee branch.\n\nmalware\n\nSoftware that is designed to compromise computer security or privacy. Malware might disrupt computer systems, leak sensitive information, or gain unauthorized access. Examples of malware include viruses, worms, ransomware, Trojan horses, spyware, and keyloggers.\n\nmanaged services\n\nAWS services for which AWS operates the infrastructure layer, the operating system, and platforms, and you access the endpoints to store and retrieve data. Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB are examples of managed services. These are also known as abstracted services.\n\nmanufacturing execution system (MES)\n\nA software system for tracking, monitoring, documenting, and controlling production processes that convert raw materials to finished products on the shop floor.\n\nMAP\n\nSee Migration Acceleration Program.\n\nmechanism\n\nA complete process in which you create a tool, drive adoption of the tool, and then inspect the results in order to make adjustments. A mechanism is a cycle that reinforces and improves itself as it operates. For more information, see Building mechanisms in the AWS Well-Architected Framework.\n\nmember account\n\nAll AWS accounts other than the management account that are part of an organization in AWS Organizations. An account can be a member of only one organization at a time.\n\nMES\n\nSee manufacturing execution system.\n\nMessage Queuing Telemetry Transport (MQTT)\n\nA lightweight, machine-to-machine (M2M) communication protocol, based on the publish/subscribe pattern, for resource-constrained IoT devices.\n\nmicroservice\n\nA small, independent service that communicates over well-defined APIs and is typically owned by small, self-contained teams. For example, an insurance system might include microservices that map to business capabilities, such as sales or marketing, or subdomains, such as purchasing, claims, or analytics. The benefits of microservices include agility, flexible scaling, easy deployment, reusable code, and resilience. For more information, see Integrating microservices by using AWS serverless services.\n\nmicroservices architecture\n\nAn approach to building an application with independent components that run each application process as a microservice. These microservices communicate through a well-defined interface by using lightweight APIs. Each microservice in this architecture can be updated, deployed, and scaled to meet demand for specific functions of an application. For more information, see Implementing microservices on AWS.\n\nMigration Acceleration Program (MAP)\n\nAn AWS program that provides consulting support, training, and services to help organizations build a strong operational foundation for moving to the cloud, and to help offset the initial cost of migrations. MAP includes a migration methodology for executing legacy migrations in a methodical way and a set of tools to automate and accelerate common migration scenarios.\n\nmigration at scale\n\nThe process of moving the majority of the application portfolio to the cloud in waves, with more applications moved at a faster rate in each wave. This phase uses the best practices and lessons learned from the earlier phases to implement a migration factory of teams, tools, and processes to streamline the migration of workloads through automation and agile delivery. This is the third phase of the AWS migration strategy.\n\nmigration factory\n\nCross-functional teams that streamline the migration of workloads through automated, agile approaches. Migration factory teams typically include operations, business analysts and owners, migration engineers, developers, and DevOps professionals working in sprints. Between 20 and 50 percent of an enterprise application portfolio consists of repeated patterns that can be optimized by a factory approach. For more information, see the discussion of migration factories and the Cloud Migration Factory guide in this content set.\n\nmigration metadata\n\nThe information about the application and server that is needed to complete the migration. Each migration pattern requires a different set of migration metadata. Examples of migration metadata include the target subnet, security group, and AWS account.\n\nmigration pattern\n\nA repeatable migration task that details the migration strategy, the migration destination, and the migration application or service used. Example: Rehost migration to Amazon EC2 with AWS Application Migration Service.\n\nMigration Portfolio Assessment (MPA)\n\nAn online tool that provides information for validating the business case for migrating to the AWS Cloud. MPA provides detailed portfolio assessment (server right-sizing, pricing, TCO comparisons, migration cost analysis) as well as migration planning (application data analysis and data collection, application grouping, migration prioritization, and wave planning). The MPA tool (requires login) is available free of charge to all AWS consultants and APN Partner consultants.\n\nMigration Readiness Assessment (MRA)\n\nThe process of gaining insights about an organization’s cloud readiness status, identifying strengths and weaknesses, and building an action plan to close identified gaps, using the AWS CAF. For more information, see the migration readiness guide. MRA is the first phase of the AWS migration strategy.\n\nmigration strategy\n\nThe approach used to migrate a workload to the AWS Cloud. For more information, see the 7 Rs entry in this glossary and see Mobilize your organization to accelerate large-scale migrations.\n\nML\n\nSee machine learning.\n\nmodernization\n\nTransforming an outdated (legacy or monolithic) application and its infrastructure into an agile, elastic, and highly available system in the cloud to reduce costs, gain efficiencies, and take advantage of innovations. For more information, see Strategy for modernizing applications in the AWS Cloud.\n\nmodernization readiness assessment\n\nAn evaluation that helps determine the modernization readiness of an organization’s applications; identifies benefits, risks, and dependencies; and determines how well the organization can support the future state of those applications. The outcome of the assessment is a blueprint of the target architecture, a roadmap that details development phases and milestones for the modernization process, and an action plan for addressing identified gaps. For more information, see Evaluating modernization readiness for applications in the AWS Cloud.\n\nmonolithic applications (monoliths)\n\nApplications that run as a single service with tightly coupled processes. Monolithic applications have several drawbacks. If one application feature experiences a spike in demand, the entire architecture must be scaled. Adding or improving a monolithic application’s features also becomes more complex when the code base grows. To address these issues, you can use a microservices architecture. For more information, see Decomposing monoliths into microservices.\n\nMPA\n\nSee Migration Portfolio Assessment.\n\nMQTT\n\nSee Message Queuing Telemetry Transport.\n\nmulticlass classification\n\nA process that helps generate predictions for multiple classes (predicting one of more than two outcomes). For example, an ML model might ask \"Is this product a book, car, or phone?\" or \"Which product category is most interesting to this customer?\"\n\nmutable infrastructure\n\nA model that updates and modifies the existing infrastructure for production workloads. For improved consistency, reliability, and predictability, the AWS Well-Architected Framework recommends the use of immutable infrastructure as a best practice.\n\nO\nOAC\n\nSee origin access control.\n\nOAI\n\nSee origin access identity.\n\nOCM\n\nSee organizational change management.\n\noffline migration\n\nA migration method in which the source workload is taken down during the migration process. This method involves extended downtime and is typically used for small, non-critical workloads.\n\nOI\n\nSee operations integration.\n\nOLA\n\nSee operational-level agreement.\n\nonline migration\n\nA migration method in which the source workload is copied to the target system without being taken offline. Applications that are connected to the workload can continue to function during the migration. This method involves zero to minimal downtime and is typically used for critical production workloads.\n\nOPC-UA\n\nSee Open Process Communications - Unified Architecture.\n\nOpen Process Communications - Unified Architecture (OPC-UA)\n\nA machine-to-machine (M2M) communication protocol for industrial automation. OPC-UA provides an interoperability standard with data encryption, authentication, and authorization schemes.\n\noperational-level agreement (OLA)\n\nAn agreement that clarifies what functional IT groups promise to deliver to each other, to support a service-level agreement (SLA).\n\noperational readiness review (ORR)\n\nA checklist of questions and associated best practices that help you understand, evaluate, prevent, or reduce the scope of incidents and possible failures. For more information, see Operational Readiness Reviews (ORR) in the AWS Well-Architected Framework.\n\noperational technology (OT)\n\nHardware and software systems that work with the physical environment to control industrial operations, equipment, and infrastructure. In manufacturing, the integration of OT and information technology (IT) systems is a key focus for Industry 4.0 transformations.\n\noperations integration (OI)\n\nThe process of modernizing operations in the cloud, which involves readiness planning, automation, and integration. For more information, see the operations integration guide.\n\norganization trail\n\nA trail that’s created by AWS CloudTrail that logs all events for all AWS accounts in an organization in AWS Organizations. This trail is created in each AWS account that’s part of the organization and tracks the activity in each account. For more information, see Creating a trail for an organization in the CloudTrail documentation.\n\norganizational change management (OCM)\n\nA framework for managing major, disruptive business transformations from a people, culture, and leadership perspective. OCM helps organizations prepare for, and transition to, new systems and strategies by accelerating change adoption, addressing transitional issues, and driving cultural and organizational changes. In the AWS migration strategy, this framework is called people acceleration, because of the speed of change required in cloud adoption projects. For more information, see the OCM guide.\n\norigin access control (OAC)\n\nIn CloudFront, an enhanced option for restricting access to secure your Amazon Simple Storage Service (Amazon S3) content. OAC supports all S3 buckets in all AWS Regions, server-side encryption with AWS KMS (SSE-KMS), and dynamic PUT and DELETE requests to the S3 bucket.\n\norigin access identity (OAI)\n\nIn CloudFront, an option for restricting access to secure your Amazon S3 content. When you use OAI, CloudFront creates a principal that Amazon S3 can authenticate with. Authenticated principals can access content in an S3 bucket only through a specific CloudFront distribution. See also OAC, which provides more granular and enhanced access control.\n\nORR\n\nSee operational readiness review.\n\nOT\n\nSee operational technology.\n\noutbound (egress) VPC\n\nIn an AWS multi-account architecture, a VPC that handles network connections that are initiated from within an application. The AWS Security Reference Architecture recommends setting up your Network account with inbound, outbound, and inspection VPCs to protect the two-way interface between your application and the broader internet.\n\nP\npermissions boundary\n\nAn IAM management policy that is attached to IAM principals to set the maximum permissions that the user or role can have. For more information, see Permissions boundaries in the IAM documentation.\n\npersonally identifiable information (PII)\n\nInformation that, when viewed directly or paired with other related data, can be used to reasonably infer the identity of an individual. Examples of PII include names, addresses, and contact information.\n\nPII\n\nSee personally identifiable information.\n\nplaybook\n\nA set of predefined steps that capture the work associated with migrations, such as delivering core operations functions in the cloud. A playbook can take the form of scripts, automated runbooks, or a summary of processes or steps required to operate your modernized environment.\n\nPLC\n\nSee programmable logic controller.\n\nPLM\n\nSee product lifecycle management.\n\npolicy\n\nAn object that can define permissions (see identity-based policy), specify access conditions (see resource-based policy), or define the maximum permissions for all accounts in an organization in AWS Organizations (see service control policy).\n\npolyglot persistence\n\nIndependently choosing a microservice’s data storage technology based on data access patterns and other requirements. If your microservices have the same data storage technology, they can encounter implementation challenges or experience poor performance. Microservices are more easily implemented and achieve better performance and scalability if they use the data store best adapted to their requirements. For more information, see Enabling data persistence in microservices.\n\nportfolio assessment\n\nA process of discovering, analyzing, and prioritizing the application portfolio in order to plan the migration. For more information, see Evaluating migration readiness.\n\npredicate\n\nA query condition that returns true or false, commonly located in a WHERE clause.\n\npredicate pushdown\n\nA database query optimization technique that filters the data in the query before transfer. This reduces the amount of data that must be retrieved and processed from the relational database, and it improves query performance.\n\npreventative control\n\nA security control that is designed to prevent an event from occurring. These controls are a first line of defense to help prevent unauthorized access or unwanted changes to your network. For more information, see Preventative controls in Implementing security controls on AWS.\n\nprincipal\n\nAn entity in AWS that can perform actions and access resources. This entity is typically a root user for an AWS account, an IAM role, or a user. For more information, see Principal in Roles terms and concepts in the IAM documentation.\n\nPrivacy by Design\n\nAn approach in system engineering that takes privacy into account throughout the whole engineering process.\n\nprivate hosted zones\n\nA container that holds information about how you want Amazon Route 53 to respond to DNS queries for a domain and its subdomains within one or more VPCs. For more information, see Working with private hosted zones in the Route 53 documentation.\n\nproactive control\n\nA security control designed to prevent the deployment of noncompliant resources. These controls scan resources before they are provisioned. If the resource is not compliant with the control, then it isn't provisioned. For more information, see the Controls reference guide in the AWS Control Tower documentation and see Proactive controls in Implementing security controls on AWS.\n\nproduct lifecycle management (PLM)\n\nThe management of data and processes for a product throughout its entire lifecycle, from design, development, and launch, through growth and maturity, to decline and removal.\n\nproduction environment\n\nSee environment.\n\nprogrammable logic controller (PLC)\n\nIn manufacturing, a highly reliable, adaptable computer that monitors machines and automates manufacturing processes.\n\npseudonymization\n\nThe process of replacing personal identifiers in a dataset with placeholder values. Pseudonymization can help protect personal privacy. Pseudonymized data is still considered to be personal data.\n\npublish/subscribe (pub/sub)\n\nA pattern that enables asynchronous communications among microservices to improve scalability and responsiveness. For example, in a microservices-based MES, a microservice can publish event messages to a channel that other microservices can subscribe to. The system can add new microservices without changing the publishing service.\n\nQ\nquery plan\n\nA series of steps, like instructions, that are used to access the data in a SQL relational database system.\n\nquery plan regression\n\nWhen a database service optimizer chooses a less optimal plan than it did before a given change to the database environment. This can be caused by changes to statistics, constraints, environment settings, query parameter bindings, and updates to the database engine.\n\nR\nRACI matrix\n\nSee responsible, accountable, consulted, informed (RACI).\n\nransomware\n\nA malicious software that is designed to block access to a computer system or data until a payment is made.\n\nRASCI matrix\n\nSee responsible, accountable, consulted, informed (RACI).\n\nRCAC\n\nSee row and column access control.\n\nread replica\n\nA copy of a database that’s used for read-only purposes. You can route queries to the read replica to reduce the load on your primary database.\n\nre-architect\n\nSee 7 Rs.\n\nrecovery point objective (RPO)\n\nThe maximum acceptable amount of time since the last data recovery point. This determines what is considered an acceptable loss of data between the last recovery point and the interruption of service.\n\nrecovery time objective (RTO)\n\nThe maximum acceptable delay between the interruption of service and restoration of service.\n\nrefactor\n\nSee 7 Rs.\n\nRegion\n\nA collection of AWS resources in a geographic area. Each AWS Region is isolated and independent of the others to provide fault tolerance, stability, and resilience. For more information, see Specify which AWS Regions your account can use.\n\nregression\n\nAn ML technique that predicts a numeric value. For example, to solve the problem of \"What price will this house sell for?\" an ML model could use a linear regression model to predict a house's sale price based on known facts about the house (for example, the square footage).\n\nrehost\n\nSee 7 Rs.\n\nrelease\n\nIn a deployment process, the act of promoting changes to a production environment.\n\nrelocate\n\nSee 7 Rs.\n\nreplatform\n\nSee 7 Rs.\n\nrepurchase\n\nSee 7 Rs.\n\nresiliency\n\nAn application's ability to resist or recover from disruptions. High availability and disaster recovery are common considerations when planning for resiliency in the AWS Cloud. For more information, see AWS Cloud Resilience.\n\nresource-based policy\n\nA policy attached to a resource, such as an Amazon S3 bucket, an endpoint, or an encryption key. This type of policy specifies which principals are allowed access, supported actions, and any other conditions that must be met.\n\nresponsible, accountable, consulted, informed (RACI) matrix\n\nA matrix that defines the roles and responsibilities for all parties involved in migration activities and cloud operations. The matrix name is derived from the responsibility types defined in the matrix: responsible (R), accountable (A), consulted (C), and informed (I). The support (S) type is optional. If you include support, the matrix is called a RASCI matrix, and if you exclude it, it’s called a RACI matrix.\n\nresponsive control\n\nA security control that is designed to drive remediation of adverse events or deviations from your security baseline. For more information, see Responsive controls in Implementing security controls on AWS.\n\nretain\n\nSee 7 Rs.\n\nretire\n\nSee 7 Rs.\n\nrotation\n\nThe process of periodically updating a secret to make it more difficult for an attacker to access the credentials.\n\nrow and column access control (RCAC)\n\nThe use of basic, flexible SQL expressions that have defined access rules. RCAC consists of row permissions and column masks.\n\nRPO\n\nSee recovery point objective.\n\nRTO\n\nSee recovery time objective.\n\nrunbook\n\nA set of manual or automated procedures required to perform a specific task. These are typically built to streamline repetitive operations or procedures with high error rates.\n\nS\nSAML 2.0\n\nAn open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS API operations without you having to create user in IAM for everyone in your organization. For more information about SAML 2.0-based federation, see About SAML 2.0-based federation in the IAM documentation.\n\nSCADA\n\nSee supervisory control and data acquisition.\n\nSCP\n\nSee service control policy.\n\nsecret\n\nIn AWS Secrets Manager, confidential or restricted information, such as a password or user credentials, that you store in encrypted form. It consists of the secret value and its metadata. The secret value can be binary, a single string, or multiple strings. For more information, see Secret in the Secrets Manager documentation.\n\nsecurity control\n\nA technical or administrative guardrail that prevents, detects, or reduces the ability of a threat actor to exploit a security vulnerability. There are four primary types of security controls: preventative, detective, responsive, and proactive.\n\nsecurity hardening\n\nThe process of reducing the attack surface to make it more resistant to attacks. This can include actions such as removing resources that are no longer needed, implementing the security best practice of granting least privilege, or deactivating unnecessary features in configuration files.\n\nsecurity information and event management (SIEM) system\n\nTools and services that combine security information management (SIM) and security event management (SEM) systems. A SIEM system collects, monitors, and analyzes data from servers, networks, devices, and other sources to detect threats and security breaches, and to generate alerts.\n\nsecurity response automation\n\nA predefined and programmed action that is designed to automatically respond to or remediate a security event. These automations serve as detective or responsive security controls that help you implement AWS security best practices. Examples of automated response actions include modifying a VPC security group, patching an Amazon EC2 instance, or rotating credentials.\n\nserver-side encryption\n\nEncryption of data at its destination, by the AWS service that receives it.\n\nservice control policy (SCP)\n\nA policy that provides centralized control over permissions for all accounts in an organization in AWS Organizations. SCPs define guardrails or set limits on actions that an administrator can delegate to users or roles. You can use SCPs as allow lists or deny lists, to specify which services or actions are permitted or prohibited. For more information, see Service control policies in the AWS Organizations documentation.\n\nservice endpoint\n\nThe URL of the entry point for an AWS service. You can use the endpoint to connect programmatically to the target service. For more information, see AWS service endpoints in AWS General Reference.\n\nservice-level agreement (SLA)\n\nAn agreement that clarifies what an IT team promises to deliver to their customers, such as service uptime and performance.\n\nservice-level indicator (SLI)\n\nA measurement of a performance aspect of a service, such as its error rate, availability, or throughput.\n\nservice-level objective (SLO)\n\nA target metric that represents the health of a service, as measured by a service-level indicator.\n\nshared responsibility model\n\nA model describing the responsibility you share with AWS for cloud security and compliance. AWS is responsible for security of the cloud, whereas you are responsible for security in the cloud. For more information, see Shared responsibility model.\n\nSIEM\n\nSee security information and event management system.\n\nsingle point of failure (SPOF)\n\nA failure in a single, critical component of an application that can disrupt the system.\n\nSLA\n\nSee service-level agreement.\n\nSLI\n\nSee service-level indicator.\n\nSLO\n\nSee service-level objective.\n\nsplit-and-seed model\n\nA pattern for scaling and accelerating modernization projects. As new features and product releases are defined, the core team splits up to create new product teams. This helps scale your organization’s capabilities and services, improves developer productivity, and supports rapid innovation. For more information, see Phased approach to modernizing applications in the AWS Cloud.\n\nSPOF\n\nSee single point of failure.\n\nstar schema\n\nA database organizational structure that uses one large fact table to store transactional or measured data and uses one or more smaller dimensional tables to store data attributes. This structure is designed for use in a data warehouse or for business intelligence purposes.\n\nstrangler fig pattern\n\nAn approach to modernizing monolithic systems by incrementally rewriting and replacing system functionality until the legacy system can be decommissioned. This pattern uses the analogy of a fig vine that grows into an established tree and eventually overcomes and replaces its host. The pattern was introduced by Martin Fowler as a way to manage risk when rewriting monolithic systems. For an example of how to apply this pattern, see Modernizing legacy Microsoft ASP.NET (ASMX) web services incrementally by using containers and Amazon API Gateway.\n\nsubnet\n\nA range of IP addresses in your VPC. A subnet must reside in a single Availability Zone.\n\nsupervisory control and data acquisition (SCADA)\n\nIn manufacturing, a system that uses hardware and software to monitor physical assets and production operations.\n\nsymmetric encryption\n\nAn encryption algorithm that uses the same key to encrypt and decrypt the data.\n\nsynthetic testing\n\nTesting a system in a way that simulates user interactions to detect potential issues or to monitor performance. You can use Amazon CloudWatch Synthetics to create these tests.\n\nT\ntags\n\nKey-value pairs that act as metadata for organizing your AWS resources. Tags can help you manage, identify, organize, search for, and filter resources. For more information, see Tagging your AWS resources.\n\ntarget variable\n\nThe value that you are trying to predict in supervised ML. This is also referred to as an outcome variable. For example, in a manufacturing setting the target variable could be a product defect.\n\ntask list\n\nA tool that is used to track progress through a runbook. A task list contains an overview of the runbook and a list of general tasks to be completed. For each general task, it includes the estimated amount of time required, the owner, and the progress.\n\ntest environment\n\nSee environment.\n\ntraining\n\nTo provide data for your ML model to learn from. The training data must contain the correct answer. The learning algorithm finds patterns in the training data that map the input data attributes to the target (the answer that you want to predict). It outputs an ML model that captures these patterns. You can then use the ML model to make predictions on new data for which you don’t know the target.\n\ntransit gateway\n\nA network transit hub that you can use to interconnect your VPCs and on-premises networks. For more information, see What is a transit gateway in the AWS Transit Gateway documentation.\n\ntrunk-based workflow\n\nAn approach in which developers build and test features locally in a feature branch and then merge those changes into the main branch. The main branch is then built to the development, preproduction, and production environments, sequentially.\n\ntrusted access\n\nGranting permissions to a service that you specify to perform tasks in your organization in AWS Organizations and in its accounts on your behalf. The trusted service creates a service-linked role in each account, when that role is needed, to perform management tasks for you. For more information, see Using AWS Organizations with other AWS services in the AWS Organizations documentation.\n\ntuning\n\nTo change aspects of your training process to improve the ML model's accuracy. For example, you can train the ML model by generating a labeling set, adding labels, and then repeating these steps several times under different settings to optimize the model.\n\ntwo-pizza team\n\nA small DevOps team that you can feed with two pizzas. A two-pizza team size ensures the best possible opportunity for collaboration in software development.\n\nU\nuncertainty\n\nA concept that refers to imprecise, incomplete, or unknown information that can undermine the reliability of predictive ML models. There are two types of uncertainty: Epistemic uncertainty is caused by limited, incomplete data, whereas aleatoric uncertainty is caused by the noise and randomness inherent in the data. For more information, see the Quantifying uncertainty in deep learning systems guide.\n\nundifferentiated tasks\n\nAlso known as heavy lifting, work that is necessary to create and operate an application but that doesn’t provide direct value to the end user or provide competitive advantage. Examples of undifferentiated tasks include procurement, maintenance, and capacity planning.\n\nupper environments\n\nSee environment.\n\nV\nvacuuming\n\nA database maintenance operation that involves cleaning up after incremental updates to reclaim storage and improve performance.\n\nversion control\n\nProcesses and tools that track changes, such as changes to source code in a repository.\n\nVPC peering\n\nA connection between two VPCs that allows you to route traffic by using private IP addresses. For more information, see What is VPC peering in the Amazon VPC documentation.\n\nvulnerability\n\nA software or hardware flaw that compromises the security of the system.\n\nW\nwarm cache\n\nA buffer cache that contains current, relevant data that is frequently accessed. The database instance can read from the buffer cache, which is faster than reading from the main memory or disk.\n\nwarm data\n\nData that is infrequently accessed. When querying this kind of data, moderately slow queries are typically acceptable.\n\nwindow function\n\nA SQL function that performs a calculation on a group of rows that relate in some way to the current record. Window functions are useful for processing tasks, such as calculating a moving average or accessing the value of rows based on the relative position of the current row.\n\nworkload\n\nA collection of resources and code that delivers business value, such as a customer-facing application or backend process.\n\nworkstream\n\nFunctional groups in a migration project that are responsible for a specific set of tasks. Each workstream is independent but supports the other workstreams in the project. For example, the portfolio workstream is responsible for prioritizing applications, wave planning, and collecting migration metadata. The portfolio workstream delivers these assets to the migration workstream, which then migrates the servers and applications.\n\nWORM\n\nSee write once, read many.\n\nWQF\n\nSee AWS Workload Qualification Framework.\n\nwrite once, read many (WORM)\n\nA storage model that writes data a single time and prevents the data from being deleted or modified. Authorized users can read the data as many times as needed, but they cannot change it. This data storage infrastructure is considered immutable.\n\nZ\nzero-day exploit\n\nAn attack, typically malware, that takes advantage of a zero-day vulnerability.\n\nzero-day vulnerability\n\nAn unmitigated flaw or vulnerability in a production system. Threat actors can use this type of vulnerability to attack the system. Developers frequently become aware of the vulnerability as a result of the attack.\n\nzombie application\n\nAn application that has an average CPU and memory usage below 5 percent. In a migration project, it is common to retire these applications."
  },
  {
    "title": "Additional resources - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/resources.html",
    "html": "Additional resources\nPDF\nRSS\n\nMore information about DynamoDB\n\nDynamoDB pricing\n\nDynamoDB documentation\n\nNoSQL design for DynamoDB\n\nWrite sharding\n\nLocal secondary indexes (LSIs)\n\nGlobal secondary indexes (GSIs)\n\nOverloading GSIs\n\nGSI sharding\n\nUsing GSIs to create an eventually consistent replica\n\nSparse indexes\n\nMaterialized aggregation queries\n\nTime series design pattern\n\nAdjacency list design pattern\n\nOn-demand and provisioned capacity models\n\nDynamoDB auto scaling\n\nDynamoDB Time to Live (TTL)\n\nModeling game player data with DynamoDB (lab)\n\nAWS services\n\nAWS CloudFormation\n\nAmazon S3\n\nTools\n\nAWS Pricing Calculator\n\nNoSQL Workbench for DynamoDB\n\nDynamoDB Local\n\nDynamoDB and AWS SDKs\n\nBest practices\n\nBest practices for designing and architecting with DynamoDB (DynamoDB documentation)\n\nBest practices for using secondary indexes (DynamoDB documentation)\n\nBest practices for storing large items and attributes (DynamoDB documentation)\n\nChoosing the right DynamoDB partition key (AWS Database blog)\n\nHow to design Amazon DynamoDBglobal secondary indexes (AWS Database blog)\n\nWhat are facets in NoSQL Workbench for Amazon DynamoDB (Medium website)\n\nAWS general resources\n\nAWS Prescriptive Guidance website\n\nAWS documentation\n\nAWS general reference"
  },
  {
    "title": "Step 9: Deploy the data model - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step9-hierarchical-model.html",
    "html": "Step 9: Deploy the data model\nPDF\nRSS\n\nFor this specific example, the deployment of the model was done using NoSQL Workbench, an application for modern database development and operation. Using this tool, you have the option of creating a data model, uploading data, and deploying it directly to your AWS account. If you want to implement this example, you can use the following AWS CloudFormation template, which was generated by NoSQL Workbench.\n\nAWSTemplateFormatVersion: 2010-09-09\nResources:\n  Components:\n    Type: 'AWS::DynamoDB::Table'\n    Properties:\n      KeySchema:\n        - AttributeName: ComponentId\n          KeyType: HASH\n      AttributeDefinitions:\n        - AttributeName: ComponentId\n          AttributeType: S\n        - AttributeName: ParentId\n          AttributeType: S\n        - AttributeName: GraphId\n          AttributeType: S\n        - AttributeName: Path\n          AttributeType: S\n      GlobalSecondaryIndexes:\n        - IndexName: GS1\n          KeySchema:\n            - AttributeName: ParentId\n              KeyType: HASH\n            - AttributeName: ComponentId\n              KeyType: RANGE\n          Projection:\n            ProjectionType: KEYS_ONLY\n        - IndexName: GSI2\n          KeySchema:\n            - AttributeName: GraphId\n              KeyType: HASH\n            - AttributeName: Path\n              KeyType: RANGE\n          Projection:\n            ProjectionType: INCLUDE\n            NonKeyAttributes:\n              - ComponentId\n      BillingMode: PAY_PER_REQUEST\n      TableName: Components\n"
  },
  {
    "title": "Step 8: Review the cost estimation - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step8-hierarchical-model.html",
    "html": "Step 8: Review the cost estimation\nPDF\nRSS\n\nReview and refine the cost estimation again. Additionally, it's a good practice to validate it with business stakeholders and get approval to move to the next step.\n\nObjectives\n\nDefine the capacity model, and estimate DynamoDB costs to refine the cost estimation from step 2.\n\nGet the final financial approval from the business analyst and stakeholders.\n\nProcess\n\nDatabase engineer identifies the data volume estimate.\n\nDatabase engineer identifies the data transfer requirements.\n\nDatabase engineer defines the required read and write capacity units.\n\nBusiness analyst decides between on-demand and provisioned capacity models.\n\nDatabase engineer identifies the need for DynamoDB auto scaling.\n\nDatabase engineer inputs the parameters in the AWS Pricing Calculator.\n\nDatabase engineer presents the final price estimation to business stakeholders.\n\nBusiness analyst and stakeholders approve or reject the solution."
  },
  {
    "title": "Step 6: Create data queries - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step6-hierarchical-model.html",
    "html": "Step 6: Create data queries\nPDF\nRSS\n\nAfter you define your access patterns and design your data model, you can query hierarchical data in the DynamoDB database. As a best practice to save on costs and help ensure performance, the following examples use only the query operation without Scan.\n\nFind ancestors of a component.\n\nTo find the ancestors (parent, grandparent, great-grandparent, and so on) of the CM8 component, query the base table using ComponentId = \"CM8\". The query will return the following record.\n\nTo reduce the size of the result data, you can use a projection expression to return only the Path attribute.\n\nComponentId\n\n\t\n\nParentId\n\n\t\n\nGraphId\n\n\t\n\nPath\n\n\n\n\nCM8\n\n\t\n\nCM4\n\n\t\n\nCM1#1\n\n\t\n\nCM1|CM2|CM4|CM8\n\nPath\n\n\n\n\nCM1|CM2|CM4|CM8\n\nNow, split the path using the pipe (\"|\"), and take the first N-1 components to get ancestors.\n\nQuery result: The ancestors of CM8 are CM1, CM2, CM4.\n\nFind immediate children of a component.\n\nTo get all immediate child, or one-level downstream, components for the CM2 component, query GSI1 using ParentId = \"CM2\". The query will return the following record.\n\nParentId\n\n\t\n\nComponentId\n\n\n\n\nCM2\n\n\t\n\nCM4\n\nCM5\n\nFind all downstream child components using a top-level component.\n\nTo get all child, or downstream, components for top-level component CM1, query GSI2 using GraphId = \"CM1#1\" and begins_with(\"Path\", \"CM1|\"), and use a projection expression with ComponentId. It will return all the components related to that tree.\n\nThis example has a single tree, with CM1 as the top component. In reality, you could have millions of top-level components in the same table.\n\nGraphId\n\n\t\n\nComponentId\n\n\n\n\n \n\nCM1#1\n\n\t\n\nCM2\n\nCM3\n\nCM4\n\nCM5\n\nCM8\n\nCM9\n\nCM10\n\nCM6\n\nCM7\n\nFind all downstream child components using a middle-level component.\n\nTo get all child, or downstream, components recursively for component CM2, you have two options. You can query recursively level by level, or you can query the GSI2 index.\n\nQuery GSI1, level by level, recursively, until reaching the last level of child components.\n\nQuery GSI1 using ParentId = \"CM2\". It will return the following record.\n\nParentId\n\n\t\n\nComponentId\n\n\n\n\nCM2\n\n\t\n\nCM4\n\nCM5\n\nAgain, query GSI1 using ParentId = \"CM4\". It will return the following record.\n\nParentId\n\n\t\n\nComponentId\n\n\n\n\nCM4\n\n\t\n\nCM8\n\nCM9\n\nAgain, query GSI1 using ParentId = \"CM5\". It will return the following record.\n\nContinue the loop: Query for each ComponentId until you reach the last level. When a query using ParentId = \"<ComponentId>\" doesn't return any results, the previous result was from the last level of the tree.\n\nParentId\n\n\t\n\nComponentId\n\n\n\n\nCM5\n\n\t\n\nCM10\n\nMerge all results.\n\n \n\nresult=[CM4, CM5] + [CM8, CM9] + [CM10]\n\n         =[CM4, CM5, CM8, CM9, CM10]\n\nQuery GSI2, which stores a hierarchical tree for a top-level component (a car, or CM1).\n\nFirst, find the top-level component or top ancestor and Path of CM2. For that, query the base table by using ComponentId = \"CM2\" to find the path of that component in the hierarchical tree. Select the GraphId and Path attributes. The query will return the following record.\n\nGraphId\n\n\t\n\nPath\n\n\n\n\nCM1#1\n\n\t\n\nCM1|CM2\n\nQuery GSI2 by using GraphId = \"CM1#1\" AND BEGINS_WITH(\"Path\", \"CM1|CM2|\"). The query will return the following results.\n\nGraphId\n\n\t\n\nPath\n\n\t\n\nComponentId\n\n\n\n\nCM1#1\n\n\t\n\nCM1|CM2|CM4\n\nCM1|CM2|CM5\n\nCM1|CM2|CM4|CM8\n\nCM1|CM2|CM4|CM9\n\nCM1|CM2|CM5|CM10\n\n\t\n\nCM4\n\nCM5\n\nCM8\n\nCM9\n\nCM10\n\nSelect the ComponentId attribute to return all the child components for CM2."
  },
  {
    "title": "Step 5: Create a DynamoDB data model - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step5-hierarchical-model.html",
    "html": "Step 5: Create a DynamoDB data model\nPDF\nRSS\n\nDefine the partition keys for your base table and global secondary indexes (GSIs):\n\nFollowing the key design best practices, use ComponentId as the partition key for the base table in this example. Because it's unique, ComponentId can offer granularity. DynamoDB uses the hash value of your partition key to determine the partition where the data is stored physically. The unique component ID generates a different hash value, which can facilitate distribution of data inside the table. You can query the base table by using a ComponentId partition key.\n\nTo find immediate children of a component, create a GSI where ParentId is the partition key, and ComponentId is the sort key. You can query this GSI by using ParentId as the partition key.\n\nTo find all recursive children of a component, create a GSI where GraphId is the partition key, and Path is the sort key. You can query this GSI by using GraphId as the partition key and the BEGINS_WITH(Path, \"$path\") operator on the sort key.\n\n\t\n\nPartition key\n\n\t\n\nSort Key\n\n\t\n\nMapping attributes\n\n\n\n\nBase table\n\n\t\n\nComponentId\n\n\t\t\n\nParentId, GraphId, Path\n\n\n\n\nGSI1\n\n\t\n\nParentId\n\n\t\n\nComponentId\n\n\t\n\n\nGSI2\n\n\t\n\nGraphId\n\n\t\n\nPath\n\n\t\n\nComponentId\n\nStoring components in the table\n\nThe next step is to store each component in the DynamoDB base table. After you insert all the components from the example tree, you get the following base table.\n\nComponentId\n\n\t\n\nParentId\n\n\t\n\nGraphId\n\n\t\n\nPath\n\n\n\n\n \n\nCM1\n\n\t\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1\n\n\n\n\n \n\nCM2\n\n\t\n\n \n\nCM1\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM2\n\n\n\n\n \n\nCM3\n\n\t\n\n \n\nCM1\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM3\n\n\n\n\n \n\nCM4\n\n\t\n\n \n\nCM2\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM2|CM4\n\n\n\n\n \n\nCM5\n\n\t\n\n \n\nCM2\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM2|CM5\n\n\n\n\n \n\nCM6\n\n\t\n\n \n\nCM3\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM3|CM6\n\n\n\n\n \n\nCM7\n\n\t\n\n \n\nCM3\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM3|CM7\n\n\n\n\n \n\nCM8\n\n\t\n\n \n\nCM4\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM2|CM4|CM8\n\n\n\n\n \n\nCM9\n\n\t\n\n \n\nCM4\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM2|CM4|CM9\n\n\n\n\n \n\nCM10\n\n\t\n\n \n\nCM5\n\n\t\n\n \n\nCM1#1\n\n\t\n\n \n\nCM1|CM2|CM5|CM10\n\nThe GSI1 index\n\nTo check all immediate children of a component, you create an index that uses ParentId as a partition key and ComponentId as a sort key. The following pivot table represents the GSI1 index. You can use this index to retrieve all immediate child components by using a parent component ID. For example, you can find out how many batteries are available in a car (CM1) or which cells are available in a module (CM4).\n\nParentId\n\n\t\n\nComponentId\n\n\n\n\nCM1\n\n\t\n\nCM2\n\nCM3\n\n\n\n\nCM2\n\n\t\n\nCM4\n\nCM5\n\n\n\n\nCM3\n\n\t\n\nCM6\n\nCM7\n\n\n\n\nCM4\n\n\t\n\nCM8\n\nCM9\n\n\n\n\nCM5\n\n\t\n\nCM10\n\nThe GSI2 index\n\nThe following pivot table represents the GSI2 index. It's configured using GraphId as a partition key and Path as a sort key. Using GraphId and the begins_with operation on the sort key (Path), you can find the full lineage of a component in a tree.\n\nGraphId\n\n\t\n\nPath\n\n\t\n\nComponentId\n\n\n\n\nCM1#1\n\n\t\n\nCM1\n\nCM1|CM2\n\nCM1|CM3\n\nCM1|CM2|CM4\n\nCM1|CM2|CM5\n\nCM1|CM2|CM4|CM8\n\nCM1|CM2|CM4|CM9\n\nCM1|CM2|CM5|CM10\n\nCM1|CM3|CM6\n\nCM1|CM3|CM7\n\n\t\n\nCM1\n\nCM2\n\nCM3\n\nCM4\n\nCM5\n\nCM8\n\nCM9\n\nCM10\n\nCM6\n\nCM7"
  },
  {
    "title": "Step 4: Identify the technical requirements - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step4-hierarchical-model.html",
    "html": "Step 4: Identify the technical requirements\nPDF\nRSS\n\nThis example doesn't have any specific technical requirements, which are outside the scope of this example. In real cases, it's a best practice to complete this step and to validate that all technical requirements are fulfilled before proceeding with development and deployment. You can use the example questionnaire to complete this step in your business case. Additionally, we recommend validating the DynamoDB service quotas to make sure that there are no hard limits in your designed solution."
  },
  {
    "title": "Step 3: Identify your data-access patterns - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step3-hierarchical-model.html",
    "html": "Step 3: Identify your data-access patterns\nPDF\nRSS\n\nThis example use case has the following access patterns for managing relationships between different car components.\n\nAccess pattern\n\n\t\n\nPriority\n\n\t\n\nRead or write\n\n\t\n\nDescription\n\n\t\n\nType\n\n\t\n\nFilters\n\n\t\n\nResult ordering\n\n\n\n\nImmediate child\n\n\t\n\nHigh\n\n\t\n\nRead\n\n\t\n\nRetrieve all the immediate child components for a parent component ID.\n\n\t\n\nMultiple\n\n\t\n\nComponentID\n\n\t\n\nN/A\n\n\n\n\nAll child components\n\n\t\n\nHigh\n\n\t\n\nRead\n\n\t\n\nRetrieve a recursive list of all child components for a component ID.\n\n\t\n\nMultiple\n\n\t\n\nComponentID\n\n\t\n\nN/A\n\n\n\n\nAncestors\n\n\t\n\nHigh\n\n\t\n\nRead\n\n\t\n\nRetrieve the ancestors of a component.\n\n\t\n\nMultiple\n\n\t\n\nComponentID\n\n\t\n\nN/A"
  },
  {
    "title": "Step 1: Identify the use cases and logical data model - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step1-hierarchical-model.html",
    "html": "Step 1: Identify the use cases and logical data model\nPDF\nRSS\n\nAn automotive company wants to build a transactional component management system to store and search for all of the available car parts and to build relationships between different components and parts. For example, a car contains multiple batteries, each battery contains multiple high-level modules, each module contains multiple cells, and each cell contains multiple low-level components.\n\nGenerally, for building a hierarchical relationship model, a graph database such as Amazon Neptune is a better choice. In some cases, however, Amazon DynamoDB is a better alternative for hierarchical data modeling because of its flexibility, security, performance, and scale.\n\nFor example, you might build a system where 80–90 percent of the queries are transactional, where DynamoDB fits well. In this example, the other 10–20 percent of queries are relational, where a graph database such as Neptune fits better. In this case, including an additional database in the architecture to fulfill only 10–20 percent of the queries could increase cost. It also adds the operational burden of maintaining multiple systems and synchronizing the data. Instead, you can model that 10–20 percent relational queries in DynamoDB.\n\nDiagramming an example tree for car components can help you map the relationship between them. The following diagram shows a dependency graph with four levels. CM1 is the top-level component for the example car itself. It has two subcomponents for two example batteries, CM2 and CM3. Each battery has two subcomponents, which are the modules. CM2 has modules CM4 and CM5, and CM3 has modules CM6 and CM7. Each module has several subcomponents, which are the cells. The CM4 module has two cells, CM8 and CM9. CM5 has one cell, CM10. CM6 and CM7 don't have any associated cells yet.\n\nThis guide will use this tree and its component identifiers as a reference. A top component will be referred to as a parent, and a subcomponent will be referred to as a child. For example, top component CM1 is the parent of CM2 and CM3. CM2 is the parent of CM4 and CM5. This graphs the parent-child relationships.\n\nFrom the tree, you can see the complete dependency graph of a component. For example, CM8 is dependent on CM4, which is dependent on CM2, which is dependent on CM1. The tree defines the complete dependency graph as the path. A path describes two things:\n\nThe dependency graph\n\nThe position in the tree\n\nFilling the templates for business requirements:\n\nProvide information about your users:\n\nUser\n\n\t\n\nDescription\n\n\n\n\nEmployee\n\n\t\n\nInternal employee of the automotive company that needs information of cars and its components\n\nProvide information about the sources of data and how data will be ingested:\n\nSource\n\n\t\n\nDescription\n\n\t\n\nUser\n\n\n\n\nManagement system\n\n\t\n\nSystem that will store all the data related to available car parts and their relationships with other components and parts.\n\n\t\n\nEmployee\n\nProvide information about how data will be consumed:\n\nConsumer\n\n\t\n\nDescription\n\n\t\n\nUser\n\n\n\n\nManagement system\n\n\t\n\nRetrieve all the immediate child components for a parent component ID.\n\n\t\n\nEmployee\n\n\n\n\nManagement system\n\n\t\n\nRetrieve a recursive list of all child components for a component ID.\n\n\t\n\nEmployee\n\n\n\n\nManagement system\n\n\t\n\nSee the ancestors of a component.\n\n\t\n\nEmployee"
  },
  {
    "title": "Best practices - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/best-practices.html",
    "html": "Best practices\nPDF\nRSS\n\nConsider using the following DynamoDB design best practices:\n\nPartition key design – Use a high-cardinality partition key to distribute load evenly.\n\nAdjacency list design pattern – Use this design pattern for managing one-to-many and many-to-many relationships.\n\nSparse index – Use sparse index for your global secondary indexes (GSIs). When you create a GSI, you specify a partition key and optionally a sort key. Only items in the base table that contain a corresponding GSI partition key appear in the sparse index. This helps to keep GSIs smaller.\n\nIndex overloading – Use the same GSI for indexing various types of items.\n\nGSI write sharding – Shard wisely to distribute data across the partitions for efficient and faster queries.\n\nLarge items – Store only metadata inside the table, save the blob in Amazon S3, and keep the reference in DynamoDB. Break large items into multiple items, and efficiently index by using sort keys.\n\nFor more design best practices, see the Amazon DynamoDB documentation."
  },
  {
    "title": "Access-patterns template - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/template-access-patterns.html",
    "html": "Access-patterns template\nPDF\nRSS\n\nCollect and document information about the access patterns for the use case by using the following fields:\n\nField\tDescription\n\n\nAccess pattern\n\n\t\n\nProvide a name for the access pattern.\n\n\n\n\nDescription\n\n\t\n\nProvide a more detailed description of the access pattern.\n\n\n\n\nPriority\n\n\t\n\nDefine a priority for the access pattern (high, medium, or low). This defines the most relevant access patterns for the application.\n\n\n\n\nRead or write\n\n\t\n\nIs it a read access or write access pattern?\n\n\n\n\nType\n\n\t\n\nDoes the pattern access a single item, multiple items, or all items?\n\n\n\n\nFilter\n\n\t\n\nDoes the access pattern require any filter?\n\n\n\n\nSort\n\n\t\n\nDoes the result require any sorting?\n\nTemplate\n\nAccess pattern\n\n\t\n\nDescription\n\n\t\n\nPriority\n\n\t\n\nRead or write\n\n\t\n\nType (single item,\n\nmultiple\n\nitems, or all)\n\n\t\n\nKey attribute\n\n\t\n\n Filters\n\n\t\n\nResult ordering\n\n\n\n\nCreate user profile\n\n\t\n\nUser creates a new profile.\n\n\t\n\nHigh\n\n\t\n\n Write\n\n\t\n\nSingle item\n\n\t\n\nUsername\n\n\t\n\n N/A\n\n\t\n\n N/A\n\n\n\n\nUpdate user profile\n\n\t\n\nUser updates their profile.\n\n\t\n\nMedium\n\n\t\n\nWrite\n\n\t\n\nSingle item\n\n\t\n\nUsername\n\n\t\n\nUsername = current user\n\n\t\n\nN/A\n\n\n\n\nGet user profile\n\n\t\n\nUser reviews their profile.\n\n\t\n\nHigh\n\n\t\n\nRead\n\n\t\n\nSingle item\n\n\t\n\nUsername\n\n\t\n\nUsername = current user\n\n\t\n\nN/A\n\n\n\n\nCreate a game\n\n\t\n\nUser creates a new game.\n\n\t\n\nHigh\n\n\t\n\nWrite\n\n\t\n\nSingle item\n\n\t\n\nGameID\n\n\t\n\nN/A\n\n\t\n\nN/A\n\n\n\n\nFind open games\n\n\t\n\nUser searches for open games. Search results are  sorted by start timestamp in descending order.\n\n\t\n\nHigh\n\n\t\n\nRead\n\n\t\n\nMultiple items\n\n\t\n\n \n\n\t\n\nGameStatus = open\n\n\t\n\nStart timestamp descendent\n\n\n\n\nFind open games by map\n\n\t\n\nUser searches for open games by using a specific map sorted by start timestamp in descending\n\norder.\n\n\t\n\nMedium\n\n\t\n\nRead\n\n\t\n\nMultiple items\n\n\t\n\n \n\n\t\n\nGameStatus = open and Map = XYZ\n\n\t\n\nStart timestamp descendent\n\n\n\n\nView game\n\n\t\n\nUser reviews the details of a game.\n\n\t\n\nHigh\n\n\t\n\nRead\n\n\t\n\nSingle item\n\n\t\n\nGameID\n\n\t\n\nN/A\n\n\t\n\nN/A\n\n\n\n\nView users in a game\n\n\t\n\nUser gets a list of all the users in a game.\n\n\t\n\nMedium\n\n\t\n\nRead\n\n\t\n\nMultiple items\n\n\t\n\n \n\n\t\n\nGameID = XYZ\n\n\t\n\nN/A\n\n\n\n\nJoin user to a game\n\n\t\n\nUser joins an open game.\n\n\t\n\nHigh\n\n\t\n\nWrite\n\n\t\n\nSingle item\n\n\t\n\nGameID and Username\n\n\t\n\nGameStatus = open\n\n\t\n\nN/A\n\n\n\n\nStart a game\n\n\t\n\nUser starts a new game.\n\n\t\n\nHigh\n\n\t\n\nWrite\n\n\t\n\nSingle item\n\n\t\n\nGameID\n\n\t\n\nN/A\n\n\t\n\nN/A\n\n\n\n\nUpdate game for user\n\n\t\n\nUpdate user position in the game.\n\n\t\n\nMedium\n\n\t\n\nWrite\n\n\t\n\nSingle item\n\n\t\n\nGameID and Username\n\n\t\n\nN/A\n\n\t\n\nN/A\n\n\n\n\nUpdate game\n\n\t\n\nGame ends; update statistics.\n\n\t\n\nMedium\n\n\t\n\nWrite\n\n\t\n\nSingle item\n\n\t\n\nGameID\n\n\t\n\nN/A\n\n\t\n\nN/A\n\n\n\n\nFind all past games for a user\n\n\t\n\nList all games that a user played ordered by the start timestamp of the game.\n\n\t\n\nLow\n\n\t\n\nRead\n\n\t\n\nMultiple items\n\n\t\n\nUsername and GameID\n\n\t\n\nUsername = current user\n\n\t\n\nStart timestamp\n\n\n\n\nExport data for data analytics\n\n\t\n\nDevelopment team will run a batch job to export data to Amazon S3.\n\n\t\n\nLow\n\n\t\n\nRead\n\n\t\n\nAll\n\n\t\n\nN/A\n\n\t\n\nN/A\n\n\t\n\nN/A"
  },
  {
    "title": "Technical-requirements assessment template - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/template-tech-req.html",
    "html": "Technical-requirements assessment template\nPDF\nRSS\n\nProvide information about data ingestion types:\n\nData ingestion type\n\n\t\n\nY/N\n\n\t\n\nDescription\n\n\t\n\nFrequency\n\n\n\n\nApplication access\n\n\t\n\nY\n\n\t\n\n \n\n\t\n\n \n\n\n\n\nAPI gateway\n\n\t\n\nY\n\n\t\n\n \n\n\t\n\n \n\n\n\n\nData streaming\n\n\t\n\nN\n\n\t\n\n \n\n\t\n\n \n\n\n\n\nBatch process\n\n\t\n\nN\n\n\t\n\n \n\n\t\n\n \n\n\n\n\nETL\n\n\t\n\nN\n\n\t\n\n \n\n\t\n\n \n\n\n\n\nData import\n\n\t\n\nN\n\n\t\n\n \n\n\t\n\n \n\n\n\n\nTime series\n\n\t\n\nN\n\n\t\n\n \n\n\t\n\n \n\nProvide information about data consumption types:\n\nData consumption type\n\n\t\n\nY/N\n\n\t\n\nDescription\n\n\t\n\nFrequency\n\n\n\n\nApplication access\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\n\n\n\nAPI gateway\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\n\n\n\nData export\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\n\n\n\nData analytics\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\n\n\n\nData aggregation\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\n\n\n\nReporting\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\n\n\n\nSearch\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\n\n\n\nData streaming\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\n\n\n\nETL\n\n\t\n\n \n\n\t\n\n \n\n\t\n\n \n\nProvide data volume estimates:\n\nEntity name\n\n\t\n\nEstimated # of records\n\n\t\n\nRecord size\n\n\t\n\nData volume\n\n\n\n\nGame Player\n\n\t\n\n1 MM\n\n\t\n\n< 1 KB\n\n\t\n\n~ 1 GB\n\n (1 MM * 1 KB)\n\n\n\n\nGame Instance\n\n\t\n\n6 MM\n\n(100K/day * 60 days)\n\n\t\n\n< 1 KB\n\n\t\n\n~ 6 GB\n\n (6 MM * 1 KB)\n\n\n\n\nGame User Mapping\n\n\t\n\n300 MM\n\n(6 MM games * 50 players)\n\n\t\n\n< 1 KB\n\n\t\n\n~ 300 GB\n\n (300 MM * 1 KB)\n\nNote\n\nThe period for data retention is 60 days. After 60 days, data must be stored in Amazon S3 for analytics, by using DynamoDB Time to Live (TTL) to automatically move data out of DynamoDB to Amazon S3.\n\nAnswer these questions about time patterns:\n\nWhat time frame is the application available to the user (for example, 24/7 or 9 AM to 5 PM on weekdays)?\n\nIs there a peak in usage during the day? How many hours? What is the percentage of application usage?\n\nSpecify write throughput requirements:\n\nEntity name\n\n\t\n\nWrites/day\n\n\t\n\nHours/day\n\n\t\n\nWrites/second\n\n\n\n\nGame Player\n\n\t\n\n10,000 updates\n\n\t\n\n18\n\n\t\n\n< 1\n\n\n\n\nGame Instance\n\n\t\n\n300,000\n\n\t\n\n18\n\n\t\n\n< 5\n\n\n\n\nGame User Mapping\n\n\t\n\n1,800,000,000\n\n\t\n\n18\n\n\t\n\n~ 27.777\n\nNotes\n\nGame Player write operations: 1 percent of users update their profiles every day, so we expect 10,000 updates for 1,000,000 users.\n\nGame Instance write operations: 100,000 games/day. For each game we have at least 3 write operations—at creation, start, and end—so the total is 300,000 write operations.\n\nGame User Mapping write operations: 100,000 games/day for each game with 50 players. The average game duration is 30 minutes, and the gamer position is updated every 5 seconds. We estimate an average of 360 updates per gamer, so the total is 100,000 * 50 * 360 = 1,800,000,000 write operations.\n\nSpecify read throughput requirements:\n\nEntity name\n\n\t\n\nReads / day\n\n\t\n\nHours / day\n\n\t\n\nReads/sec\n\n\n\n\nGame Player\n\n\t\n\n200,000\n\n\t\n\n18\n\n\t\n\n~ 3\n\n\n\n\nGame Instance\n\n\t\n\n5,000,000\n\n\t\n\n18\n\n\t\n\n~ 77\n\n\n\n\nGame User Mapping\n\n\t\n\n1,800,000,000\n\n\t\n\n18\n\n\t\n\n~ 27.777\n\nNotes\n\nGame Player read operations: 20 percent of users start games, so 1 MM * 0.2 = 200,000.\n\nGame Instance read operations: 100,000 games/day. For each game we have at least 1 read operation per player, and 50 players per game, so the total is 5,000,000 read operations.\n\nGame User Mapping read operations: 100,000 games/day for 50 players. The average game duration is 30 minutes, and the gamer position is updated every 5 seconds. We estimate an average of 360 updates per gamer, and each update requires a read operation, so the total is 100,000 * 50 * 360 = 1,800,000,000 read operations.\n\nSpecify data access latency requirements:\n\nOperation\n\n\t\n\n99 percentiles\n\n\t\n\nMaximum latency\n\n\n\n\nRead\n\n\t\n\n30 ms\n\n\t\n\n100 ms\n\n\n\n\nWrite\n\n\t\n\n10 ms\n\n\t\n\n50 ms\n\nSpecify data availability requirements:\n\nRequirement\n\n\t\n\nY/N\n\n\t\n\nMetric\n\n\t\n\nNotes\n\n\n\n\nHigh availability\n\n\t\n\nY\n\n\t\n\n99.9%\n\n\t\n\n \n\n\n\n\nRTO\n\n\t\n\nY\n\n\t\n\n1 hour\n\n\t\n\nRecovery time objective\n\n\n\n\nRPO\n\n\t\n\nY\n\n\t\n\n1 hour\n\n\t\n\nRecovery point objective\n\n\n\n\nDisaster recovery\n\n\t\n\nN\n\n\t\n\n \n\n\t\n\n \n\n\n\n\nIn-Region data replication\n\n\t\n\nN\n\n\t\n\n \n\n\t\n\n \n\n\n\n\nCross-Region data replication\n\n\t\n\nN\n\n\t\n\n3 sec latency\n\n\t\n\nWhich AWS Regions?\n\nSpecify security requirements:\n\nRequirement\n\n\t\n\nY/N\n\n\t\n\nNotes\n\n\n\n\nSensitive data store\n\n\t\n\nN\n\n\t\n\nProtected health information (PHI), payment card industry (PCI) information, personally identifiable information (PII)?\n\n\n\n\nEncryption at rest\n\n\t\n\nY\n\n\t\n\n \n\n\n\n\nEncryption in transit\n\n\t\n\nY\n\n\t\n\n \n\n\n\n\nClient-side encryption\n\n\t\n\nN\n\n\t\n\n \n\n\n\n\nAny proprietary or third- vendor encryption library\n\n\t\n\nN\n\n\t\n\n \n\n\n\n\nData access logging\n\n\t\n\nN\n\n\t\n\n \n\n\n\n\nData access auditing\n\n\t\n\nN\n\n\t\n\n "
  },
  {
    "title": "Business-requirements assessment template - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/template-business-req.html",
    "html": "Business-requirements assessment template\nPDF\nRSS\n\nProvide a description for the use case:\n\nDescription\n\nImagine that you are building an online multiplayer game. In your game, groups of 50 players join a session to play a game, which typically takes around 30 minutes to play. During the game, you have to update a specific player’s record to indicate the amount of time the player has been playing, their statistics, or whether they won the game. Users want to see earlier games they’ve played, either to view the games’ winners or to watch a replay of each game’s action.\n\nProvide information about your users:\n\nUser\n\n\t\n\nDescription\n\n\t\n\nExpected number\n\n\n\n\nGame player\n\n\t\n\nOnline game player.\n\n\t\n\n1 MM\n\n\n\n\nDevelopment team\n\n\t\n\nInternal team that will use the game statistics to improve the\n\ngame experience.\n\n\t\n\n100\n\nProvide information about the sources of data and how data will be ingested:\n\nSource\n\n\t\n\nDescription\n\n\t\n\nUser\n\n\n\n\nOnline game\n\n\t\n\nGame players will create profiles and start new games.\n\n\t\n\nGame player\n\n\n\n\nGame app\n\n\t\n\nGame app will automatically collect statistics about the games, such as start and end time, number of players, position of each player, and map for the game.\n\n\t\n\n \n\nProvide information about how data will be consumed:\n\nConsumer\n\n\t\n\nDescription\n\n\t\n\nUser\n\n\n\n\nOnline game\n\n\t\n\nGame players will view profiles and review their game statistics.\n\n\t\n\nGame player\n\n\n\n\nData analytics\n\n\t\n\nThe game development team will extract game statistics for data analysis and to improve the user experience. Data will be exported from the data store and imported into Amazon S3 to support analytics through a Spark application.\n\n\t\n\nDevelopment team\n\nProvide a list of entities and how they are identified:\n\nEntity name\n\n\t\n\nDescription\n\n\t\n\nIdentifier\n\n\n\n\nGame Player\n\n\t\n\nStores information such as identification, address, demographics, interests for each user (gamer).\n\n\t\n\nUsername\n\n\n\n\nGame Instance\n\n\t\n\nProvides information about each game played, including creator, start, end, and map Yplayed.\n\n\t\n\nGame ID\n\n\n\n\nGame User Mapping\n\n\t\n\nRepresents the many-to-many relationships between users and games.\n\n\t\n\nGame ID AND Username\n\nCreate an ER model for the entities:\n\nProvide high-level statistics about the entities:\n\nEntity Name\n\n\t\n\nEstimated # of records\n\n\t\n\nRecord size\n\n\t\n\nNotes\n\n\n\n\nGame Player\n\n\t\n\n1 MM\n\n\t\n\n< 1 KB\n\n\t\n\nThe game platform has about 1 MM users.\n\n\n\n\nGame Instance\n\n\t\n\n6 MM\n\n(100,000K/day * 60 days)\n\n\t\n\n< 1 KB\n\n\t\n\nOn average, there are 100K games every day. We need to store the last 60 days.\n\n\n\n\nGame User Mapping\n\n\t\n\n300 MM\n\n(6 MM games * 50 players)\n\n\t\n\n< 1 KB\n\n\t\n\nOn average, each game has 50 players that we need to store information about."
  },
  {
    "title": "Templates - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/templates.html",
    "html": "Templates\nPDF\nRSS\n\nThe templates provided in this section are based on the Modeling Game Player Data with Amazon DynamoDB on the AWS website.\n\nNote\n\nThe tables in this section use MM as an abbreviation for million, and K as an abbreviation for thousand.\n\nTopics\nBusiness-requirements assessment template\nTechnical-requirements assessment template\nAccess-patterns template"
  },
  {
    "title": "Step 9. Deploy the data model - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step9.html",
    "html": "Step 9. Deploy the data model\nPDF\nRSS\nObjective\n\nDeploy the DynamoDB table (or tables) to the AWS Region.\n\nProcess\n\nDevOps architect creates an AWS CloudFormation template or other infrastructure as code (IaC) tool for the DynamoDB table (or tables). AWS CloudFormation provides an automated way to provision and configure your tables and associated resources.\n\nTools and resources\n\nAWS CloudFormation\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nI\n\n\t\n\nI\n\n\t\n\nC\n\n\t\n\nC\n\n\t\n\n\t\n\nR/A\n\nOutputs\n\nAWS CloudFormation template\n\nExample\n\nmySecondDDBTable:\n   Type: AWS::DynamoDB::\n   Table DependsOn: \"myFirstDDBTable\" \n   Properties:  \n   AttributeDefinitions:\n      -  AttributeName: \"ArtistId\" \n         AttributeType: \"S\"\n      -  AttributeName: \"Concert\" \n         AttributeType: \"S\"\n      -  AttributeName: \"TicketSales\" \n         AttributeType: \"S\"\n   KeySchema:\n      -  AttributeName: \"ArtistId\" \n         KeyType: \"HASH\"\n      -  AttributeName: \"Concert\" \n         KeyType: \"RANGE\"\n   ProvisionedThroughput: \n      ReadCapacityUnits:\n         Ref: \"ReadCapacityUnits\" \n      WriteCapacityUnits:\n         Ref: \"WriteCapacityUnits\" \n   GlobalSecondaryIndexes:\n      -  IndexName: \"myGSI\" \n         KeySchema:\n            - AttributeName: \"TicketSales\" \n            KeyType: \"HASH\"\n         Projection:\n            ProjectionType: \"KEYS_ONLY\" \n         ProvisionedThroughput: \n         ReadCapacityUnits:\n            Ref: \"ReadCapacityUnits\" \n         WriteCapacityUnits:\n            Ref: \"WriteCapacityUnits\" \n   Tags:\n      -  Key: mykey \n         Value: myvalue\n\n   "
  },
  {
    "title": "Step 8. Review the cost estimation - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step8.html",
    "html": "Step 8. Review the cost estimation\nPDF\nRSS\nObjectives\n\nDefine the capacity model and estimate DynamoDB costs to refine the cost estimation from step 2.\n\nGet the final financial approval from the business analyst and stakeholders.\n\nProcess\n\nDatabase engineer identifies the data volume estimate.\n\nDatabase engineer identifies the data transfer requirements.\n\nDatabase engineer defines the required read and write capacity units.\n\nBusiness analyst decides between on-demand and provisioned capacity models.\n\nDatabase engineer identifies the need for DynamoDB auto scaling.\n\nDatabase engineer inputs the parameters in the Simple Monthly Calculator tool.\n\nDatabase engineer presents the final price estimation to business stakeholders.\n\nBusiness analyst and stakeholders approve or reject the solution.\n\nTools and resources\n\nAWS Pricing Calculator\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nC\n\n\t\n\nA\n\n\t\n\nI\n\n\t\n\nR\n\n\t\n\n\t\n\nOutputs\n\nCapacity model\n\nRevised cost estimation"
  },
  {
    "title": "Step 7. Validate the data model - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step7.html",
    "html": "Step 7. Validate the data model\nPDF\nRSS\nObjective\n\nEnsure that the data model will satisfy your requirements.\n\nProcess\n\nDatabase engineer populates the DynamoDB table with sample data.\n\nDatabase engineer runs the code to query the DynamoDB table.\n\nDatabase engineer collects the query results.\n\nDatabase engineer collects the query performance metrics.\n\nBusiness user validates that query results satisfy business needs.\n\nBusiness analysts validate the technical requirements.\n\nTools and resources\n\nAn active AWS account, to gain access to the DynamoDB console\n\nDynamoDB Local (optional), if you want to build the database on your computer without accessing the DynamoDB web service\n\nAWS SDK in your choice of language\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nA\n\n\t\n\nR\n\n\t\n\nI\n\n\t\n\nC\n\n\t\n\n\t\n\nOutputs\n\nApproved data model"
  },
  {
    "title": "Step 5. Create the DynamoDB data model - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step5.html",
    "html": "Step 5. Create the DynamoDB data model\nPDF\nRSS\nObjective\n\nCreate the DynamoDB data model.\n\nProcess\n\nDatabase engineer identifies how many tables will be required for each use case. We recommend maintaining as few tables as possible in a DynamoDB application.\n\nBased on the most common access patterns, identify the primary key that can be one of two types: a primary key with a partition key that identifies data, or a primary key with a partition key and a sort key. A sort key is a secondary key for grouping and organizing data so it can be queried within a partition efficiently. You can use sort keys to define hierarchical relationships in your data that you can query at any level of the hierarchy (see blog post).\n\nPartition key design\n\nDefine the partition key and evaluate its distribution.\n\nIdentify the need for write sharding to distribute workloads evenly.\n\nSort key design\n\nIdentify the sort key.\n\nIdentify the need for a composite sort key.\n\nIdentify the need for version control.\n\nBased on the access patterns, identify the secondary indexes to satisfy the query requirements.\n\nIdentify the need for local secondary indexes (LSIs). These are indexes that have the same partition key as the base table, but a different sort key.\n\nFor tables with LSIs, there is a 10 GB size limit per partition key value. A table with LSIs can store any number of items, as long as the total size for any one partition key value does not exceed 10 GB.\n\nIdentify the need for global secondary indexes (GSIs). These are indexes that have a partition key and a sort key that can be different from those on the base table (see blog post).\n\nDefine the index projections. Consider projecting fewer attributes to minimize the size of items written to the index. In this step, you should determine whether you want to use the following:\n\nSparse indexes\n\nMaterialized aggregation queries\n\nGSI overloading\n\nGSI sharding\n\nAn eventually consistent replica using GSI\n\nDatabase engineer determines whether the data will include large items. If so, they design the solution by using compression or by storing data in Amazon Simple Storage Service (Amazon S3).\n\nDatabase engineer determines whether time series data will be needed. If so, they use the time series design pattern to model the data.\n\nDatabase engineer determines whether the ER model includes many-to-many relationships. If so, they use an adjacency list design pattern to model the data.\n\nTools and resources\n\nNoSQL Workbench for Amazon DynamoDB — Provides data modeling, data visualization, and query development and testing features to help you design your DynamoDB database\n\nNoSQL design for DynamoDB (DynamoDB documentation)\n\nChoosing the Right DynamoDB Partition Key (AWS Database blog)\n\nBest practices for using secondary indexes in DynamoDB (DynamoDB documentation)\n\nHow to design Amazon DynamoDB global secondary indexes (AWS Database blog)\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nI\n\n\t\n\nI\n\n\t\n\nI\n\n\t\n\nR/A\n\n\t\n\n\t\n\nOutputs\n\nDynamoDB table schema that satisfies your access patterns and requirements\n\nExample\n\nThe following screenshot shows NoSQL Workbench."
  },
  {
    "title": "Step 6. Create the data queries - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step6.html",
    "html": "Step 6. Create the data queries\nPDF\nRSS\nObjective\n\nCreate the main queries to validate the data model.\n\nProcess\n\nDatabase engineer manually creates a DynamoDB table in the AWS Region or on their computer (DynamoDB Local).\n\nDatabase engineer adds sample data to the DynamoDB table.\n\nDatabase engineer builds facets using the NoSQL Workbench for Amazon DynamoDB or the AWS SDK for Java or Python to build sample queries (see blog post).\n\nFacets are like a view of the DynamoDB table.\n\nDatabase engineer and cloud developer build sample queries by using the AWS Command Line Interface (AWS CLI) or AWS SDK for the preferred language.\n\nTools and resources\n\nAn active AWS account, to gain access to the DynamoDB console\n\nDynamoDB Local (optional), if you want to build the database on your computer without accessing the DynamoDB web service\n\nNoSQL Workbench for Amazon DynamoDB (download and documentation)\n\nAWS SDK in your choice of language (JavaScript, Python, PHP, .NET, Ruby, Java, Go, Node.js, C++, and SAP ABAP)\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nI\n\n\t\n\nI\n\n\t\n\nI\n\n\t\n\nR/A\n\n\t\n\nR\n\n\t\n\nOutputs\n\nCode to query the DynamoDB table\n\nExamples\n\nDynamoDB examples using the AWS SDK for Java\n\nPython examples\n\nJavaScript examples"
  },
  {
    "title": "Step 4. Identify the technical requirements - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step4.html",
    "html": "Step 4. Identify the technical requirements\nPDF\nRSS\nObjective\n\nGather the technical requirements for the DynamoDB database.\n\nProcess\n\nBusiness analysts interview the business user and DevOps team to gather the technical requirements by using the assessment questionnaire.\n\nTools and resources\n\nTechnical requirements assessment (see example questionnaire)\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nC\n\n\t\n\nC\n\n\t\n\nA\n\n\t\n\nR\n\n\t\n\n\t\n\nOutputs\n\nTechnical requirements document"
  },
  {
    "title": "Step 2. Create a preliminary cost estimation - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/step2.html",
    "html": "Step 2. Create a preliminary cost estimation\nPDF\nRSS\nObjective\n\nDevelop a preliminary cost estimation for DynamoDB.\n\nProcess\n\nDatabase engineer creates the initial cost analysis using available information and the examples presented on the DynamoDB pricing page.\n\nCreate a cost estimate for on-demand capacity (see example).\n\nCreate a cost estimate for provisioned capacity (see example).\n\nFor the provisioned capacity model, get the estimate cost from the calculator and apply discount for reserved capacity.\n\nCompare the estimated costs of the two capacity models.\n\nCreate an estimation for all the environments (Dev, Prod, QA).\n\nBusiness analyst reviews and approves or rejects the preliminary cost estimate.\n\nTools and resources\n\nAWS Pricing Calculator\n\nRACI\nBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\nC\n\n\t\n\nA\n\n\t\n\nI\n\n\t\n\nR\n\n\t\n\n\t\n\nOutputs\n\nPreliminary cost estimation"
  },
  {
    "title": "Modeling data with Amazon DynamoDB - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/welcome.html",
    "html": "Modeling data with Amazon DynamoDB\nPDF\nRSS\n\nAmazon Web Services (AWS)\n\nDecember 2023 (document history)\n\nNoSQL databases provide flexible schemas for building modern applications. They are widely recognized for their ease of development, functionality, and performance at scale. Amazon DynamoDB provides fast and predictable performance with seamless scalability for NoSQL databases in the Amazon Web Services (AWS) Cloud. As a fully managed database service, DynamoDB helps you offload the administrative burdens of operating and scaling a distributed database. You don't have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling.\n\nNoSQL schema design requires a different approach from traditional relational database management system (RDBMS) design. RDBMS data model focuses on the structure of data and its relationships with other data. NoSQL data modeling focuses on access patterns, or how the application is going to consume the data, so it stores the data in a way that supports straightforward query operations. For an RDBMS such as Microsoft SQL Server or IBM Db2, you can create a normalized data model without thinking much about access patterns. You can extend the data model to support your patterns and queries later.\n\nThis guide presents a data modeling process for using DynamoDB that provides functional requirements, performance, and effective costs. The guide is for database engineers who are planning to use DynamoDB as the operational database for their applications that are running on AWS. AWS Professional Services has used the recommended process to help enterprise companies with DynamoDB data modeling for different use cases and workloads."
  },
  {
    "title": "Data-modeling process flow - AWS Prescriptive Guidance",
    "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-data-modeling/process-flow.html",
    "html": "Data-modeling process flow\nPDF\nRSS\n\nWe recommend the following process when modeling data using Amazon DynamoDB. The steps are discussed in detail later in this guide.\n\nRACI matrix\n\nSome organizations use a responsibility assignment matrix (also known as a RACI matrix) to describe the various roles involved in one specific project or business process. This guide presents a suggested RACI matrix that could help your organization identify the right people and right responsibilities for the DynamoDB data modeling process. For each step in the process, it lists the stakeholders and their involvement:\n\nR – responsible for completing the step\n\nA – accountable for approving and signing off on the work\n\nC – consulted to provide input for a task\n\nI – informed of progress, but not directly involved in the task\n\nDepending on the structure of your organization and project team, the roles in the following RACI matrix can be performed by the same stakeholder. In some situations, stakeholders are both responsible and accountable for specific steps. For example, database engineers can be responsible for both creating and approving the data model, because this is their domain area.\n\nProcess step\tBusiness user\tBusiness analyst\tSolutions architect\tDatabase engineer\tApplication developer\tDevOps engineer\n\n\n1. Identify the use cases and logical data model\n\n\t\n\nC\n\n\t\n\nR/A\n\n\t\n\nI\n\n\t\n\nR\n\n\t\n\n\t\n\n\n\n\n2. Create a preliminary cost estimation\n\n\t\n\nC\n\n\t\n\nA\n\n\t\n\nI\n\n\t\n\nR\n\n\t\n\n\t\n\n\n\n\n3. Identify your data access patterns\n\n\t\n\nC\n\n\t\n\nA\n\n\t\n\nI\n\n\t\n\nR\n\n\t\n\n\t\n\n\n\n\n4. Identify the technical requirements\n\n\t\n\nC\n\n\t\n\nC\n\n\t\n\nA\n\n\t\n\nR\n\n\t\n\n\t\n\n\n\n\n5. Create the DynamoDB data model\n\n\t\n\nI\n\n\t\n\nI\n\n\t\n\nI\n\n\t\n\nR/A\n\n\t\n\n\t\n\n\n\n\n6. Create the data queries\n\n\t\n\nI\n\n\t\n\nI\n\n\t\n\nI\n\n\t\n\nR/A\n\n\t\n\nR\n\n\t\n\n\n\n\n7. Validate the data model\n\n\t\n\nA\n\n\t\n\nR\n\n\t\n\nI\n\n\t\n\nC\n\n\t\n\n\t\n\n\n\n\n8. Review the cost estimation\n\n\t\n\nC\n\n\t\n\nA\n\n\t\n\nI\n\n\t\n\nR\n\n\t\n\n\t\n\n\n\n\n9. Deploy the DynamoDB data model\n\n\t\n\nI\n\n\t\n\nI\n\n\t\n\nC\n\n\t\n\nC\n\n\t\n\n\t\n\nR/A"
  }
]