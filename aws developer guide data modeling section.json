[
  {
    "title": "Using DynamoDB as a data store for an online shop - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-online-shop.html",
    "html": "Using DynamoDB as a data store for an online shop\nPDF\nRSS\n\nThis use case talks about using DynamoDB as a data store for an online shop (or e-store).\n\nUse case\n\nAn online store lets users browse through different products and eventually purchase them. Based on the generated invoice, a customer can pay using a discount code or gift card and then pay the remaining amount with a credit card. Purchased products will be picked from one of several warehouses and will be shipped to the provided address. Typical access patterns for an online store include:\n\nGet customer for a given customerId\n\nGet product for a given productId\n\nGet warehouse for a given warehouseId\n\nGet a product inventory for all warehouses by a productId\n\nGet order for a given orderId\n\nGet all products for a given orderId\n\nGet invoice for a given orderId\n\nGet all shipments for a given orderId\n\nGet all orders for a given productId for a given date range\n\nGet invoice for a given invoiceId\n\nGet all payments for a given invoiceId\n\nGet shipment details for a given shipmentId\n\nGet all shipments for a given warehouseId\n\nGet inventory of all products for a given warehouseId\n\nGet all invoices for a given customerId for a given date range\n\nGet all products ordered by a given customerId for a given date range\n\nEntity relationship diagram\n\nThis is the entity relationship diagram (ERD) we'll be using to model DynamoDB as a data store for an online shop.\n\nAccess patterns\n\nThese are the access patterns we'll be considering when using DynamoDB as a data store for an online shop.\n\ngetCustomerByCustomerId\n\ngetProductByProductId\n\ngetWarehouseByWarehouseId\n\ngetProductInventoryByProductId\n\ngetOrderDetailsByOrderId\n\ngetProductByOrderId\n\ngetInvoiceByOrderId\n\ngetShipmentByOrderId\n\ngetOrderByProductIdForDateRange\n\ngetInvoiceByInvoiceId\n\ngetPaymentByInvoiceId\n\ngetShipmentDetailsByShipmentId\n\ngetShipmentByWarehouseId\n\ngetProductInventoryByWarehouseId\n\ngetInvoiceByCustomerIdForDateRange\n\ngetProductsByCustomerIdForDateRange\n\nSchema design evolution\n\nUsing NoSQL Workbench for DynamoDB , import AnOnlineShop_1.json to create a new data model called AnOnlineShop and a new table called OnlineShop. Note that we use the generic names PK and SK for the partition key and sort key. This is a practice used in order to store different types of entities in the same table.\n\nStep 1: Address access pattern 1 (getCustomerByCustomerId)\n\nImport AnOnlineShop_2.json to handle access pattern 1 (getCustomerByCustomerId). Some entities do not have relationships to other entities, so we will use the same value of PK and SK for them. In the example data, note that the keys use a prefix c# in order to distinguish the customerId from other entities that will be added later. This practice is repeated for other entities as well.\n\nTo address this access pattern, a GetItem operation can be used with PK=customerId and SK=customerId.\n\nStep 2: Address access pattern 2 (getProductByProductId)\n\nImport AnOnlineShop_3.json to address access pattern 2 (getProductByProductId) for the product entity. The product entities are prefixed by p# and the same sort key attribute has been used to store customerID as well as productID. Generic naming and vertical partitioning allows us to create such item collections for an effective single table design.\n\nTo address this access pattern, a GetItem operation can be used with PK=productId and SK=productId.\n\nStep 3: Address access pattern 3 (getWarehouseByWarehouseId)\n\nImport AnOnlineShop_4.json to address access pattern 3 (getWarehouseByWarehouseId) for the warehouse entity. We currently have the customer, product, and warehouse entities added to the same table. They are distinguished using prefixes and the EntityType attribute. A type attribute (or prefix naming) improves the model’s readability. The readability would be affected if we simply stored alphanumeric IDs for different entities in the same attribute. It would be difficult to tell one entity from the other in the absence of these identifiers.\n\nTo address this access pattern, a GetItem operation can be used with PK=warehouseId and SK=warehouseId.\n\nBase table:\n\nStep 4: Address access pattern 4 (getProductInventoryByProductId)\n\nImport AnOnlineShop_5.json to address access pattern 4 (getProductInventoryByProductId). warehouseItem entity is used to keep track of the number of products in each warehouse. This item would normally be updated when a product is added or removed from a warehouse. As seen in the ERD, there is a many-to-many relationship between product and warehouse. Here, the one-to-many relationship from product to warehouse is modeled as warehouseItem. Later on, the one-to-many relationship from warehouse to product will be modeled as well.\n\nAccess pattern 4 can be addressed with a query on PK=ProductId and SK begins_with “w#“.\n\nFor more information about begins_with() and other expressions that can be applied to sort keys, see Key Condition Expressions.\n\nBase table:\n\nStep 5: Address access patterns 5 (getOrderDetailsByOrderId) and 6 (getProductByOrderId)\n\nAdd some more customer, product, and warehouse items to the table by importing AnOnlineShop_6.json. Then, import AnOnlineShop_7.json to build an item collection for order that can address access patterns 5 (getOrderDetailsByOrderId) and 6 (getProductByOrderId). You can see the one-to-many relationship between order and product modeled as orderItem entities.\n\nTo address access pattern 5 (getOrderDetailsByOrderId), query the table with PK=orderId. This will provide all information about the order including customerId and ordered products.\n\nBase table:\n\nTo address access pattern 6 (getProductByOrderId), we need to read products in an order only. Query the table with PK=orderId and SK begins_with “p#” to accomplish this.\n\nBase table:\n\nStep 6: Address access pattern 7 (getInvoiceByOrderId)\n\nImport AnOnlineShop_8.json to add an invoice entity to the order item collection to handle access pattern 7 (getInvoiceByOrderId). To address this access pattern, you can use a query operation with PK=orderId and SK begins_with “i#”.\n\nBase table:\n\nStep 7: Address access pattern 8 (getShipmentByOrderId)\n\nImport AnOnlineShop_9.json to add shipment entities to the order item collection to address access pattern 8 (getShipmentByOrderId). We are extending the same vertically partitioned model by adding more types of entities in the single table design. Notice how the order item collection contains the different relationships that an order entity has with the shipment, orderItem, and invoice entities.\n\nTo get shipments by orderId, you can perform a query operation with PK=orderId and SK begins_with “sh#”.\n\nBase table:\n\nStep 8: Address access pattern 9 (getOrderByProductIdForDateRange)\n\nWe created an order item collection in the previous step. This access pattern has new lookup dimensions (ProductID and Date) which requires you to scan the whole table and filter out relevant records to fetch targeted items. In order to address this access pattern, we'll need to create a global secondary index (GSI). Import AnOnlineShop_10.json to create a new item collection using the GSI that makes it possible to retrieve orderItem data from several order item collections. The data now has GSI1-PK and GSI1-SK which will be GSI1’s partition key and sort key, respectively.\n\nDynamoDB automatically populates items which contain a GSI’s key attributes from the table to the GSI. There is no need to manually do any additional inserts into the GSI.\n\nTo address access pattern 9, perform a query on GSI1 with GSI1-PK=productId and GSI1SK between (date1, date2).\n\nBase table:\n\nGSI1:\n\nStep 9: Address access patterns 10 (getInvoiceByInvoiceId) and 11 (getPaymentByInvoiceId)\n\nImport AnOnlineShop_11.json to address access patterns 10 (getInvoiceByInvoiceId) and 11 (getPaymentByInvoiceId), both of which are related to invoice. Even though these are two different access patterns, they are realized using the same key condition. Payments are defined as an attribute with the map data type on the invoice entity.\n\nNote\n\nGSI1-PK and GSI1-SK is overloaded to store information about different entities so that multiple access patterns can be served from the same GSI. For more information about GSI overloading, see Overloading Global Secondary Indexes.\n\nTo address access pattern 10 and 11, query GSI1 with GSI1-PK=invoiceId and GSI1-SK=invoiceId.\n\nGSI1:\n\nStep 10: Address access patterns 12 (getShipmentDetailsByShipmentId) and 13 (getShipmentByWarehouseId)\n\nImport AnOnlineShop_12.json to address access patterns 12 (getShipmentDetailsByShipmentId) and 13 (getShipmentByWarehouseId).\n\nNotice that shipmentItem entities are added to the order item collection on the base table in order to be able to retrieve all details about an order in a single query operation.\n\nBase table:\n\nThe GSI1 partition and sort keys have already been used to model a one-to-many relationship between shipment and shipmentItem. To address access pattern 12 (getShipmentDetailsByShipmentId), query GSI1 with GSI1-PK=shipmentId and GSI1-SK=shipmentId.\n\nGSI1:\n\nWe’ll need to create another GSI (GSI2) to model the new one-to-many relationship between warehouse and shipment for access pattern 13 (getShipmentByWarehouseId). To address this access pattern, query GSI2 with GSI2-PK=warehouseId and GSI2-SK begins_with “sh#”.\n\nGSI2:\n\nStep 11: Address access patterns 14 (getProductInventoryByWarehouseId) 15 (getInvoiceByCustomerIdForDateRange), and 16 (getProductsByCustomerIdForDateRange)\n\nImport AnOnlineShop_13.json to add data related to the next set of access patterns. To address access pattern 14 (getProductInventoryByWarehouseId), query GSI2 with GSI2-PK=warehouseId and GSI2-SK begins_with “p#”.\n\nGSI2:\n\nTo address access pattern 15 (getInvoiceByCustomerIdForDateRange), query GSI2 with GSI2-PK=customerId and GSI2-SK between (i#date1, i#date2).\n\nGSI2:\n\nTo address access pattern 16 (getProductsByCustomerIdForDateRange), query GSI2 with GSI2-PK=customerId and GSI2-SK between (p#date1, p#date2).\n\nGSI2:\n\nNote\n\nIn NoSQL Workbench, facets represent an application's different data access patterns for DynamoDB. Facets give you a way to view a subset of the data in a table, without having to see records that don't meet the constraints of the facet. Facets are considered a visual data modeling tool, and don't exist as a usable construct in DynamoDB as they are purely an aid for modeling access patterns.\n\nImport AnOnlineShop_facets.json to see the facets for this use case.\n\nAll access patterns and how the schema design addresses them are summarized in the table below:\n\nAccess pattern\tBase table/GSI/LSI\tOperation\tPartition key value\tSort key value\ngetCustomerByCustomerId\tBase table\tGetItem\tPK=customerId\tSK=customerId\ngetProductByProductId\tBase table\tGetItem\tPK=productId\tSK=productId\ngetWarehouseByWarehouseId\tBase table\tGetItem\tPK=warehouseId\tSK=warehouseId\ngetProductInventoryByProductId\tBase table\tQuery\tPK=productId\tSK begins_with \"w#\"\ngetOrderDetailsByOrderId\tBase table\tQuery\tPK=orderId\t\ngetProductByOrderId\tBase table\tQuery\tPK=orderId\tSK begins_with \"p#\"\ngetInvoiceByOrderId\tBase table\tQuery\tPK=orderId\tSK begins_with \"i#\"\ngetShipmentByOrderId\tBase table\tQuery\tPK=orderId\tSK begins_with \"sh#\"\ngetOrderByProductIdForDateRange\tGSI1\tQuery\tPK=productId\tSK between date1 and date2\ngetInvoiceByInvoiceId\tGSI1\tQuery\tPK=invoiceId\tSK=invoiceId\ngetPaymentByInvoiceId\tGSI1\tQuery\tPK=invoiceId\tSK=invoiceId\ngetShipmentDetailsByShipmentId\tGSI1\tQuery\tPK=shipmentId\tSK=shipmentId\ngetShipmentByWarehouseId\tGSI2\tQuery\tPK=warehouseId\tSK begins_with \"sh#\"\ngetProductInventoryByWarehouseId\tGSI2\tQuery\tPK=warehouseId\tSK begins_with \"p#\"\ngetInvoiceByCustomerIdForDateRange\tGSI2\tQuery\tPK=customerId\tSK between i#date1 and i#date2\ngetProductsByCustomerIdForDateRange\tGSI2\tQuery\tPK=customerId\tSK between p#date1 and p#date2\nOnline shop final schema\n\nHere are the final schema designs. To download this schema design as a JSON file, see DynamoDB Design Patterns on GitHub.\n\nBase table\n\nGSI1\n\nGSI2\n\nUsing NoSQL Workbench with this schema design\n\nYou can import this final schema into NoSQL Workbench, a visual tool that provides data modeling, data visualization, and query development features for DynamoDB, to further explore and edit your new project. Follow these steps to get started:\n\nDownload NoSQL Workbench. For more information, see Download NoSQL Workbench for DynamoDB.\n\nDownload the JSON schema file listed above, which is already in the NoSQL Workbench model format.\n\nImport the JSON schema file into NoSQL Workbench. For more information, see Importing an existing data model.\n\nOnce you've imported into NOSQL Workbench, you can edit the data model. For more information, see Editing an existing data model.\n\nTo visualize your data model, add sample data, or import sample data from a CSV file, use the Data Visualizer feature of NoSQL Workbench."
  },
  {
    "title": "Monitoring device status updates in DynamoDB - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-device-status.html",
    "html": "Monitoring device status updates in DynamoDB\nPDF\nRSS\n\nThis use case talks about using DynamoDB to monitor device status updates (or changes in device state) in DynamoDB.\n\nUse case\n\nIn IoT use-cases (a smart factory for instance) many devices need to be monitored by operators and they periodically send their status or logs to a monitoring system. When there is a problem with a device, the status for the device changes from normal to warning. There are different log levels or statuses depending on the severity and type of abnormal behavior in the device. The system then assigns an operator to check on the device and they may escalate the problem to their supervisor if needed.\n\nSome typical access patterns for this system include:\n\nCreate log entry for a device\n\nGet all logs for a specific device state showing the most recent logs first\n\nGet all logs for a given operator between two dates\n\nGet all escalated logs for a given supervisor\n\nGet all escalated logs with a specific device state for a given supervisor\n\nGet all escalated logs with a specific device state for a given supervisor for a specific date\n\nEntity relationship diagram\n\nThis is the entity relationship diagram (ERD) we'll be using for monitoring device status updates.\n\nAccess patterns\n\nThese are the access patterns we'll be considering for monitoring device status updates.\n\ncreateLogEntryForSpecificDevice\n\ngetLogsForSpecificDevice\n\ngetWarningLogsForSpecificDevice\n\ngetLogsForOperatorBetweenTwoDates\n\ngetEscalatedLogsForSupervisor\n\ngetEscalatedLogsWithSpecificStatusForSupervisor\n\ngetEscalatedLogsWithSpecificStatusForSupervisorForDate\n\nSchema design evolution\n\nStep 1: Address access patterns 1 (createLogEntryForSpecificDevice) and 2 (getLogsForSpecificDevice)\n\nThe unit of scaling for a device tracking system would be individual devices. In this system, a deviceID uniquely identifies a device. This makes deviceID a good candidate for the partition key. Each device sends information to the tracking system periodically (say, every five minutes or so). This ordering makes date a logical sorting criterion and therefore, the sort key. The sample data in this case would look something like this:\n\nTo fetch log entries for a specific device, we can perform a query operation with partition key DeviceID=\"d#12345\".\n\nStep 2: Address access pattern 3 (getWarningLogsForSpecificDevice)\n\nSince State is a non-key attribute, addressing access pattern 3 with the current schema would require a filter expression. In DynamoDB, filter expressions are applied after data is read using key condition expressions. For example, if we were to fetch warning logs for d#12345, the query operation with partition key DeviceID=\"d#12345\" will read four items from the above table and then filter out the one item with the warning status. This approach is not efficient at scale. A filter expression can be a good way to exclude items that are queried if the ratio of excluded items is low or the query is performed infrequently. However, for cases where many items are retrieved from a table and the majority of the items are filtered out, we can continue evolving our table design so it runs more efficiently.\n\nLet's change how to handle this access pattern by using composite sort keys. You can import sample data from DeviceStateLog_3.json where the sort key is changed to State#Date. This sort key is the composition of the attributes State, #, and Date. In this example, # is used as a delimiter. The data now looks something like this:\n\nTo fetch only warning logs for a device, the query becomes more targeted with this schema. The key condition for the query uses partition key DeviceID=\"d#12345\" and sort key State#Date begins_with “WARNING”. This query will only read the relevant three items with the warning state.\n\nStep 3: Address access pattern 4 (getLogsForOperatorBetweenTwoDates)\n\nYou can import DeviceStateLog_4.jsonD where the Operator attribute was added to the DeviceStateLog table with example data.\n\nSince Operator is not currently a partition key, there is no way to perform a direct key-value lookup on this table based on OperatorID. We’ll need to create a new item collection with a global secondary index on OperatorID. The access pattern requires a lookup based on dates so Date is the sort key attribute for the global secondary index (GSI). This is what the GSI now looks like:\n\nFor access pattern 4 (getLogsForOperatorBetweenTwoDates), you can query this GSI with partition key OperatorID=Liz and sort key Date between 2020-04-11T05:58:00 and 2020-04-24T14:50:00.\n\nStep 4: Address access patterns 5 (getEscalatedLogsForSupervisor) 6 (getEscalatedLogsWithSpecificStatusForSupervisor), and 7 (getEscalatedLogsWithSpecificStatusForSupervisorForDate)\n\nWe’ll be using a sparse index to address these access patterns.\n\nGlobal secondary indexes are sparse by default, so only items in the base table that contain primary key attributes of the index will actually appear in the index. This is another way of excluding items that are not relevant for the access pattern being modeled.\n\nYou can import DeviceStateLog_6.json where the EscalatedTo attribute was added to the DeviceStateLog table with example data. As mentioned earlier, not all of the logs gets escalated to a supervisor.\n\nYou can now create a new GSI where EscalatedTo is the partition key and State#Date is the sort key. Notice that only items that have both EscalatedTo and State#Date attributes appear in the index.\n\nThe rest of the access patterns are summarized as follows:\n\nAll access patterns and how the schema design addresses them are summarized in the table below:\n\nAccess pattern\tBase table/GSI/LSI\tOperation\tPartition key value\tSort key value\tOther conditions/filters\ncreateLogEntryForSpecificDevice\tBase table\tPutItem\tDeviceID=deviceId\tState#Date=state#date\t\ngetLogsForSpecificDevice\tBase table\tQuery\tDeviceID=deviceId\tState#Date begins_with \"state1#\"\tScanIndexForward = False\ngetWarningLogsForSpecificDevice\tBase table\tQuery\tDeviceID=deviceId\tState#Date begins_with \"WARNING\"\t\ngetLogsForOperatorBetweenTwoDates\tGSI-1\tQuery\tOperator=operatorName\tDate between date1 and date2\t\ngetEscalatedLogsForSupervisor\tGSI-2\tQuery\tEscalatedTo=supervisorName\t\t\ngetEscalatedLogsWithSpecificStatusForSupervisor\tGSI-2\tQuery\tEscalatedTo=supervisorName\tState#Date begins_with \"state1#\"\t\ngetEscalatedLogsWithSpecificStatusForSupervisorForDate\tGSI-2\tQuery\tEscalatedTo=supervisorName\tState#Date begins_with \"state1#date1\"\t\nFinal schema\n\nHere are the final schema designs. To download this schema design as a JSON file, see DynamoDB Examples on GitHub.\n\nBase table\n\nGSI-1\n\nGSI-2\n\nUsing NoSQL Workbench with this schema design\n\nYou can import this final schema into NoSQL Workbench, a visual tool that provides data modeling, data visualization, and query development features for DynamoDB, to further explore and edit your new project. Follow these steps to get started:\n\nDownload NoSQL Workbench. For more information, see Download NoSQL Workbench for DynamoDB.\n\nDownload the JSON schema file listed above, which is already in the NoSQL Workbench model format.\n\nImport the JSON schema file into NoSQL Workbench. For more information, see Importing an existing data model.\n\nOnce you've imported into NOSQL Workbench, you can edit the data model. For more information, see Editing an existing data model.\n\nTo visualize your data model, add sample data, or import sample data from a CSV file, use the Data Visualizer feature of NoSQL Workbench."
  },
  {
    "title": "Recurring payments schema design in DynamoDB - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-schema-recurring-payments.html",
    "html": "Recurring payments schema design in DynamoDB\nPDF\nRSS\nRecurring payments business use case\n\nThis use case talks about using DynamoDB to implement a recurring payments system. The data model has the following entities: accounts, subscriptions, and receipts. The specifics for our use case include the following:\n\nEach account can have multiple subscriptions\n\nThe subscription has a NextPaymentDate when the next payment needs to be processed and a NextReminderDate when an email reminder is sent to the customer\n\nThere is an item for the subscription that is stored and updated when the payment been processed (the average item size is around 1KB and the throughput depends on the number of accounts and subscriptions)\n\nThe payment processor will also create a receipt as part of the process which is stored in the table and are set to expire after a period of time by using a TTL attribute\n\nRecurring payments entity relationship diagram\n\nThis is the entity relationship diagram (ERD) we'll be using for the recurring payments system schema design.\n\nRecurring payments system access patterns\n\nThese are the access patterns we'll be considering for the recurring payments system schema design.\n\ncreateSubscription\n\ncreateReceipt\n\nupdateSubscription\n\ngetDueRemindersByDate\n\ngetDuePaymentsByDate\n\ngetSubscriptionsByAccount\n\ngetReceiptsByAccount\n\nRecurring payments schema design\n\nThe generic names PK and SK are used for key attributes to allow storing different types of entities in the same table such as the account, subscription, and receipt entities. The user first creates a subscription, which is where the user agrees to pay an amount on the same day each month in return for a product. They get the choice on which day of the month to process the payment. There is also a reminder that will be sent prior to the payment being processed. The application works by having two batch jobs that run each day: one batch job sends reminders due that day and the other batch job processes any payments due that day.\n\nStep 1: Address access pattern 1 (createSubscription)\n\nAccess pattern 1 (createSubscription) is used to initially create the subscription, and the details including SKU, NextPaymentDate, NextReminderDate and PaymentDetails are set. This step shows the state of the table for just one account with one subscription. There can be multiple subscriptions in the item collection so this is a one-to-many relationship.\n\nStep 2: Address access patterns 2 (createReceipt) and 3 (updateSubscription)\n\nAccess pattern 2 (createReceipt) is used to create the receipt item. After the payment is processed each month, the payment processor will write a receipt back to the base table. There, could be multiple receipts in the item collection so this is a one-to-many relationship. The payment processor will also update the subscription item (access Pattern 3 (updateSubscription)) to update for the NextReminderDate or the NextPaymentDate for the next month.\n\nStep 3: Address access pattern 4 (getDueRemindersByDate)\n\nThe application processes reminders for the payment in batches for the current day. Therefore the application needs to access the subscriptions on a different dimension: date rather than account. This is a good use case for a global secondary index (GSI). In this step we add the index GSI-1, which uses the NextReminderDate as the GSI partition key. We do not need to replicate all the items. This GSI is a sparse index and the receipts items are not replicated. We also do not need to project all the attributes—we only need to include a subset of the attributes. The image below shows the schema of GSI-1 and it gives the information needed for the application to send the reminder email.\n\nStep 4: Address access pattern 5 (getDuePaymentsByDate)\n\nThe application processes the payments in batches for the current day in the same way it does with reminders. We add GSI-2 in this step, and it uses the NextPaymentDate as the GSI partition key. We do not need to replicate all the items. This GSI is a sparse index as the receipts items are not replicated. The image below shows the schema of GSI-2.\n\nStep 5: Address access patterns 6 (getSubscriptionsByAccount) and 7 (getReceiptsByAccount)\n\nThe application can retrieve all the subscriptions for an account by using a query on the base table that targets the account identifier (the PK) and uses the range operator to get all the items where the SK begins with “SUB#”. The application can also use the same query structure to retrieve all the receipts by using a range operator to get all the items where the SK begins with “REC#”. This allows us to satisfy access patterns 6 (getSubscriptionsByAccount) and 7 (getReceiptsByAccount). The application uses these access patterns so the user can see their current subscriptions and their past receipts for the last six months. There is no change to the table schema in this step and we can see below how we target just the subscription item(s) in access pattern 6 (getSubscriptionsByAccount).\n\nAll access patterns and how the schema design addresses them are summarized in the table below:\n\nAccess pattern\tBase table/GSI/LSI\tOperation\tPartition key value\tSort key value\ncreateSubscription\tBase table\tPutItem\tACC#account_id\tSUB#<SUBID>#SKU<SKUID>\ncreateReceipt\tBase table\tPutItem\tACC#account_id\tREC#<RecieptDate>#SKU<SKUID>\nupdateSubscription\tBase table\tUpdateItem\tACC#account_id\tSUB#<SUBID>#SKU<SKUID>\ngetDueRemindersByDate\tGSI-1\tQuery\t<NextReminderDate>\t\ngetDuePaymentsByDate\tGSI-2\tQuery\t<NextPaymentDate>\t\ngetSubscriptionsByAccount\tBase table\tQuery\tACC#account_id\tSK begins_with “SUB#”\ngetReceiptsByAccount\tBase table\tQuery\tACC#account_id\tSK begins_with “REC#”\nRecurring payments final schema\n\nHere are the final schema designs. To download this schema design as a JSON file, see DynamoDB Examples on GitHub.\n\nBase table\n\nGSI-1\n\nGSI-2\n\nUsing NoSQL Workbench with this schema design\n\nYou can import this final schema into NoSQL Workbench, a visual tool that provides data modeling, data visualization, and query development features for DynamoDB, to further explore and edit your new project. Follow these steps to get started:\n\nDownload NoSQL Workbench. For more information, see Download NoSQL Workbench for DynamoDB.\n\nDownload the JSON schema file listed above, which is already in the NoSQL Workbench model format.\n\nImport the JSON schema file into NoSQL Workbench. For more information, see Importing an existing data model.\n\nOnce you've imported into NOSQL Workbench, you can edit the data model. For more information, see Editing an existing data model.\n\nTo visualize your data model, add sample data, or import sample data from a CSV file, use the Data Visualizer feature of NoSQL Workbench."
  },
  {
    "title": "Complaint management system schema design in DynamoDB - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-complaint-management.html",
    "html": "Complaint management system schema design in DynamoDB\nPDF\nRSS\nComplaint management system business use case\n\nDynamoDB is a database well-suited for a complaint management system (or contact center) use case as most access patterns associated with them would be key-value based transactional lookups. The typical access patterns in this scenario would be to:\n\nCreate and update complaints\n\nEscalate a complaint\n\nCreate and read comments on a complaint\n\nGet all complaints by a customer\n\nGet all comments by an agent and get all escalations\n\nSome comments may have attachments describing the complaint or solution. While these are all key-value access patterns, there can be additional requirements such as sending out notifications when a new comment is added to a complaint or running analytical queries to find complaint distribution by severity (or agent performance) per week. An additional requirement related to lifecycle management or compliance would be to archive complaint data after three years of logging the complaint.\n\nComplaint management system architecture diagram\n\nApart from the key-value transactional access patterns that we will be handling in the DynamoDB data modeling section later, we have three non-transactional requirements. The architecture diagram above can be broken down into the following three workflows:\n\nSend a notification when a new comment is added to a complaint\n\nRun analytical queries on weekly data\n\nArchive data older than three years\n\nLet's take a more in-depth look at each one.\n\nSend a notification when a new comment is added to a complaint\n\nWe can use the below workflow to achieve this requirement:\n\nDynamoDB Streams is a change data capture mechanism to record all write activity on your DynamoDB tables. You can configure Lambda functions to trigger on some or all of these changes. An event filter can be configured on Lambda triggers to filter out events that are not relevant to the use-case. In this instance, we can use a filter to trigger Lambda only when a new comment is added and send out notification to relevant email ID(s) which can be fetched from AWS Secrets Manager or any other credential store.\n\nRun analytical queries on weekly data\n\nDynamoDB is suitable for workloads that are primarily focused on online transactional processing (OLTP). For the other 10-20% access patterns with analytical requirements, data can be exported to S3 with the managed Export to Amazon S3 feature with no impact to the live traffic on DynamoDB table. Take a look at this workflow below:\n\nAmazon EventBridge can be used to trigger AWS Lambda on schedule - it allows you to configure a cron expression for Lambda invocation to take place periodically. Lambda can invoke the ExportToS3 API call and store DynamoDB data in S3. This S3 data can then be accessed by a SQL engine such as Amazon Athena to run analytical queries on DynamoDB data without affecting the live transactional workload on the table. A sample Athena query to find number of complaints per severity level would look like this:\n\nSELECT Item.severity.S as \"Severity\", COUNT(Item) as \"Count\" \nFROM \"complaint_management\".\"data\" \nWHERE NOT Item.severity.S = '' \nGROUP BY Item.severity.S ;\n\nThis results in the following Athena query result:\n\nArchive data older than three years\n\nYou can leverage the DynamoDB Time to Live (TTL) feature to delete obsolete data from your DynamoDB table at no additional cost (except in the case of global tables replicas for the 2019.11.21 (Current) version, where TTL deletes replicated to other Regions consume write capacity). This data appears and can be consumed from DynamoDB Streams to be archived off into Amazon S3. The workflow for this requirement is as follows:\n\nComplaint management system entity relationship diagram\n\nThis is the entity relationship diagram (ERD) we'll be using for the complaint management system schema design.\n\nComplaint management system access patterns\n\nThese are the access patterns we'll be considering for the complaint management schema design.\n\ncreateComplaint\n\nupdateComplaint\n\nupdateSeveritybyComplaintID\n\ngetComplaintByComplaintID\n\naddCommentByComplaintID\n\ngetAllCommentsByComplaintID\n\ngetLatestCommentByComplaintID\n\ngetAComplaintbyCustomerIDAndComplaintID\n\ngetAllComplaintsByCustomerID\n\nescalateComplaintByComplaintID\n\ngetAllEscalatedComplaints\n\ngetEscalatedComplaintsByAgentID (order from newest to oldest)\n\ngetCommentsByAgentID (between two dates)\n\nComplaint management system schema design evolution\n\nSince this is a complaint management system, most access patterns revolve around a complaint as the primary entity. The ComplaintID being highly cardinal will ensure even distribution of data in the underlying partitions and is also the most common search criteria for our identified access patterns. Therefore, ComplaintID is a good partition key candidate in this data set.\n\nStep 1: Address access patterns 1 (createComplaint), 2 (updateComplaint), 3 (updateSeveritybyComplaintID), and 4 (getComplaintByComplaintID)\n\nWe can use a generic sort key valued called \"metadata\" (or \"AA\") to store complaint-specific information such as CustomerID, State, Severity, and CreationDate. We use singleton operations with PK=ComplaintID and SK=“metadata” to do the following:\n\nPutItem to create a new complaint\n\nUpdateItem to update the severity or other fields in the complaint metadata\n\nGetItem to fetch metadata for the complaint\n\nStep 2: Address access pattern 5 (addCommentByComplaintID)\n\nThis access pattern requires a one-to-many relationship model between a complaint and comments on the complaint. We will use the vertical partitioning technique here to use a sort key and create an item collection with different types of data. If we look at access patterns 6 (getAllCommentsByComplaintID) and 7 (getLatestCommentByComplaintID), we know that comments will need to be sorted by time. We can also have multiple comments coming in at the same time so we can use the composite sort key technique to append time and CommentID in the sort key attribute.\n\nOther options to deal with such possible comment collisions would be to increase the granularity for the timestamp or add an incremental number as a suffix instead of using Comment_ID. In this case, we’ll prefix the sort key value for items corresponding to comments with “comm#” to enable range-based operations.\n\nWe also need to ensure that the currentState in the complaint metadata reflects the state when a new comment is added. Adding a comment might indicate that the complaint has been assigned to an agent or it has been resolved and so on. In order to bundle the addition of comment and update of current state in the complaint metadata, in an all-or-nothing manner, we will use the TransactWriteItems API. The resulting table state now looks like this:\n\nLet’s add some more data in the table and also add ComplaintID as a separate field from our PK for future-proofing the model in case we need additional indexes on ComplaintID. Also note that some comments may have attachments which we will store in Amazon Simple Storage Service and only maintain their references or URLs in DynamoDB. It’s a best practice to keep the transactional database as lean as possible to optimize cost and performance. The data now looks like this:\n\nStep 3: Address access patterns 6 (getAllCommentsByComplaintID) and 7 (getLatestCommentByComplaintID)\n\nIn order to get all comments for a complaint, we can use the query operation with the begins_with condition on the sort key. Instead of consuming additional read capacity to read the metadata entry and then having the overhead of filtering the relevant results, having a sort key condition like this help us only read what we need. For example, a query operation with PK=Complaint123 and SK begins_with comm# would return the following while skipping the metadata entry:\n\nSince we need the latest comment for a complaint in pattern 7 (getLatestCommentByComplaintID), let's use two additional query parameters:\n\nScanIndexForward should be set to False to get results sorted in a descending order\n\nLimit should be set to 1 to get the latest (only one) comment\n\nSimilar to access pattern 6 (getAllCommentsByComplaintID), we skip the metadata entry using begins_with comm# as the sort key condition. Now, you can perform access pattern 7 on this design using the query operation with PK=Complaint123 and SK=begins_with comm#, ScanIndexForward=False, and Limit 1. The following targeted item will be returned as a result:\n\nLet's add more dummy data to the table.\n\nStep 4: Address access patterns 8 (getAComplaintbyCustomerIDAndComplaintID) and 9 (getAllComplaintsByCustomerID)\n\nAccess patterns 8 (getAComplaintbyCustomerIDAndComplaintID) and 9 (getAllComplaintsByCustomerID) introduces new a search criteria: CustomerID. Fetching it from the existing table requires an expensive Scan to read all data and then filter relevant items for the CustomerID in question. We can make this search more efficient by creating a global secondary index (GSI) with CustomerID as the partition key. Keeping in mind the one-to-many relationship between customer and complaints as well as access pattern 9 (getAllComplaintsByCustomerID), ComplaintID would be the right candidate for the sort key.\n\nThe data in the GSI would look like this:\n\nAn example query on this GSI for access pattern 8 (getAComplaintbyCustomerIDAndComplaintID) would be: customer_id=custXYZ, sort key=Complaint1321. The result would be:\n\nTo get all complaints for a customer for access pattern 9 (getAllComplaintsByCustomerID), the query on the GSI would be: customer_id=custXYZ as the partition key condition. The result would be:\n\nStep 5: Address access pattern 10 (escalateComplaintByComplaintID)\n\nThis access introduces the escalation aspect. To escalate a complaint, we can use UpdateItem to add attributes such as escalated_to and escalation_time to the existing complaint metadata item. DynamoDB provides flexible schema design which means a set of non-key attributes can be uniform or discrete across different items. See below for an example:\n\nUpdateItem with PK=Complaint1444, SK=metadata\n\nStep 6: Address access patterns 11 (getAllEscalatedComplaints) and 12 (getEscalatedComplaintsByAgentID)\n\nOnly a handful of complaints are expected to be escalated out of the whole data set. Therefore, creating an index on the escalation-related attributes would lead to efficient lookups as well as cost-effective GSI storage. We can do this by leveraging the sparse index technique. The GSI with partition key as escalated_to and sort key as escalation_time would look like this:\n\nTo get all escalated complaints for access pattern 11 (getAllEscalatedComplaints), we simply scan this GSI. Note that this scan will be performant and cost-efficient due to the size of the GSI. To get escalated complaints for a specific agent (access pattern 12 (getEscalatedComplaintsByAgentID)), the partition key would be escalated_to=agentID and we set ScanIndexForward to False for ordering from newest to oldest.\n\nStep 7: Address access pattern 13 (getCommentsByAgentID)\n\nFor the last access pattern, we need to perform a lookup by a new dimension: AgentID. We also need time-based ordering to read comments between two dates so we create a GSI with agent_id as the partition key and comm_date as the sort key. The data in this GSI will look like the following:\n\nAn example query on this GSI would be partition key agentID=AgentA and sort key=comm_date between (2023-04-30T12:30:00, 2023-05-01T09:00:00), the result of which is:\n\nAll access patterns and how the schema design addresses them are summarized in the table below:\n\nAccess pattern\tBase table/GSI/LSI\tOperation\tPartition key value\tSort key value\tOther conditions/filters\ncreateComplaint\tBase table\tPutItem\tPK=complaint_id\tSK=metadata\t\nupdateComplaint\tBase table\tUpdateItem\tPK=complaint_id\tSK=metadata\t\nupdateSeveritybyComplaintID\tBase table\tUpdateItem\tPK=complaint_id\tSK=metadata\t\ngetComplaintByComplaintID\tBase table\tGetItem\tPK=complaint_id\tSK=metadata\t\naddCommentByComplaintID\tBase table\tTransactWriteItems\tPK=complaint_id\tSK=metadata, SK=comm#comm_date#comm_id\t\ngetAllCommentsByComplaintID\tBase table\tQuery\tPK=complaint_id\tSK begins_with \"comm#\"\t\ngetLatestCommentByComplaintID\tBase table\tQuery\tPK=complaint_id\tSK begins_with \"comm#\"\tscan_index_forward=False, Limit 1\ngetAComplaintbyCustomerIDAndComplaintID\tCustomer_complaint_GSI\tQuery\tcustomer_id=customer_id\tcomplaint_id = complaint_id\t\ngetAllComplaintsByCustomerID\tCustomer_complaint_GSI\tQuery\tcustomer_id=customer_id\tN/A\t\nescalateComplaintByComplaintID\tBase table\tUpdateItem\tPK=complaint_id\tSK=metadata\t\ngetAllEscalatedComplaints\tEscalations_GSI\tScan\tN/A\tN/A\t\ngetEscalatedComplaintsByAgentID (order from newest to oldest)\tEscalations_GSI\tQuery\tescalated_to=agent_id\tN/A\tscan_index_forward=False\ngetCommentsByAgentID (between two dates)\tAgents_Comments_GSI\tQuery\tagent_id=agent_id\tSK between (date1, date2)\t\nComplaint management system final schema\n\nHere are the final schema designs. To download this schema design as a JSON file, see DynamoDB Examples on GitHub.\n\nBase table\n\nCustomer_Complaint_GSI\n\nEscalations_GSI\n\nAgents_Comments_GSI\n\nUsing NoSQL Workbench with this schema design\n\nYou can import this final schema into NoSQL Workbench, a visual tool that provides data modeling, data visualization, and query development features for DynamoDB, to further explore and edit your new project. Follow these steps to get started:\n\nDownload NoSQL Workbench. For more information, see Download NoSQL Workbench for DynamoDB.\n\nDownload the JSON schema file listed above, which is already in the NoSQL Workbench model format.\n\nImport the JSON schema file into NoSQL Workbench. For more information, see Importing an existing data model.\n\nOnce you've imported into NOSQL Workbench, you can edit the data model. For more information, see Editing an existing data model.\n\nTo visualize your data model, add sample data, or import sample data from a CSV file, use the Data Visualizer feature of NoSQL Workbench."
  },
  {
    "title": "Gaming profile schema design in DynamoDB - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-schema-gaming-profile.html",
    "html": "Gaming profile schema design in DynamoDB\nPDF\nRSS\nGaming profile business use case\n\nThis use case talks about using DynamoDB to store player profiles for a gaming system. Users (in this case, players) need to create profiles before they can interact with many modern games, especially online ones. Gaming profiles typically include the following:\n\nBasic information such as their user name\n\nGame data such as items and equipment\n\nGame records such as tasks and activities\n\nSocial information such as friend lists\n\nTo meet the fine-grained data query access requirements for this application, the primary keys (partition key and sort key) will use generic names (PK and SK) so they can be overloaded with various types of values as we will see below.\n\nThe access patterns for this schema design are:\n\nGet a user's friend list\n\nGet all of a player's information\n\nGet a user's item list\n\nGet a specific item from the user's item list\n\nUpdate a user's character\n\nUpdate the item count for a user\n\nThe size of the gaming profile will vary in different games. Compressing large attribute values can let them fit within item limits in DynamoDB and reduce costs. The throughput management strategy would depend various on factors such as: number of players, number of games played per second, and seasonality of the workload. Typically for a newly launched game, the number of players and the level of popularity are unknown so we will start with the on-demand throughput mode.\n\nGaming profile entity relationship diagram\n\nThis is the entity relationship diagram (ERD) we'll be using for the gaming profile schema design.\n\nGaming profile access patterns\n\nThese are the access patterns we'll be considering for the social network schema design.\n\ngetPlayerFriends\n\ngetPlayerAllProfile\n\ngetPlayerAllItems\n\ngetPlayerSpecificItem\n\nupdateCharacterAttributes\n\nupdateItemCount\n\nGaming profile schema design evolution\n\nFrom the above ERD, we can see that this is a one-to-many relationship type of data modeling. In DynamoDB, one-to-many data models can be organized into item collections, which is different from traditional relational databases where multiple tables are created and linked through foreign keys. An item collections is a group of items that share the same partition key value but have different sort key values. Within an item collection, each item has a unique sort key value that distinguishes it from other items. With this in mind, let’s use the following pattern for HASH and RANGE values for each entity type.\n\nTo begin, we use generic names like PK and SK to store different types of entities in the same table to make the model future-proof. For better readability, we can include prefixes to denote the type of data or include an arbitrary attribute called Entity_type or Type. In the current example, we use a string starting with player to store player_ID as the PK; use entity name# as the prefix of SK, and add a Type attribute to indicate which entity type this piece of data is. This allows us to support storing more entity types in the future, and use advanced technologies such as GSI Overloading and Sparse GSI to meet more access patterns.\n\nLet’s start implementing the access patterns. Access patterns such as adding players and adding equipment can be realized through the PutItem operation, so we can ignore them. In this document, we’ll focus on the typical access patterns listed above.\n\nStep 1: Address access pattern 1 (getPlayerFriends)\n\nWe address access pattern 1 (getPlayerFriends) with this step. In our current design, friendship is simple and the number of friends in the game is small. For simplicity's sake, we use a list data type to store friend lists (1:1 modeling). In this design, we use GetItem to satisfy this access pattern. In the GetItem operation, we explicitly provide the partition key and sort key value to get a specific item.\n\nHowever, if a game has a large number of friends, and the relationships between them are complex (such as friendships being bi-directional with both an invite and accept component) it would be necessary to use a many-to-many relationship to store each friend individually, in order to scale to an unlimited friend list size. And if the friendship change involves operating on multiple items at the same time, DynamoDB transactions can be used to group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation.\n\nStep 2: Address access patterns 2 (getPlayerAllProfile), 3 (getPlayerAllItems), and 4 (getPlayerSpecificItem)\n\nWe address access patterns 2 (getPlayerAllProfile), 3 (getPlayerAllItems), and 4 (getPlayerSpecificItem) using this step. What these three access patterns have in common is a range query, which uses the Query operation. Depending on the scope of the query, Key Condition and Filter Expressions are used, which are commonly used in practical development.\n\nIn the Query operation, we provide a single value for Partition Key and get all items with that Partition Key value. Access pattern 2 (getPlayerAllProfile) is implemented in this way. Optionally, we can add a sort key condition expression — a string that determines the items to be read from the table. Access pattern 3 (getPlayerAllItems) is implemented by adding the key condition of sort key begins_with ITEMS#. Further, in order to simplify the development of the application side, we can use filter expressions to implement access pattern 4 (getPlayerSpecificItem).\n\nHere's a pseudocode example using filter expression that filters items of the Weapon category:\n\nfilterExpression: \"ItemType = :itemType\" \nexpressionAttributeValues: {\":itemType\": \"Weapon\"}\nNote\n\nA filter expression is applied after a Query finishes, but before the results are returned to the client. Therefore, a Query consumes the same amount of read capacity regardless of whether a filter expression is present.\n\nIf the access pattern is to query a large dataset and filter out a large amount of data to keep only a small subset of data, the appropriate approach is to design DynamoDB Partition Key and Sort Key more effectively. For example, in the above example for obtaining a certain ItemType, if there are many items for each player and querying for a certain ItemType is a typical access pattern, it would be more efficient to bring ItemType into the SK as a composite key. The data model would look like this: ITEMS#ItemType#ItemId.\n\nStep 3: Address access patterns 5 (updateCharacterAttributes) and 6 (updateItemCount)\n\nWe address access patterns 5 (updateCharacterAttributes) and 6 (updateItemCount) using this step. When the player needs to modify the character, such as reducing the currency, or modifying the quantity of a certain weapon in their items, use UpdateItem to implement these access patterns. To update a player's currency but ensure it never goes below a minimum amount, we can add a Condition expressions to reduce the balance only if it's greater than or equal to the minimum amount. Here is a pseudocode example:\n\nUpdateExpression: \"SET currency = currency - :amount\"\nConditionExpression: \"currency >= :minAmount\"\n\nWhen developing with DynamoDB and using Atomic Counters to decrement inventory, we can ensure idempotency by using optimistic locking. Here is a pseudocode example for Atomic Counters:\n\nUpdateExpression: \"SET ItemCount = ItemCount - :incr\" \nexpression-attribute-values: '{\":incr\":{\"N\":\"1\"}}'\n\nIn addition, in a scenario where the player purchases an item with currency, the entire process needs to deduct currency and add an item at the same time. We can use DynamoDB Transactions to group multiple actions together and submit them as a single all-or-nothing TransactWriteItems or TransactGetItems operation. TransactWriteItems is a synchronous and idempotent write operation that groups up to 100 write actions in a single all-or-nothing operation. The actions are completed atomically so that either all of them succeed or none of them succeeds. Transactions help eliminate the risk of duplication or vanishing currency. For more information on transactions, see DynamoDB transactions example .\n\nAll access patterns and how the schema design addresses them are summarized in the table below:\n\nAccess pattern\tBase table/GSI/LSI\tOperation\tPartition key value\tSort key value\tOther conditions/filters\ngetPlayerFriends\tBase table\tGetItem\tPK=PlayerID\tSK=“FRIENDS#playerID”\t\ngetPlayerAllProfile\tBase table\tQuery\tPK=PlayerID\t\t\ngetPlayerAllItems\tBase table\tQuery\tPK=PlayerID\tSK begins_with “ITEMS#”\t\ngetPlayerSpecificItem\tBase table\tQuery\tPK=PlayerID\tSK begins_with “ITEMS#”\tfilterExpression: \"ItemType = :itemType\" expressionAttributeValues: { \":itemType\": \"Weapon\" }\nupdateCharacterAttributes\tBase table\tUpdateItem\tPK=PlayerID\tSK=“#METADATA#playerID”\tUpdateExpression: \"SET currency = currency - :amount\" ConditionExpression: \"currency >= :minAmount\"\nupdateItemCount\tBase table\tUpdateItem\tPK=PlayerID\tSK =“ITEMS#ItemID”\tupdate-expression: \"SET ItemCount = ItemCount - :incr\" expression-attribute-values: '{\":incr\":{\"N\":\"1\"}}'\nGaming profile final schema\n\nHere is the final schema design. To download this schema design as a JSON file, see DynamoDB Examples on GitHub.\n\nBase table:\n\nUsing NoSQL Workbench with this schema design\n\nYou can import this final schema into NoSQL Workbench, a visual tool that provides data modeling, data visualization, and query development features for DynamoDB, to further explore and edit your new project. Follow these steps to get started:\n\nDownload NoSQL Workbench. For more information, see Download NoSQL Workbench for DynamoDB.\n\nDownload the JSON schema file listed above, which is already in the NoSQL Workbench model format.\n\nImport the JSON schema file into NoSQL Workbench. For more information, see Importing an existing data model.\n\nOnce you've imported into NOSQL Workbench, you can edit the data model. For more information, see Editing an existing data model.\n\nTo visualize your data model, add sample data, or import sample data from a CSV file, use the Data Visualizer feature of NoSQL Workbench."
  },
  {
    "title": "Social network schema design in DynamoDB - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-schema-social-network.html",
    "html": "Social network schema design in DynamoDB\nPDF\nRSS\nSocial network business use case\n\nThis use case talks about using DynamoDB as a social network. A social network is an online service that lets different users interact with each other. The social network we'll design will let the user see a timeline consisting of their posts, their followers, who they are following, and the posts written by who they are following. The access patterns for this schema design are:\n\nGet user information for a given userID\n\nGet follower list for a given userID\n\nGet following list for a given userID\n\nGet post list for a given userID\n\nGet user list who likes the post for a given postID\n\nGet the like count for a given postID\n\nGet the timeline for a given userID\n\nSocial network entity relationship diagram\n\nThis is the entity relationship diagram (ERD) we'll be using for the social network schema design.\n\nSocial network access patterns\n\nThese are the access patterns we'll be considering for the social network schema design.\n\ngetUserInfoByUserID\n\ngetFollowerListByUserID\n\ngetFollowingListByUserID\n\ngetPostListByUserID\n\ngetUserLikesByPostID\n\ngetLikeCountByPostID\n\ngetTimelineByUserID\n\nSocial network schema design evolution\n\nDynamoDB is a NoSQL database, so it does not allow you to perform a join - an operation that combines data from multiple databases. Customers unfamiliar with DynamoDB might apply relational database management system (RDBMS) design philosophies (such as creating a table for each entity) to DynamoDB when they do not need to. The purpose of DynamoDB's single-table design is to write data in a pre-joined form according to the application's access pattern, and then immediately use the data without additional computation. For more information, see Single-table vs. multi-table design in DynamoDB.\n\nNow, let's step through how we'll evolve our schema design to address all the access patterns.\n\nStep 1: Address access pattern 1 (getUserInfoByUserID)\n\nTo get a given user's information, we'll need to Query the base table with a key condition of PK=<userID>. The query operation lets you paginate the results, which can be useful when a user has many followers. For more information on Query, see Query operations in DynamoDB.\n\nIn our example, we track two types of data for our user: their \"count\" and their \"info.\" A user's \"count\" reflects how many followers they have, how many users they are following, and how many posts they've created. A user's \"info\" reflects their personal information such as their name.\n\nWe see these two kinds of data represented by the two items below. The item that has \"count\" in its sort key (SK) is more likely to change than the item with \"info.\" DynamoDB considers the size of the item as it appears before and after the update and the provisioned throughput consumed will reflect the larger of these item sizes. So even if you update just a subset of the item's attributes, UpdateItem will still consume the full amount of provisioned throughput (the larger of the before and after item sizes). You can get the items via a single Query operation and use UpdateItem to add or subtract from existing numeric attributes.\n\nStep 2: Address access pattern 2 (getFollowerListByUserID)\n\nTo get a list of users who are following a given user, we'll need to Query the base table with a key condition of PK=<userID>#follower.\n\nStep 3: Address access pattern 3 (getFollowingListByUserID)\n\nTo get a list of users a given user is following, we'll need to Query the base table with a key condition of PK=<userID>#following. You can then use a TransactWriteItems operation group up several requests together and do the following:\n\nAdd User A to User B's follower list, and then increment User B's follower count by one\n\nAdd User B to User A's follower list, and then increment User A's follower count by one\n\nStep 4: Address access pattern 4 (getPostListByUserID)\n\nTo get a list of posts created by a given user, we'll need to Query the base table with a key condition of PK=<userID>#post. One important thing to note here is that a user's postIDs must be incremental: the second postID value must be greater than the first postID value (since users want to see their posts in a sorted manner). You can do this by generating postIDs based on a time value like a Universally Unique Lexicographically Sortable Identifier (ULID).\n\nStep 5: Address access pattern 5 (getUserLikesByPostID)\n\nTo get a list of users who liked a given user's post, we'll need to Query the base table with a key condition of PK=<postID>#likelist. This approach is the same pattern that we used for retrieving the follower and following lists in access pattern 2 (getFollowerListByUserID) and access pattern 3 (getFollowingListByUserID).\n\nStep 6: Address access pattern 6 (getLikeCountByPostID)\n\nTo get a count of likes for a given post, we'll need to perform a GetItem operation on the base table with a key condition of PK=<postID>#likecount. This access pattern can cause throttling issues whenever a user with many followers (such as a celebrity) creates a post since throttling occurs when a partition's throughput exceeds 1000 WCU per second. This problem is not a result of DynamoDB, it just appears in DynamoDB since it's at the end of the software stack.\n\nYou should evaluate whether it's really essential for all users to view the like count simultaneously or if it can happen gradually over time. In general, a post's like count doesn't need to be immediately 100% accurate. You can implement this strategy by putting a queue between your application and DynamoDB to have the updates happen periodically.\n\nStep 7: Address access pattern 7 (getTimelineByUserID)\n\nTo get the timeline for a given user, we'll need to perform a Query operation on the base table with a key condition of PK=<userID>#timeline. Let's consider a scenario where a user's followers need to view their post synchronously. Every time a user writes a post, their follower list is read and their userID and postID are slowly entered into the timeline key of all its followers. Then, when your application starts, you can read the timeline key with the Query operation and fill the timeline screen with a combination of userID and postID using the BatchGetItem operation for any new items. You cannot read the timeline with an API call, but this is a more cost effective solution if the posts could be edited frequently.\n\nThe timeline is a place that shows recent posts, so we'll need a way to clean up the old ones. Instead of using WCU to delete them, you can use DynamoDB's TTL feature to do it for free.\n\nAll access patterns and how the schema design addresses them are summarized in the table below:\n\nAccess pattern\tBase table/GSI/LSI\tOperation\tPartition key value\tSort key value\tOther conditions/filters\ngetUserInfoByUserID\tBase table\tQuery\tPK=<userID>\t\t\ngetFollowerListByUserID\tBase table\tQuery\tPK=<userID>#follower\t\t\ngetFollowingListByUserID\tBase table\tQuery\tPK=<userID>#following\t\t\ngetPostListByUserID\tBase table\tQuery\tPK=<userID>#post\t\t\ngetUserLikesByPostID\tBase table\tQuery\tPK=<postID>#likelist\t\t\ngetLikeCountByPostID\tBase table\tGetItem\tPK=<postID>#likecount\t\t\ngetTimelineByUserID\tBase table\tQuery\tPK=<userID>#timeline\t\t\nSocial network final schema\n\nHere is the final schema design. To download this schema design as a JSON file, see DynamoDB Examples on GitHub.\n\nBase table:\n\nUsing NoSQL Workbench with this schema design\n\nYou can import this final schema into NoSQL Workbench, a visual tool that provides data modeling, data visualization, and query development features for DynamoDB, to further explore and edit your new project. Follow these steps to get started:\n\nDownload NoSQL Workbench. For more information, see Download NoSQL Workbench for DynamoDB.\n\nDownload the JSON schema file listed above, which is already in the NoSQL Workbench model format.\n\nImport the JSON schema file into NoSQL Workbench. For more information, see Importing an existing data model.\n\nOnce you've imported into NOSQL Workbench, you can edit the data model. For more information, see Editing an existing data model.\n\nTo visualize your data model, add sample data, or import sample data from a CSV file, use the Data Visualizer feature of NoSQL Workbench."
  },
  {
    "title": "Data modeling schema design packages in DynamoDB - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-schemas.html",
    "html": "Data modeling schema design packages in DynamoDB\nPDF\nRSS\n\nThis section covers the data layer to go over different examples you can use in your DynamoDB table design. Each example will go over their use case, access patterns, how to achieve the access patterns, and then what the final schema will look like.\n\nPrerequisites\n\nBefore we attempt to design our schema for DynamoDB, we must first gather some prerequisite data on the use case the schema needs to support. Unlike relational databases, DynamoDB is sharded by default, meaning that the data will live on multiple servers behind the scenes so designing for data locality is important. We'll need to put together the following list for each schema design:\n\nList of entities (ER Diagram)\n\nEstimated volumes and throughput for each entity\n\nAccess patterns that need to be supported (queries and writes)\n\nData retention requirements\n\nTopics\nSocial network schema design in DynamoDB\nGaming profile schema design in DynamoDB\nComplaint management system schema design in DynamoDB\nRecurring payments schema design in DynamoDB\nMonitoring device status updates in DynamoDB\nUsing DynamoDB as a data store for an online shop"
  },
  {
    "title": "Data modeling building blocks in DynamoDB - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-blocks.html",
    "html": "Data modeling building blocks in DynamoDB\nPDF\nRSS\n\nThis section covers the building block layer to give you design patterns you can use in your application.\n\nTopics\nComposite sort key building block\nMulti-tenancy building block\nSparse index building block\nTime to live building block\nTime to live for archival building block\nVertical partitioning building block\nWrite sharding building block\nComposite sort key building block\n\nWhen people think of NoSQL, they may also think of it as non-relational. Ultimately, there is no reason relationships cannot be placed into a DynamoDB schema, they just look different than relational databases and their foreign keys. One of the most critical patterns we can use to develop a logical hierarchy of our data in DynamoDB is a composite sort key. The most common style for designing one is with each layer of the hierarchy (parent layer > child layer > grandchild layer) separated by a hashtag. For example, PARENT#CHILD#GRANDCHILD#ETC.\n\nWhile a partition key in DynamoDB always requires the exact value to query for data, we can apply a partial condition to the sort key from left to right similar to traversing a binary tree.\n\nIn the example above, we have an e-Commerce store with a Shopping Cart that needs to be maintained across user sessions. Whenever the user logs in, they may want to see the entire Shopping Cart including items saved for later. But when they enter the checkout, only items in the active cart should be loaded for purchase. Since both of these KeyConditions explicitly ask for CART sort keys, the additional wishlist data is simply ignored by DynamoDB at read time. While both saved and active items are a part of the same cart, we need to treat them differently in different parts of the application, so applying a KeyCondition to the prefix of the sort key is the most optimized way of retrieving only the data needed for each part of the application.\n\nKey features of this building block\n\nRelated items are stored locally to each other for effective data access\n\nUsing KeyCondition expressions, subsets of the hierarchy can be selectively retrieved meaning there are no wasted RCUs\n\nDifferent parts of the application can store their items under a specific prefix preventing overwritten items or conflicting writes\n\nMulti-tenancy building block\n\nMany customers use DynamoDB to host data for their multi-tenant applications. For these scenarios, we want to design the schema in a way that keeps all data from a single tenant in its own logical partition of the table. This leverages the concept of an Item Collection, which is a term for all items in a DynamoDB table with the same partition key. For more information on how DynamoDB approaches multitenancy, see Multitenancy on DynamoDB.\n\nFor this example, we are running a photo hosting site with potentially thousands of users. Each user will only upload photos to their own profile initially, but by default we will not allow a user to see the photos of any other user. An additional level of isolation would ideally be added to the authorization of each user's call to your API to ensure they are only requesting data from their own partition, but at the schema level, unique partition keys is adequate.\n\nKey features of this building block\n\nThe amount of data read by any one user or tenant can only be as much as the total amount of items in their partition\n\nRemoval of a tenant's data due to an account closure or compliance request can be done tactfully and cheaply. Simply run a query where the partition key equals their tenant ID, then execute a DeleteItem operation for each primary key returned\n\nNote\n\nDesigned with multi-tenancy in mind, you can use different encryption key providers across a single table to safely isolate data. AWS Database Encryption SDK for Amazon DynamoDB enables you to include client-side encryption in your DynamoDB workloads. You can perform attribute-level encryption, enabling you to encrypt specific attribute values before storing them in your DynamoDB table and search on encrypted attributes without decrypting the entire database beforehand.\n\nSparse index building block\n\nSometimes an access pattern requires looking for items that match a rare item or an item that receives a status (which requires an escalated response). Rather than regularly query across the entire dataset for these items, we can leverage the fact that global secondary indexes (GSI) are sparsely loaded with data. This means that only items in the base table that have the attributes defined in the index will be replicated to the index.\n\nIn this example, we see an IOT use case where each device in the field is reporting back a status on a regular basis. For the majority of the reports we expect the device to report everything is okay, but on occasion there can be a fault and it must be escalated to a repair technician. For reports with an escalation, the attribute EscalatedTo is added the item, but is not present otherwise. The GSI in this example is partitioned by EscalatedTo and since the GSI brings over keys from the base table we can still see which DeviceID reported the fault and at what time.\n\nWhile reads are cheaper than writes in DynamoDB, sparse indexes are a very powerful tool for use cases where instances of a specific type of item is rare but reads to find them are common.\n\nKey features of this building block\n\nWrite and storage costs for the sparse GSI only apply to items that match the key pattern, so the cost of the GSI can be substantially less than other GSIs that have all items replicated to them\n\nA composite sort key can still be used to further narrow down the items that match the desired query, for instance, a timestamp could be used for the sort key to only view faults reported in the last X minutes (SK > 5 minutes ago, ScanIndexForward: False)\n\nTime to live building block\n\nMost data have some duration of time for which it can be considered worth keeping in a primary datastore. To facilitate data aging out from DynamoDB, it has a feature called time to live (TTL). The TTL feature allows you to define a specific attribute at the table level that needs monitoring for items with an epoch timestamp (that's in the past). This allows you to delete expired records from the table for free.\n\nNote\n\nIf you are using Global Tables version 2019.11.21 (Current) of global tables and you also use the Time to Live feature, DynamoDB replicates TTL deletes to all replica tables. The initial TTL delete does not consume write capacity in the Region in which the TTL expiry occurs. However, the replicated TTL delete to the replica table(s) consumes replicated write capacity in each of the replica Regions and applicable charges will apply.\n\nIn this example, we have an application designed to let a user create messages that are short-lived. When a message is created in DynamoDB, the TTL attribute is set to a date seven days in the future by the application code. In roughly seven days, DynamoDB will see that the epoch timestamp of these items is in the past and delete them.\n\nSince the deletes done by TTL are free, it is strongly recommended to use this feature to remove historical data from the table. This will reduce the overall storage bill each month and will likely reduce the costs of user reads since there will be less data to be retrieved by their queries. While TTL is enabled at the table level, it is up to you which items or entities to create a TTL attribute for and how far into the future to set the epoch timestamp to.\n\nKey features of this building block\n\nTTL deletes are run behind the scenes with no impact to your table performance\n\nTTL is an asynchronous process that runs roughly every six hours, but can take over 48 hours for an expired record to be deleted\n\nDo not rely on TTL deletes for use cases like lock records or state management if stale data must be cleaned up in less than 48 hours\n\nYou can name the TTL attribute a valid attribute name, but the value must be a number type\n\nTime to live for archival building block\n\nWhile TTL is an effective tool for deleting older data from DynamoDB, many use cases require an archive of the data be kept for a longer period of time than the primary datastore. In this instance, we can leverage TTL's timed deletion of records to push expired records into a long-term datastore.\n\nWhen a TTL delete is done by DynamoDB, it is still pushed into the DynamoDB Stream as a Delete event. When DynamoDB TTL is the one who performs the delete though, there is an attribute on the stream record of principal:dynamodb. Using a Lambda subscriber to the DynamoDB Stream, we can apply an event-filter for only the DynamoDB principal attribute and know that any records that match that filter are to be pushed to an archival store like S3 Glacier.\n\nKey features of this building block\n\nOnce the low-latency reads of DynamoDB are no longer needed for the historical items, migrating them to a colder storage service like S3 Glacier can reduce storage costs significantly while meeting the data compliance needs of your use case\n\nIf the data is persisted into Amazon S3, cost-efficient analytics tools like Amazon Athena or Redshift Spectrum can be used to perform historical analysis of the data\n\nVertical partitioning building block\n\nUsers familiar with a document model database will be familar with the idea of storing all related data within a single JSON document. While DynamoDB supports JSON data types, it does not support excuting KeyConditions on nested JSON. Since KeyConditions are what dictate how much data is read from disk and effectively how many RCUs a query consumes, this can result in inefficiencies at scale. To better optimize the writes and reads of DynamoDB, we recommend breaking apart the document's individual entities into individual DynamoDB items, also referred to as vertical partitioning.\n\nVertical partitoning, as shown above, is a key example of single table design in action but can also be implemented across multiple tables if desired. Since DynamoDB bills writes in 1KB increments, you should ideally partition the document in a way that results in items under 1KB.\n\nKey features of this building block\n\nA hierarchy of data relationships is maintained via sort key prefixes so the singular document structure could be rebuilt client-side if needed\n\nSingular components of the data structure can be updated independently resulting in small item updates being only 1 WCU\n\nBy using the sort key BeginsWith, the application can retrieve similar data in a single query, aggregating read costs for reduced total cost/latency\n\nLarge documents can easily be larger than the 400 KB individual item size limit in DynamoDB and vertical partitioning helps work around this limit\n\nWrite sharding building block\n\nOne of the very few hard limits DynamoDB has in place is the restriction of how much throughput a single physical partition can maintain per second (not necessarily a single partition key). These limits are presently:\n\n1000 WCU (or 1000 <=1KB items written per second) and 3000 RCU (or 3000 <=4KB reads per second) strongly consistent or\n\n6000 <=4KB reads per second eventually consistent\n\nIn the event requests against the table exceed either of these limits, an error is sent back to the client SDK of ThroughputExceededException, more commonly referred to as throttling. Use cases that require read operations beyond that limit will mostly be served best by placing a read cache in front of DynamoDB, but write operations require a schema level design known as write sharding.\n\nTo solve this problem, we'll append a random integer onto the end of the partition key for each contestant in the application's UpdateItem code. The range of the random integer generator will need to have an upper bound matching or exceeding the expected amount of writes per second for a given contestant divided by 1000. To support 20,000 votes per second, it would look like rand(0,19). Now that the data is stored under separate logical partitions, it must be combined back together at read time. Since vote totals doesn't need to be real time, a Lambda function scheduled to read all vote partitions every X minutes could perform occasional aggregation for each contestant and write it back to a single vote total record for live reads.\n\nKey features of this building block\n\nFor use cases with extremely high write throughput for a given partition key that cannot be avoided, write operations can be artificially spread across multiple DynamoDB partitions\n\nGSIs with a low cardinality partition key should also utilize this pattern since throttling on a GSI will apply backpressure to write operations on the base table"
  },
  {
    "title": "Data Modeling foundations in DynamoDB - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling-foundations.html",
    "html": "Data Modeling foundations in DynamoDB\nPDF\nRSS\n\nThis section covers the foundation layer by examining the two types of table design: single table and multiple table.\n\nSingle table design foundation\n\nOne choice for the foundation of our DynamoDB schema is single table design. Single table design is a pattern that allows you to store multiple types (entities) of data in a single DynamoDB table. It aims to optimize data access patterns, improve performance, and reduce costs by eliminating the need for maintaining multiple tables and complex relationships between them. This is possible because DynamoDB stores items with the same partition key (known as an item collection) on the same partition(s) as each other. In this design, different types of data are stored as items in the same table, and each item is identified by a unique sort key.\n\nAdvantages\n\nData locality to support queries for multiple entity types in a single database call\n\nReduces overall financial and latency costs of reads:\n\nA single query for two items totalling less than 4KB is 0.5 RCU eventually consistent\n\nTwo queries for two items totalling less than 4KB is 1 RCU eventually consistent (0.5 RCU each)\n\nThe time to return two separate database calls will average higher than a single call\n\nReduces the number of tables to manage:\n\nPermissions do not need to be maintained across multiple IAM roles or policies\n\nCapacity management for the table is averaged across all entities, usually resulting in a more predictable consumption pattern\n\nMonitoring requires fewer alarms\n\nCustomer Managed Encryption Keys only need to be rotated on one table\n\nSmooths traffic to the table:\n\nBy aggregating multiple usage patterns to the same table, the overall usage tends to be smoother (the way a stock index's performance tends to be smoother than any individual stock) which works better for achieving higher utilization with provisioned mode tables\n\nDisadvantages\n\nLearning curve can be steep due to paradoxical design compared to relational databases\n\nData requirements must be consistent across all entity types\n\nBackups are all or nothing so if some data is not mission critical, consider keeping it in a separate table\n\nTable encryption is shared across all items. For multi-tenant applications with individual tenant encryption requirements, client side encryption would be required\n\nTables with a mix of historical data and operational data will not see as much of a benefit from enabling the Infrequent Access Storage Class. For more information, see Table classes\n\nAll changed data will be propagated to DynamoDB Streams even if only a subset of entities need to be processed.\n\nThanks to Lambda event filters, this will not affect your bill when using Lambda, but will be an added cost when using the Kinesis Consumer Library\n\nWhen using GraphQL, single table design will be more difficult to implement\n\nWhen using higher-level SDK clients like Java's DynamoDBMapper or Enhanced Client, it can be more difficult to process results because items in the same response may be associated with different classes\n\nWhen to use\n\nSingle table design is the recommended design pattern for DynamoDB unless your use case would be impacted heavily by one of the above disadvantages. For most customers, the long term benefits outweigh the short term challenges of designing their tables this way.\n\nMultiple table design foundation\n\nThe second choice for the foundation of our DynamoDB schema is multiple table design. Multiple table design is a pattern that is more like a traditional database design where you store a single type(entity) of data in a each DynamoDB table. Data within each table will still be organized by partition key so performance within a single entity type will be optimized for scalability and performance, but queries across multiple tables must be done independently.\n\nAdvantages\n\nSimpler to design for those who aren't used to working with single table design\n\nEasier implementation of GraphQL resolvers due to each resolver mapping to a single entity(table)\n\nAllows for unique data requirements across different entity types:\n\nBackups can be made for the individual tables that are mission critical\n\nTable encryption can be managed for each table. For multi-tenant applications with individual tenant encryption requirements, separate tenant tables make it possible for each customer to have their own encryption key\n\nInfrequent Access Storage Class can be enabled on just the tables with historical data to realize the full cost savings benefit. For more information, see Table classes\n\nEach table will have its own change data stream allowing for a dedicated Lambda function to be designed for each type of item rather than a single monolithic processor\n\nDisadvantages\n\nFor access patterns that require data across multiple tables, multiple reads from DynamoDB will be required and data may need to be processed/joined on the client code.\n\nOperations and monitoring of multiple tables requires more CloudWatch alarms and each table must be scaled independently\n\nEach tables permissions will need to be managed separately. The addition of tables in the future will require a change to any necessary IAM roles or policies\n\nWhen to use\n\nIf your application’s access patterns do not have the need to query multiple entities or tables together, then multiple table design is a good and sufficient approach."
  },
  {
    "title": "Data modeling for DynamoDB tables - Amazon DynamoDB",
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/data-modeling.html",
    "html": "Data modeling for DynamoDB tables\nPDF\nRSS\n\nBefore we dive into data modeling, it's important to understand some DynamoDB fundamentals. DynamoDB is a key-value NoSQL database which allows flexible schema. The set of data attributes apart from the key attributes for each item can be either uniform or discrete. The DynamoDB key schema is in the form of either a simple primary key where a partition key uniquely identifies an item, or in the form of a composite primary key where a combination of a partition key and sort key uniquely defines an item. The partition key is hashed to determine the physical location of data and retrieve it. Therefore, it is important to choose a high cardinality and horizontally scalable attribute as a partition key to ensure even distribution of data. The sort key attribute is optional in the key schema and having a sort key enables modelling one-to-many relationships and creating item collections in DynamoDB. Sort keys are also referred to as range keys—they are used to sort items in an item collection and also allow flexible range-based operations.\n\nFor more details and best practices on DynamoDB key schema, you can refer to the following:\n\nPartitions and data distribution\n\nBest practices for designing and using partition keys effectively\n\nBest practices for using sort keys to organize data\n\nChoosing the right DynamoDB partition key\n\nSecondary indexes are often needed to support additional query patterns in DynamoDB. Secondary indexes are shadow tables where the same data is organised via a different key schema compared to the base table. A local secondary index (LSI) shares the same partition key as the base table and allows having an alternate sort key allowing it to share the base table’s capacity. A global secondary index (GSI) can have a different partition key as well as a different sort key attribute than the base table which means throughput management for a GSI is independent of the base table.\n\nFor more details on secondary indexes and best practices, you can refer to the following:\n\nImproving data access with secondary indexes\n\nBest practices for using secondary indexes in DynamoDB\n\nLet's now look at data modeling a little closer. The process of designing a flexible and highly-optimized schema on DynamoDB, or any NoSQL database for that matter, can be a challenging skill to learn. The goal of this module is to help you develop a mental flowchart for designing a schema that will take you from use case into production. We will start with an introduction to the foundational choice of any design, single table versus multiple table design. Then we will review the multitude of design patterns (building blocks) that can be used to achieve various organizational or performance results for your application. Finally, we are including a variety of complete schema design packages for different use cases and industries.\n\nTopics\nData Modeling foundations in DynamoDB\nData modeling building blocks in DynamoDB\nData modeling schema design packages in DynamoDB"
  }
]