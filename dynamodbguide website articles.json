[
  {
    "title": "Additional Reading | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/additional-reading/",
    "html": "Articles:\nSQL, NoSQL, and Scale: How DynamoDB scales where relational databases don't - This is a post of mine explaining the core architectural decisions that allow NoSQL databases to scale further than their SQL brethren.\nThe What, Why, and When of Single-Table Design with DynamoDB - A deep look at what it means to do single-table design in DynamoDB and why you would want to. It also includes a few situations where you may want to avoid single-table design.\nFaux-SQL or NoSQL? Examining four DynamoDB Patterns in Serverless Applications - This is a post I wrote on the common data modeling patterns I see with DynamoDB in serverless applications.\nWhy Amazon DynamoDB isn't for everyone - My favorite post on this topic. Forrest Brazeal does a great job breaking down the pros and cons of DynamoDB.\nFrom relational DB to single DynamoDB table: a step-by-step exploration - Another great post by Forrest Brazeal. It's a detailed walkthrough of how to use the single-table DynamoDB pattern in a complex use case.\nWhy the PIE theorem is more relevant than the CAP theorem - Another post I wrote about choosing a database that includes consideration of DynamoDB.\nVideos:\nAdvanced Design Patterns for DynamoDB (reInvent 2017). Rick Houlihan is a master of DynamoDB and has some great tips.\nAdvanced Design Pattens for DynamoDB (reInvent 2018). Rick Houlihan is back with more tips. The first half is similar to 2017, but the second half has different examples. Highly recommended. Get the slides here.\nAdvanced Design Patterns for DynamoDB (reInvent 2019). The most recent edition of Rick Houlihan's DynamoDB talk.\nAdvanced Design Patterns for DynamoDB (reInvent 2020) Part 1 & Part 2\nData Modeling with DynamoDB (reInvent 2020) -- Part 1 & Part 2. An intermediate level look at the concepts behind DynamoDB to understand the core data modeling principles.\nData Modeling with DynamoDB (reInvent 2019). A gentler introduction to DynamoDB single-table concepts. Watch this video if Rick's is too advanced.\nUsing (and ignoring) DynamoDB Best Practices in Serverless (ServerlessConf NYC 2019). This talk focuses on using DynamoDB in Serverless applications.\nReference material:\nAwesome DynamoDB -- A GitHub repo with DynamoDB links and resources.\nDynamoDB landing page\nAWS Developer Guide Docs\nAWS CLI reference for DynamoDB\nBoto3 (Python client library for AWS) docs\nJavascript client library for AWS\nDocument Client cheat sheet (Javascript). Created by Nader Dabit."
  },
  {
    "title": "MongoDB vs. DynamoDB | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/mongo-db-vs-dynamo-db/",
    "html": "MongoDB is an open-source document database first released in 2009. It was enormously influential in kicking off the NoSQL revolution due to its strong focus on developer experience and ease of use.\n\nIn this article, we will compare MongoDB with DynamoDB. In doing so, we will compare these two databases in four main areas:\n\nPopularity and availability\nData modeling\nOperations model\nConnection & security model\n\nLet's get started!\n\nPopularity and availability\n\nTo begin, let's take a look at the relative popularity of the two databases.\n\nAs of February 2020, MongoDB is the fifth most-popular database and DynamoDB is the sixteenth most-popular database:\n\nMongoDB's lead is due to a few reasons. First, MongoDB was released three years earlier than DynamoDB. This allowed MongoDB to build a healthy user base, particularly as it focused on developer experience and happiness. MongoDB was used by a lot of application developers that didn't necessarily need the scale of NoSQL but did enjoy the ergonomics of working with a schemaless, document-oriented database.\n\nIf we look at the popularity trendline over time, you can see that MongoDB started with a huge lead back in 2013 when DB-Engines started tracking. Over time, DynamoDB has been closing that gap.\n\nA second reason is that MongoDB is open-source and freely available, where DynamoDB is a proprietary database from Amazon Web Services (AWS). This necessarily limits DynamoDB to developers that are using AWS, while MongoDB can be run on other cloud providers or even on-prem. While this availability does increase the places where you can use MongoDB, I argue in operations section below that the proprietary nature of DynamoDB is an asset.\n\nData modeling\n\nLet's move on to the data modeling aspect of these two databases. How does MongoDB's data model compare to DynamoDB's data model?\n\nAt a quick glance, you might say MongoDB is a document database while DynamoDB is a wide-column store. While true, those descriptions are opaque and obscure the underlying similarities between the two databases. To accurately answer this question, you really need to split this into two categories: high-scale and low-scale.\n\nData modeling at high scale\n\nNoSQL databases were designed to handle high-scale applications that relational databases could not handle. To do this, NoSQL databases removed some features from relational databases that were sources of slowness, like joins and aggregations. They also use notions of partitions to help shard data across multiple storage nodes, allowing for consistently fast response times as your data scales.\n\nIf you are working with high scale, your data modeling is going to be very similar between DynamoDB and MongoDB. You'll need to take advantage of single-table design to \"pre-join\" your data by colocating related items near each other. And you'll need to handle aggregations yourself by maintaining counters rather than relying on read-time aggregations.\n\nAt this scale, the differences between document databases and wide-column stores fade away. It's in the lower-scale use cases that the differences between MongoDB and DynamoDB are more apparent.\n\nData modeling at lower scale\n\nNot every application has the scalability concerns of Amazon.com or Lyft. Most applications have significantly lower data volume and velocity. So how do the data models of DynamoDB and MongoDB compare at lower scale?\n\nAt lower scale, there is a clear difference in philosophy between DynamoDB and MongoDB.\n\nDynamoDB's philosophy is that it won't let you write a bad query. And by 'bad query', that means a query that won't scale. Even if your data is less than 10 GB, you won't be able to do things like joins or aggregations with DynamoDB. This results in a rigid data model. On the plus side, you know exactly what you're getting with DynamoDB. Your performance won't degrade over time, even if your application scales to 1000x the original size.\n\nMongoDB has a different philosophy. It is designed for developer productivity and easier on-boarding. This has a few implications. First, it's easier to make the transition from a relational database to MongoDB. MongoDB has significantly more flexible indexing capabilities, limited join capabilities, and an aggregation framework that allows for read-time aggregations. For those on the on-ramp to NoSQL, this can make it easier.\n\nHowever, this flexibility comes with a cost. Joins and aggregations have performance characteristics that are difficult to know up front and can degrade significantly as your data changes. The time you saved up front may bite you down the road as you're performing a significant refactor to rip out joins and aggregations.\n\nOperations model\n\nWith the data modeling section out of the way, let's move on to the operations model. After all, there's more to a database than simply using it in your application. You also need to make sure you can run it in a fast, safe, and reliable way.\n\nThe operations models for MongoDB and DynamoDB are quite different and are directly related to the availability model discussed above. MongoDB is freely-available software that you can run yourself while DynamoDB is only available as a service from AWS. This has big implications on operations.\n\nOperationally, MongoDB uses a more traditional model for databases. You will install the MongoDB software onto one or more compute instances (whether bare metal, virtual machines, or containers). Each of these instances will have some amount of compute, memory, and storage associated with it that will be used by MongoDB.\n\nDynamoDB has a more radical model. Rather than thinking about instances, you only think about usage. You are not responsible for installing software, managing servers, or increasing your cluster size. You only tell AWS how much read and write capacity you want (or skip it altogether by using On-Demand Pricing). As your data usage needs grow, you simply increase the capacity you need.\n\nAWS will completely handle the instances in their DynamoDB fleet, so you don't need to think about instance failure or degredation. You don't need to worry about backups. You don't need to worry about increasing your cluster size.\n\nThere are more 'managed' MongoDB offerings in recent years, such as MongoDB Atlas or AWS DocumentDB (with MongoDB compatibility). These offerings help with the low-level aspects of running a database, but you are still bound to instance-based thinking. Scaling down is near impossible. Scaling up requires increase the size or number of instances and the associated migration. Rick Houlihan has shared stories of MongoDB customers that need to wait nine months to add shards due to all the rebalancing of data that occurs.\n\nAs someone who has managed database instances before, the service-based, pay-per-use model of DynamoDB is a huge advancement over the instance-based model of prior databases.\n\nConnection & security model\n\nThe last area I want to discuss is the connection and security model. This section is most relevant if you're using serverless compute like AWS Lambda, but it can affect other concerns as well.\n\nMongoDB is like most traditional, server-based databases. You have a known number of instances of your database. To protect your database from the outside world, you usually place your MongoDB instance in a private area of your network. Your application servers have network access to your MongoDB databases but the public internet does not. Your application servers will spin up a persistent connection over TCP to interact with MongoDB.\n\nIn contrast, DynamoDB is a service-based database hosted by AWS. With DynamoDB, all access is done using the DynamoDB API over HTTPS. Rather than relying on network partitioning to protect your database, DynamoDB relies on AWS IAM permissions to authenticate and authorize clients.\n\nIf you're using traditional compute like virtual machines or containers, this doesn't make much of a difference either way. However, if you're using hyper-ephemeral compute like AWS Lambda, this can make a huge difference.\n\nWith traditional compute, your compute instances are fairly long-lived. This could be days or months, with VMs, or it could be hours with containers. The main point is that your compute sticks around to handle multiple requests over a period of time.\n\nWith compute like AWS Lambda, you have tiny compute instances spinning up and going away constantly. For these ephemeral compute instances, it's harder to work with a traditional server-based database. You need to make sure your compute has the relevant network access to access MongoDB. This adds latency to your compute initialization and complexity to your infrastructure.\n\nThis connection and security model is a major reason why AWS Lambda users have been reaching for DynamoDB over traditional databases.\n\nConclusion\n\nIn this post, we saw how DynamoDB compares to MongoDB. The major points are:\n\nMongoDB is more popular than DynamoDB, partly due to its head start and partly due to its wider availability.\nDynamoDB has a more hands-off operations model than MongoDB as you don't think to think about instances and scaling up or down.\nAt high scale, DynamoDB and MongoDB have very similar data modeling principles.\nAt lower scale, MongoDB has a more flexible data model.\nDynamoDB's connection and security model make it a popular choice for use with serverless compute like AWS Lambda."
  },
  {
    "title": "Local or global: Choosing a secondary index type in DynamoDB | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/local-or-global-choosing-a-secondary-index-type-in-dynamo-db/",
    "html": "Secondary indexes are a critical part of modeling your data in DynamoDB. With a secondary index, you can add additional access patterns to your application without the hassle of maintaining multiple copies of the same data. DynamoDB will handle all replication from the base table to your secondary index.\n\nThere are two types of secondary indexes: local secondary indexes and global secondary indexes. They are pretty similar overall, but there are some slight differences in how they function.\n\nIn this lesson, we'll see how to choose between a local and global secondary index. We'll cover:\n\nThe benefits of a local secondary index;\nThe limitations of a local secondary index.\nThe TL;DR flow chart version of choosing an index type\nThe benefits of a local secondary index\n\nI default to using global secondary index for all of my indexes and rarely consider using local secondary indexes. That said, there are two features of local secondary indexes that you can't get with global secondary indexes. These features are:\n\nStrongly-consistent reads\nReuse of base table capacity\n\nLet's review each of these.\n\nStrongly-consistent reads\n\nWhen reading from your base table in DynamoDB, you can choose between strongly consistent reads and eventually consistent reads. A strongly consistent read will return the latest value for a given item, incorporating all successful write operations that have occurred. An eventually consistent read will likely include the latest value for a given item but may reflect slightly stale data. This is due to how DynamoDB (and NoSQL databases generally) replicate data across replicas.\n\nBy default, all reads will be eventually consistent. An eventually consistent read consumes half the read capacity units of a strongly consistent read. If you want a strongly consistent read, you'll need to include the ConsistentRead property in your API request.\n\nWith a local secondary index, you have the same options as with a base table. By default, all reads from a local secondary index are eventually consistent. However, you may opt into a strongly consistent read if you need it. With a global secondary index, you don't have this option. All reads from a global secondary index are eventually consistent.\n\nIf you have strong consistency requirements in your application, the local secondary index is a better choice for you.\n\nReuse of base table capacity\n\nA second benefit of local secondary indexes is that they reuse the provisioned throughput from your base table. In certain situations, this can be cheaper than global secondary indexes.\n\nIf you're using the Provisioned pricing mode for DynamoDB, you'll provision a certain number of read and write capacity units for your DynamoDB table. This specifies the maximum number of read and write operations per second that your table can handle.\n\nIf you're using a global secondary index, you will need to specify additional read and write capacity units for that index separately from your main table. However, with a local secondary index, you will use the capacity from your base table.\n\nIn most situations, this won't save you much money. You'll need to figure the capacity needed for your main table and the capacity needed for your local secondary index. Add them together, and this is the number you'll need for your base table's provisioned throughput.\n\nHowever, these shared capacity units could be helpful if you have an uneven workload across time such that sharing units could reduce the overall capacity units you need. This could come up in two scenarios.\n\nFirst, if your read patterns on your base table and secondary index are temporally different. Perhaps your base table is read a lot in the morning and afternoon, while your local secondary index is read a lot overnight when runnning reports. In this case, you wouldn't need to provision capacity units on two separate entities that both account for their respective high water marks. Rather, you could share across them.\n\nThe second example is if you had multiple sparse local secondary indexes on your table. Generally, a write to your table with a local secondary index will cost 2X what it would cost for just the base table, as any write will be replicated to the index as well. However, if you had three sparse indexes on your table, and any write to your base table was only replicated to a single local secondary index, you could get efficiencies by sharing the capacity units across the multiple indexes.\n\nIn reality, both of these situations aren't that common. It's unlikely you'll be able to save a meaningful amount of money by choosing a local secondary index over a global secondary index.\n\nDownsides of local secondary indexes\n\nNow that we've seen the potential upsides of a local secondary index, let's take a look at the downsides. There are three main downsides to local secondary indexes:\n\nMust use the same partition key as the base table\nMust be created when the table is created\nAdds a 10GB limit to any single item collection\n\nLet's review each of these in turn.\n\nMust use the same partition key as the base table\n\nThe most common reason that a local secondary index won't work for your application is because a local secondary index must use the same partition key as your base table. For many application patterns, this is not feasible.\n\nThat said, this can be a useful pattern for some folks. If you have a dataset where you're usually searching within a particular partition but have different sorting patterns within the partion, it can be a nice fit.\n\nOne example is a table containing actors and actresses and the various movies in which they've appeared. You might have an access pattern where you want to fetch all movies for a particular actor in a way where you can filter based on the name of the movie. Your base table would look as follows:\n\nIn this table, the actor's name is the partition key and the movie name is the sort key.\n\nThen, you might have a second access pattern that wants to find all movies for an actor within a given time range. You could set up a local secondary index where the partition key is Actor and the sort key is Year.\n\nNote that this is the same data, it has just been rearranged due to a secondary indexes. In this example, the partition key is the same for our base table and index -- the actor's name.\n\nIf you had a third access pattern where you wanted to find all the actors in a particular movie, a local secondary index wouldn't work. In this secondary index, you'd need to make Movie your partition key and Actor your sort key.\n\nBecause the partition key is different than your base table, you couldn't use a local secondary index.\n\nLocal secondary indexes must be created when the table is created\n\nA second reason that local secondary indexes aren't used is because of the timing -- they must be created when the table is created. If your table already exists and you want to add an additional secondary index to enable more access patterns, you must use a global secondary index.\n\nIn general, this shouldn't be a huge deal. When using DynamoDB, you should plan for all of your access patterns up front, before you design and create your table. It is at this point that you can plan for a local secondary index.\n\nThat said, applications evolve and access patterns change. You may find yourself adding secondary indexes after your table has been live for a while. In those cases, a global secondary index is your only option.\n\nLocal secondary indexes add an item collection size limit\n\nThe final downside to local secondary indexes is more obscure but can really bite you in the wrong situation. If you add a local secondary index, there is a 10GB limit to any given item collection.\n\nWhat is an item collection? I'm glad you asked! An item collection is all of the items with the same partition key in your base table and local secondary indexes. For example, in our Movies example from earlier, we would have three item collections: Tom Hanks, Tim Allen, and Natalie Portman.\n\nIn a single actor's partition, you would need to add up the total size of the items to get the size of the item collection. When you're using a local secondary index, the item collection includes the size of all items with that partition key in the local secondary index as well. If you're projecting the full item into your index, this would double the size of your item collection.\n\nThis won't be a problem for most applications if you have a partition key with high cardinality to distribute values across your key space. But what if you had an actor that's been in a lot of movies?\n\nSamuel L. Jackson has been in a lot of movies. Now you might hit the item collection limit. This would block reads on your base table if you would exceed the 10GB limit, which would be a frightening surprise.\n\nNote that the 10GB item collection size limit does not apply at all if you don't have a local secondary index on your table. If you're using local secondary indexes and are worried about hitting the size limit, the DynamoDB docs have good advice on monitoring the size of your item collections to alert you before it's too late.\n\nThe 'Too long; Didn't Read' version of choosing an index\n\nIn the sections above, we walked through the pros and cons of local secondary indexes. A global secondary index is a more vanilla version of the local secondary index. It doesn't have the upsides like strong consistency or shared capacity, but it also doesn't have the downsides of item collection size limits or the same partition key requirement.\n\nFor those of you that want a quick answer to choosing an index, this section has the quick version. The basic flow is as follows:\n\nStarting at the top left, work your way down the left side to see if there's any reason you should consider a local secondary index. If neither of the two main reasons help you, then choose a global secondary index.\n\nIf you think you do have a reason for using a local secondary index, then move to the right side to see if there will be any reasons you can't use a local secondary index.\n\nDoes my secondary index have a different partition key from my base table?\nDoes my table already exist?\nWill one of my item collections exceed 10GB?\n\nIf the answer to all of these questions is \"No\", then I could use a local secondary index. If any of them are \"Yes\", then I need to rethink my needs, recreate my table, or just use a global secondary index."
  },
  {
    "title": "Hierarchical Data | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/hierarchical-data/",
    "html": "In this example, we'll show how to model hierarchical data using DynamoDB. We'll insert a real dataset of 25,000 Starbucks locations into DynamoDB. You can follow along with some real example code from the examples directory in this site's repository.\n\nHierarchical data is a common relational data pattern for representing tree-like data structures, such as an organizational structure, a project breakdown list, or even a family tree. In a relational database, it often uses quite a few JOINs to get your answer. Here, we'll see how we can model this data using a single table to enable fast, precise lookups.\n\nThis example is inspired by Rick Houlihan's talk at reInvent 2017. Check here for the relevant section.\n\nExample background\n\nImagine we are Starbucks, a multi-national corporation with locations all around the globe. We want to keep our store locations in DynamoDB, and we have five main access patterns:\n\nRetrieve a single store by its Store Number;\nGather all stores in a particular country;\nGather all stores in a particular state or province;\nGather all stores in a particular city; and\nGather all stores in a particular zip code.\n\nThe first query pattern is straight-forward -- that's a 1:1 relationship using a simple key structure. The next four are more difficult. It could require four different global secondary indexes on each of those distinct attributes. Alternatively, we could make use of filtering to narrow down to our desired query, but this would consume more read capacity than is necessary.\n\nInstead, we can leverage the hierarchical nature of the location data to answer all four \"gather\" queries using a single global secondary index 💥 !\n\nAmazingly, there is a Kaggle dataset with all of the Starbucks locations worldwide -- over 25,000 locations! This means we can actually load the data into a DynamoDB table and test it for ourselves.\n\nThe examples directory of the repo includes the code snippets discussed below.\n\nBefore you start\n\nTo run this example, you'll need to download the Starbucks locations dataset from Kaggle. Unzip it, and move the CSV file into your working directory as directory.csv.\n\nYou'll also need Python and Boto3, the AWS SDK for Python. You can install boto3 with pip via pip install boto3. If you don't have Python or pip, you'll need to do some Googling to find it.\n\nFinally, some of the examples use Click, which is a Python tool for quickly making CLI interfaces. You can install it with pip install click.\n\nSchema Design & Loading the Table\n\nOnce we've downloaded our dataset and installed our requirements, it's time to create and load our table.\n\nFirst, let's think of our table structure which means choosing a primary key. A good primary key does at least two things:\n\nit enables you to uniquely identify each item for writes & updates, and\nit evenly distributes your data across the partition key.\n\nIdeally your primary key will also satisfy at least one of your read access patterns as well.\n\nFor us, we'll use the Store Number as a simple primary key. If we were updating a particular store's location information, we would surely know the Store's Number. This would satisfy the first requirement. The Store Number should also distribute data pretty well. Further, it even satisfies our first Read pattern of retrieving a store by its Store Number. This is a pretty good primary key.\n\nWe then need to think about our other four read access patterns -- gathering all stores by country, state, city, and zip. We'll discuss this pattern more in the Gather queries section, but for now, we're going to create a global secondary index named \"StoreLocationIndex\" with the following key structure:\n\na HASH key of Country, indicating the country where the store is located, and\na RANGE key named StateCityPostcode that is a string combining the State, City, and Postcode with each element separated by the pound sign (<STATE>#<CITY>#<POSTCODE>). For example, a store in Omaha, NE would be stored as NE#OMAHA#68144.\n\nTo create this table, run the script to create the table. If it works, you should get a success message:\n\n$ python create_table.py\nTable created successfully!\n\n\nThen, let's load our items into DynamoDB. The insert_items.py script in the example directory opens our CSV file with the Starbucks locations, iterates over the rows, and stores them in our DynamoDB table with the given structure. Note: this may take a while since there are 25,000 items. It took 2 minutes on my Macbook Pro.\n\n$ python insert_items.py\n1000 locations written...\n2000 locations written...\n... <snip> ...\n24000 locations written...\n25000 locations written...\n\n\nLet's run a quick scan with a COUNT to make sure we've got all of our items:\n\n$ aws dynamodb scan \\\n    --table-name StarbucksLocations \\\n    --select COUNT \\\n    $LOCAL\n\n\nIt should return with 25,599 items:\n\n{\n    \"Count\": 25599,\n    \"ScannedCount\": 25599,\n    \"ConsumedCapacity\": null\n}\n\n\nLet's start querying our data!\n\nRetrieve Item\n\nOur first query pattern was to \"Retrieve a single store by its Store Number.\" We'll use the Store Number \"5860-29255\" as an example.\n\nBecause our table's primary key is Store Number, we can use the familiar GetItem API call to retrieve a single Item based on its primary key.\n\nRun the get_store_location script provided in the repo. By default, it will use our default Store Number:\n\nIt should print out the details for our retrieved Item:\n\n$ python get_store_location.py\nAttempting to retrieve store number 5860-29255...\n\nStore number found! Here's your store:\n\n{'City': {'S': 'Pasadena'},\n 'Country': {'S': 'US'},\n 'Latitude': {'S': '34.16'},\n 'Longitude': {'S': '-118.15'},\n 'PhoneNumber': {'S': '626-440-9962'},\n 'Postcode': {'S': '911033383'},\n 'State': {'S': 'CA'},\n 'StateCityPostcode': {'S': 'CA#PASADENA#911033383'},\n 'StoreName': {'S': 'Fair Oaks & Orange Grove, Pasadena'},\n 'StoreNumber': {'S': '5860-29255'},\n 'StreetAddress': {'S': '671 N. Fair Oaks Avenue'}}\n\n\nNice! Notice that it matches our requested StoreNumber. This will satisfy our first access pattern.\n\nIf you want to use the script to retrieve other stores, just pass a --store-number option:\n\n$ python get_store_location.py --store-number 3513-125945\nAttempting to retrieve store number 3513-125945...\n\nStore number found! Here's your store:\n\n{'City': {'S': 'Anchorage'},\n 'Country': {'S': 'US'},\n 'Latitude': {'S': '61.21'},\n 'Longitude': {'S': '-149.78'},\n 'PhoneNumber': {'S': '907-339-0900'},\n 'Postcode': {'S': '995042300'},\n 'State': {'S': 'AK'},\n 'StateCityPostcode': {'S': 'AK#ANCHORAGE#995042300'},\n 'StoreName': {'S': 'Safeway-Anchorage #1809'},\n 'StoreNumber': {'S': '3513-125945'},\n 'StreetAddress': {'S': '5600 Debarr Rd Ste 9'}}\n\nGather queries\n\nTime to move onto the fun part. We have four additional queries we want to use, and we're going to handle it with a single global secondary index.\n\nThe hierarchical structure of the data is important here. Stores in the same State are in the same Country, stores in the same City are in the same State, and stores in the same Postcode are in the same City, (ssh, apparently that's not exactly true).\n\nBecause of this hierarchical structure, we can use our computed SORT key plus the begins_with() function to find all stores at a particular level.\n\nLet's use an example. Think of two different Starbucks stores - our Pasadena example from the previous section, plus a store in San Francisco.\n\nOur StateCityPostcode RANGE key for the first store is CA#PASADENA#911033383. The StateCityPostcode for the second store is CA#SAN FRANCISCO#94158. Notice how they both start with \"CA\", and then add their city and post codes.\n\nIf I wanted to query for all California stores, I would make a key expression that looks like this:\n\nCountry = \"US\" AND begins_with(StateCityPostcode, \"CA\")\n\nNote: This is simplified, as I would actually need to use expression attribute values to represent \"US\" and \"CA\".\n\nWhat if I want to get even more specific and query all stores in San Francisco? Now my key expression looks like:\n\nCountry = \"US\" AND begins_with(StateCityPostcode, \"CA#SAN FRANCISCO\")\n\nFinally, I could get all the way to the post code level by using:\n\nCountry = \"US\" AND begins_with(StateCityPostcode, \"CA#SAN FRANCISCO#94158\")\n\nYou can see this in action by using the query_store_locations.py in the example repo.\n\nFirst, let's query all of the stores in the US. I'm using the --count flag to only return the count of stores, rather than the full item to avoid trashing my terminal:\n\n$ python query_store_locations.py --country 'US' --count\nQuerying locations in country US.\nNo statecitypostcode specified. Retrieving all results in Country.\n\nRetrieved 4648 locations.\n\n\nNote that it says \"No statecitypostcode specified\", so it returned the results for the US. This was 4,648 locations. Americans love their Starbucks.\n\nLet's narrow it down to a state level. We'll try the same query for all Nebraska Starbucks:\n\n$ python query_store_locations.py --country 'US' --state 'NE' --count\nQuerying locations in country US, state NE.\nThe key expression includes a begins_with() function with input of 'NE'\n\nRetrieved 58 locations.\n\n\nIn this one, it shows us that we did use a key expression with a begins_with() function that used \"NE\". It returned 58 locations.\n\nWe can go another level by specifying the city of Omaha:\n\n$ python query_store_locations.py --country 'US' --state 'NE' --city 'Omaha' --count\nQuerying locations in country US, state NE, city Omaha.\nThe key expression includes a begins_with() function with input of 'NE#OMAHA'\n\nRetrieved 30 locations.\n\n\nNow our key expression uses NE#OMAHA to check the start of the SORT key, and we're down to 30 locations.\n\nFinally, let's check it at a post code level. Since this has fewer results, I'll remove the --count flag so we can see the full Items:\n\n$ python query_store_locations.py --country 'US' --state 'NE' --city 'Omaha' --postcode '68144'\nQuerying locations in country US, state NE, city Omaha, postcode 68144.\nThe key expression includes a begins_with() function with input of 'NE#OMAHA#68144'\n\n{'Count': 2,\n 'Items': [{'City': {'S': 'OMAHA'},\n            'Country': {'S': 'US'},\n            'Latitude': {'S': '41.23'},\n            'Longitude': {'S': '-96.14'},\n            'PhoneNumber': {'S': '402-334-1415'},\n            'Postcode': {'S': '68144'},\n            'State': {'S': 'NE'},\n            'StateCityPostcode': {'S': 'NE#OMAHA#68144'},\n            'StoreName': {'S': 'Family Fare 3784 Omaha'},\n            'StoreNumber': {'S': '48135-261124'},\n            'StreetAddress': {'S': '14444 W. CENTER RD., Westwood Plaza'}},\n           {'City': {'S': 'Omaha'},\n            'Country': {'S': 'US'},\n            'Latitude': {'S': '41.23'},\n            'Longitude': {'S': '-96.1'},\n            'PhoneNumber': {'S': '4027785900'},\n            'Postcode': {'S': '681443957'},\n            'State': {'S': 'NE'},\n            'StateCityPostcode': {'S': 'NE#OMAHA#681443957'},\n            'StoreName': {'S': '125th & W. Center Rd.'},\n            'StoreNumber': {'S': '2651-53179'},\n            'StreetAddress': {'S': '12245 West Center Rd.'}}],\n 'ResponseMetadata': {'HTTPHeaders': {'content-length': '738',\n                                      'content-type': 'application/x-amz-json-1.0',\n                                      'server': 'Jetty(8.1.12.v20130726)',\n                                      'x-amz-crc32': '2237738683',\n                                      'x-amzn-requestid': '5acf463b-6341-45b7-a485-dd2860845d97'},\n                      'HTTPStatusCode': 200,\n                      'RequestId': '5acf463b-6341-45b7-a485-dd2860845d97',\n                      'RetryAttempts': 0},\n 'ScannedCount': 2}\n\n\nOur key expression used NE#OMAHA#68144, and we narrowed it down to 2 results. It returned the full items, which is likely what we would use in our application if we were making this query.\n\nGo ahead and play around with the query_store_locations.py script using your own locations. If you want help on the available commands, use the --help flag:\n\n$ python query_store_locations.py --help\nUsage: query_store_locations.py [OPTIONS]\n\nOptions:\n  --country TEXT      Country for stores to query. Default is 'US'.\n  --state TEXT        State abbreviation for stores to query. E.g.: 'NE'\n  --city TEXT         City for stores to query. E.g.: 'Omaha'\n  --postcode TEXT     Post code for stores to query. E.g.: '68144'\n  --default-state     Use defaults to query at state level.\n  --default-city      Use defaults to query at city level.\n  --default-postcode  Use defaults to query at post code level.\n  --count             Only show counts of items.\n  --help              Show this message and exit.\n\n\nIs anything in this example unclear? Hit me up and let me know!"
  },
  {
    "title": "Leaderboard & Write Sharding | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/leaderboard-write-sharding/",
    "html": "In this example, we'll show how to maintain a global leaderboard with a dataset in DynamoDB. A leaderboard is a common need for data applications. Imagine that you're saving information on individual items that you need for individual lookups, but you also want to be able to find the Top N Items as ranked by a particular attribute.\n\nThe guiding example for this is a website that hosts a number of images -- think Unsplash. In addition to retrieving the details on any given image, we also want to find the top-viewed items to show to users.\n\nIn this walkthrough you will understand how to use write sharding combined with a scatter-gather query to satisfy the leaderboard use case.\n\nHat tip to Chris Shenton for initially discussing this use case with me. Also, AWS provides a leaderboard example using game scores in the DynamoDB docs. However, that example uses leaderboards within multiple different games, rather than a global leaderboard as we have here.\n\nExample background\n\nIn this example, we're a photo sharing website. People can upload photos to our site, and other users can view those photos. Additionally, we want to have a discovery mechanism where we show the 'top' photos based on number of views.\n\nBased on this, we have four main access patterns:\n\nAdd a new image (CREATE);\nRetrieve a single image by its URL path (READ);\nIncrease the view count on an image (UPDATE);\nRetrieve the top N images based on total view count (LEADERBOARD).\n\nThe first three access patterns are straight-forward. These are key-based operations, which are perfect fits for DynamoDB.\n\nThe fourth access pattern is the tricky one. Recall that DynamoDB is designed for specific, key-based operations. But finding the top score means we need knowledge of the entire DynamoDB key space. This sounds like it requires a scan, which we know we should never do.\n\nFortunately, we have a better answer. We're going to use a combination of global secondary indexes, write sharding, and scatter-gather queries to implement this without running a full table scan.\n\nLet's get started!\n\nSchema Design: Primary Key and core attributes\n\nNow, let's start with the nitty-gritty of designing and using our table. We will start with the three core use cases.\n\nRecall that our three core use cases (create, read, and update an image) are all key-based lookups for a particular image. Each image will have a URL path in the form of images/<id>.jpg. This path is a unique identifier for the image, so we'll use it for the table's primary key.\n\nWe'll also include some data about the image, such as its storage location(s), its owner, its dimensions, etc. These can be in a data property on each image item.\n\nFurther, we want to track the view count for each image. This can be in an integer property called ViewCount. Whenever someone views an image, we will update the ViewCount with a UpdateItem API call.\n\nAt this point, our schema looks like this:\n\nName\t\t\tType\t\tPrimary Key\nImage\t\t\tString \t\tHASH\nData\t\t\tMap\t\t\t--\nViewCount\t\tInteger\t\t--\nAdding a Leaderboard -- No Write-Sharding\n\nNow we need to update our schema to allow for our leaderboard use case. We will use a combination of a Global Secondary Index and a write-sharding pattern to do this. Let's discuss why.\n\nTo find the top N images, we need to be able to order by ViewCount. Whenever you need to order by in DynamoDB, you have that attribute as a sort (RANGE) key within the given partition (HASH) key that you want to sort.\n\nThus, it sounds like we want to create a Global Secondary Index, or \"GSI\". A GSI needs both a partition key and a sort key. The sort key is ViewCount as that's the attribute we want to order.\n\nSince we want to see top views across all images on the site, we need the same partition key for all images. Let's add an attribute that will be the same for all images in our system. For each image we add to the system, we will have an attribute called \"GSI Hash\" with a value of \"IMAGES\".\n\nOur updated schema looks as follows:\n\nName\t\t\tType\t\tPrimary Key\t\tLeaderboard GSI\nImage\t\t\tString \t\tHASH\nData\t\t\tMap\t\t\t--\nViewCount\t\tInteger\t\t--\t\t\t\tRANGE\nGSI Hash\t\tString\t\t--\t\t\t\tHASH\n\nLet's look through some diagrams of our sample data in our table.\n\nFirst, the image below shows how our table is structured in its main format with the primary key. Each item has the \"Image\" property to uniquely identify it. It also has a \"ViewCount\" property for the number of views, and a \"GSI Hash\" with a value of \"IMAGES\" for each item.\n\nThe next image shows how the Leaderboard GSI will restructure our data into a different format. On the left is our table in its main format. On the right is our table in the Leaderboard GSI. Note how the items have the same data, they're just organized in a different fashion.\n\nFinally, this last image shows why we want to use the Leaderboard GSI. See how the images have been sorted by ViewCount within the IMAGES partition. We can query this index to find the top images by ViewCount.\n\nHot Partitions and Write-Sharding\n\nWhile the format above could work for a simple table with low write traffic, we would run into an issue at higher load. Let's understand why, and then understand how to handle it.\n\nDynamoDB splits its data across multiple nodes using consistent hashing. As part of this, each item is assigned to a node based on its partition key. You want to structure your data so that access is relatively even across partition keys. This ensures that you are making use of DynamoDB's multiple nodes.\n\nThat's the problem with our current Leaderboard GSI -- all of our items use the same partition key (\"IMAGES\"). This means all data will be routed to the same node. If we're doing a large amount of writes, our writes to the Leaderboard GSI could get throttled since all operations are pounding the same node.\n\nTo handle this, we will use a strategy called write-sharding.\n\nRather than putting all images in the same Leaderboard GSI partition, we will arbitrarily split them across N number of partitions. The images within each partition will be sorted by view count.\n\nFor example, imagine we want to split our Leaderboard GSI into three partitions. Upon creating an item, we will add an attribute called \"Partition\" and randomly assign the item to one of our three partitions -- \"PARTITION_0\", \"PARTITION_1\", or \"PARTITION_2\".\n\nOur schema now looks as follows:\n\nName\t\t\tType\t\tPrimary Key\t\tLeaderboard GSI\nImage\t\t\tString \t\tHASH\nData\t\t\tMap\t\t\t--\nViewCount\t\tInteger\t\t--\t\t\t\tRANGE\nPartition\t\tString\t\t--\t\t\t\tHASH\n\nOur initial table would look as shown in the image below. Each item has an Image attribute, a ViewCount attribute, and a Partition attribute.\n\nLike in the previous example, the Leaderboard GSI would reorganize our data into a different structure, as shown below.\n\nThe difference from the original example is that our images are only sorted within a particular partition. As shown below, all images in PARTITION_1 are ordered by ViewCount. PARTITION_0 and PARTITION_2 are also ordered by ViewCount, but separately from PARTITION_1.\n\nWe have solved our hot partition problem. However, now we have a different problem -- images are only sorted within a particular partition. How do we find the overall top images?\n\nIn the next section, we will show how to use a scatter-gather pattern to build our overall leaderboard.\n\nBuilding a leaderboard with Scatter-Gather\n\nIn the previous section, we sharded our leaderboard GSI across multiple partitions to alleviate hot partition issues. In this section, we will show how to work with these partitions to build an overall leaderboard.\n\nIn our example, we used three partitions to shard our data. Imagine that we want to find the top 3 images by view count across our entire data set.\n\nTo do this, we need to do the following steps.\n\nQuery each of our partitions to find the top 3 images by view count within each partition.\n\nThe image below demonstrates this:\n\nOn the left is our table as organized by the Leaderboard GSI index. In our application code, we will query each of the three partitions for the top three images. On the right are our results of the three queries in our application code.\n\nThis is the \"scatter-gather\" portion. We are making multiple, separate queries to our database (\"scatter\") and then combining the results (\"gather\") in our application code.\n\nIn our application code, sort the results by view count to obtain a consolidated view.\n\nThe image below shows our data after it is gathered but before sorting (left), then after sorting (right).\n\nReturn the top 3 results to our users.\n\nNow that you have a global sorting of your data, you can return the proper results to your user.\n\nThat's it! Write sharding combined with scatter-gather can be a powerful tool to use with DynamoDB.\n\nWrite Sharding Considerations\n\nNow that we understand the pattern of write sharding, let's close with a few things you should consider when using the write-sharding pattern.\n\nTo write shard or not to write shard?\n\nThe first question is whether you truly need write sharding at all. Write sharding adds complication to your application, both at write time (randomizing your partition attribute) and at read time (implementing scatter-gather).\n\nYou should only use write sharding if your write traffic is too high for the GSI to handle writing to a single partition.\n\nHow many partitions to shard across?\n\nA second question is how many partitions to shard across. In our simple example, we used three partitions. However, you could easily imagine high write traffic patterns that need significantly more write partitions to avoid throttling.\n\nThe important thing to understand is that partitions are a tradeoff. Adding more write partitions will decrease throttling pressure. However, it will add read complexity as you will need to \"scatter\" across more partitions to build your final data set.\n\nHow many items do I need to read from each partition to build a leaderboard?\n\nA final question is how many items you need to read from each partition to build a proper leaderboard. This is a pretty interesting question, and it depends on your application needs.\n\nIn our example, we had a small number of partitions and only wanted the top three images. We queried for the top three images across each partition. By querying each partition for the number of end results we wanted, we ensured we would get the proper result.\n\nYou could imagine this being more expensive. What if we needed the top 1000 items, and we had 10 partitions? Do we really need to retrieve 10,000 items? After all, it seems pretty unlikely that the 1000th item in partition 1 would be higher than the top item in all of the other partitions.\n\nI'm sure there's some math on how few items you could retrieve from each randomly-assigned partition to have some confidence that you have the right answer. Ultimately, this is a game of probabilities -- there is the chance, however tiny, that the top 1000 items will all be in the same partition.\n\nThe question of whether you can take that chance and how big of a chance you can take depends on your application. Do you need exact precision? If not, how imprecise can you be?\n\nConclusion\n\nIn this walkthrough, we learned how to implement a global leaderboard in DynamoDB. We discussed how to use a write-sharding pattern with scatter-gather queries to alleviate throttling pressure on write-heavy applications.\n\nIs anything in this example unclear? Hit me up and let me know!"
  },
  {
    "title": "Overview | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/data-modeling-overview/",
    "html": "This chapter includes a number of case studies, each exploring a way to model data in DynamoDB for a particular scenario. Sometimes, seeing a single example is better than reading pages of documentation.\n\nThis chapter is intended to grow over time. Are there particular use cases you'd like to see? Hit me up and let me know!\n\nAvailable examples:\n\nBuilding a global leaderboard using write sharding\n\nLearn how to organize your DynamoDB to allow for leaderboard-like queries -- \"What are the most-viewed items in my table?\" \"Which users have the top score in my game?\"\n\nYou will also learn how to use write-sharding and scatter-gather queries to alleviate write throttling for high-usage keys.\n\nModeling hierarchical data*\n\nThis example shows how to model hierarchical data. It includes a full code sample that uses >25,000 Starbucks store locations. Learn how to satisfy multiple access patterns, including finding all stores in a particular state, all stores in a particular city, and all stores in a particular zip code.\n\nPlanned examples:\nTime-based workflows*\nEnum attributes*\nWorking with large items*\nACID transactions*\nGeo-hashing*\nGraph queries and adjacency lists*\n\n* -- Example from Rick Houlihan's excellent 2017 reInvent talk."
  },
  {
    "title": "Global Secondary Indexes | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/global-secondary-indexes/",
    "html": "In this example, we'll learn about global secondary indexes. Just like the last lesson, we'll cover the basics first before diving into an example using some tables we've already created.\n\nBasics of Global Secondary Indexes\n\nUnlike local secondary indexes, you can add global secondary indexes to tables with either simple primary keys or composite primary keys. Further, you're not limited to creating a composite key schema for your index -- you can create a simple key schema.\n\nThere are a few ways in which global secondary indexes differ from local secondary indexes:\n\nSeparate throughput. You provision read and write capacity units for a global secondary index separate than those units for the underlying table. This may add complexity and cost but also gives you flexibility to tailor the capacity to different workloads.\n\nEventual consistency. When writing an Item to a table, it is asynchronously replicated to global secondary indexes. This means you may get different results when querying a table and a global secondary index at the same time. You do not have the ability to specify strong consistency.\n\nNo partition key size limits. Partition keys are limited to 10GB between the table Items and all local secondary indexes. Global secondary indexes are not counted in these limits.\n\nUse on any table. Local secondary indexes may only be used on tables with composite primary keys. Global secondary indexes do not have this restriction -- you can use them on tables with simple or composite primary keys.\n\nUse with any key schema. When specifying the key schema for your global secondary index, you can use either a simple or a composite key schema.\n\nThe last two points can be confusing. The first refers to the primary key schema of the underlying table, while the second refers to the key schema of the secondary index you're creating.\n\nCreating a Global Secondary Index\n\nLike local secondary indexes, you may specify a global secondary index when you initially create a table. However, you may also add a global secondary index after a table is already created. DynamoDB will backfill the global secondary index based on the existing data in the table.\n\nIn this example, let's show how we might use a sparse index for our global secondary index. A sparse index is when not every Item contains the attribute you're indexing. Only Items with the attribute(s) matching the key schema for your index will be copied into the index, so you may end up with fewer Items in the index than in the underlying table.\n\nImagine we want to keep track of Orders that were returned by our Users. We'll store the date of the return in a ReturnDate attribute. We'll also add a global secondary index with a composite key schema using ReturnDate as the HASH key and OrderId as the RANGE key.\n\nThe API call for this is as follows:\n\n$ aws dynamodb update-table \\\n    --table-name UserOrdersTable \\\n    --attribute-definitions '[\n      {\n          \"AttributeName\": \"ReturnDate\",\n          \"AttributeType\": \"S\"\n      },\n      {\n          \"AttributeName\": \"OrderId\",\n          \"AttributeType\": \"S\"\n      }\n    ]' \\\n    --global-secondary-index-updates '[\n        {\n            \"Create\": {\n                \"IndexName\": \"ReturnDateOrderIdIndex\",\n                \"KeySchema\": [\n                    {\n                        \"AttributeName\": \"ReturnDate\",\n                        \"KeyType\": \"HASH\"\n                    },\n                    {\n                        \"AttributeName\": \"OrderId\",\n                        \"KeyType\": \"RANGE\"\n                    }\n                ],\n                \"Projection\": {\n                    \"ProjectionType\": \"ALL\"\n                },\n                \"ProvisionedThroughput\": {\n                    \"ReadCapacityUnits\": 1,\n                    \"WriteCapacityUnits\": 1\n                }\n            }\n        }\n    ]' \\\n    $LOCAL\n\n\nThe syntax to add a global secondary index is similar to that of adding a local secondary index. Note that I didn't need to redefine the keys of my underlying table in the --attribute-definitions section, only the new attribute that I was using for the index.\n\nQuerying a Global Secondary Index\n\nQuerying a global secondary index is just like using a local secondary index -- you use the Query or Scan operations and specify the name of the index you're using.\n\nOne thing that's different in this example is that we've set up a sparse index. Our HASH key is ReturnDate, but none of the Items we've written have a ReturnDate attribute. Let's do a Scan on the index just to confirm:\n\n$ aws dynamodb scan \\\n    --table-name UserOrdersTable \\\n    --index-name ReturnDateOrderIdIndex \\\n    $LOCAL\n\n\nThe response is empty -- no Items are present in the index:\n\n{\n    \"Count\": 0,\n    \"Items\": [],\n    \"ScannedCount\": 0,\n    \"ConsumedCapacity\": null\n}\n\n\nLet's use the BatchWriteItem API call to insert a few Orders that have been returned:\n\n$ aws dynamodb batch-write-item \\\n    --request-items '{\n        \"UserOrdersTable\": [\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20160630-12928\"},\n                        \"Amount\": {\"N\": \"142.23\"},\n                        \"ReturnDate\": {\"S\": \"20160705\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"daffyduck\"},\n                        \"OrderId\": {\"S\": \"20170608-10171\"},\n                        \"Amount\": {\"N\": \"18.95\"},\n                        \"ReturnDate\": {\"S\": \"20170628\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"daffyduck\"},\n                        \"OrderId\": {\"S\": \"20170609-25875\"},\n                        \"Amount\": {\"N\": \"116.86\"},\n                        \"ReturnDate\": {\"S\": \"20170628\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"yosemitesam\"},\n                        \"OrderId\": {\"S\": \"20170609-18618\"},\n                        \"Amount\": {\"N\": \"122.45\"},\n                        \"ReturnDate\": {\"S\": \"20170615\"}\n                    }\n                }\n            }\n        ]\n    }' \\\n    $LOCAL\n\n\nThese four Items are overwriting four of our previous Items by adding a ReturnDate. Usually this would happen individually with an UpdateItem call, but this will suffice for the experiment.\n\nLet's try running our Scan operation again:\n\n$ aws dynamodb scan \\\n    --table-name UserOrdersTable \\\n    --index-name ReturnDateOrderIdIndex \\\n    $LOCAL\n\n\nSuccess! We receive our 4 Items back:\n\n{\n    \"Count\": 4,\n    \"Items\": [\n        {\n            \"OrderId\": {\n                \"S\": \"20160630-12928\"\n            },\n            \"Username\": {\n                \"S\": \"alexdebrie\"\n            },\n            \"Amount\": {\n                \"N\": \"142.23\"\n            },\n            \"ReturnDate\": {\n                \"S\": \"20160705\"\n            }\n        },\n        {\n            \"OrderId\": {\n                \"S\": \"20170609-18618\"\n            },\n            \"Username\": {\n                \"S\": \"yosemitesam\"\n            },\n            \"Amount\": {\n                \"N\": \"122.45\"\n            },\n            \"ReturnDate\": {\n                \"S\": \"20170615\"\n            }\n        },\n        {\n            \"OrderId\": {\n                \"S\": \"20170608-10171\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"18.95\"\n            },\n            \"ReturnDate\": {\n                \"S\": \"20170628\"\n            }\n        },\n        {\n            \"OrderId\": {\n                \"S\": \"20170609-25875\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"116.86\"\n            },\n            \"ReturnDate\": {\n                \"S\": \"20170628\"\n            }\n        }\n    ],\n    \"ScannedCount\": 4,\n    \"ConsumedCapacity\": null\n}\n\n\nThis global secondary index could enable use cases, such as finding all the returns entered yesterday, that would require full table scans without the index.\n\nThis concludes the lesson on global secondary indexes. The next advanced topic is DynamoDB Streams."
  },
  {
    "title": "Local Secondary Indexes | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/local-secondary-indexes/",
    "html": "In the previous lesson, we learned some basics about secondary indexes. In this lesson, we'll dive deeper into local secondary indexes. First we'll cover some basics about local secondary indexes, then we'll walk through an example.\n\nBasics of Local Secondary Indexes\n\nYou can only add local secondary indexes on tables with composite primary keys. A local secondary index maintains the same HASH key as the underlying table while allowing for a different RANGE key.\n\nSome additional notes about local secondary indexes:\n\nMust be specified at table creation. You cannot add a local secondary index to an existing table. It must be provided at creation. This is different than global secondary indexes.\n\n10GB limit per HASH key. For a given HASH key, you may only store 10GB of data. This includes the size of the items in the base table plus the combined size of the items in all local secondary indexes. This is a tricky one and is a good reason for being judicious with your projected attributes.\n\nConsistency options. For local secondary indexes, you may choose between strong consistency and eventual consistency, just like on the underlying table. Strong consistency will consume more read capacity but can be the right choice in some situations.\n\nShares throughput with underlying table. All local secondary indexes use the read and write capacity units of the underlying table.\n\nCreating a Local Secondary Index\n\nLet's see a local secondary index in action. Remember that it can only be used on a table with a composite primary key, so that rules out our Users table from earlier. Let's apply it to our UserOrdersTable instead.\n\nRecall that we did an example in the filtering lesson about searching for a particular User's Orders that exceeded a given amount. Because Amount wasn't a part of the primary key, we had to first retrieve all Orders for a User, then apply the filter to return only those beyond a certain amount.\n\nThis query could be inefficient if we were paging through a large number of Orders for a User -- we might have to make multiple requests to find the small number of Orders over a given amount. Instead, we'll add a local secondary index using the Amount as a sort key. This will enable fast, specific lookups using the Amount column.\n\nUnfortunately, local secondary indexes must be specified at time of table creation. First, we'll need to delete our table:\n\n$ aws dynamodb delete-table \\\n    --table-name UserOrdersTable \\\n    $LOCAL\n\n\nwhich will return a description of your table.\n\nThen, let's recreate the table:\n\n$ aws dynamodb create-table \\\n    --table-name UserOrdersTable \\\n    --attribute-definitions '[\n      {\n          \"AttributeName\": \"Username\",\n          \"AttributeType\": \"S\"\n      },\n      {\n          \"AttributeName\": \"OrderId\",\n          \"AttributeType\": \"S\"\n      },\n      {\n          \"AttributeName\": \"Amount\",\n          \"AttributeType\": \"N\"\n      }\n    ]' \\\n    --key-schema '[\n      {\n          \"AttributeName\": \"Username\",\n          \"KeyType\": \"HASH\"\n      },\n      {\n          \"AttributeName\": \"OrderId\",\n          \"KeyType\": \"RANGE\"\n      }\n    ]' \\\n    --local-secondary-indexes '[\n      {\n          \"IndexName\": \"UserAmountIndex\",\n          \"KeySchema\": [\n              {\n                  \"AttributeName\": \"Username\",\n                  \"KeyType\": \"HASH\"\n              },\n              {\n                  \"AttributeName\": \"Amount\",\n                  \"KeyType\": \"RANGE\"\n              }\n          ],\n          \"Projection\": {\n              \"ProjectionType\": \"KEYS_ONLY\"\n          }\n      }\n    ]' \\\n    --provisioned-throughput '{\n      \"ReadCapacityUnits\": 1,\n      \"WriteCapacityUnits\": 1\n    }' \\\n    $LOCAL\n\n\nThis is the same as the CreateTable command we initially issued for this table, with the additions of: (1) the \"Amount\" attribute definition, and (2) the \"--local-secondary-indexes\" flag.\n\nFinally, load our 25 UserOrder Items into the table by following the same BatchWriteItem call from last chapter. I'm not going to re-paste it here as it's too long.\n\nBy this point, you should have 25 items in your recreated UserOrdersTable. You can check by running a quick Scan command to return the count:\n\n$ aws dynamodb scan \\\n    --table-name UserOrdersTable \\\n    --select COUNT \\\n    $LOCAL\n\n\nAnd it should return a count of 25 items:\n\n{\n    \"Count\": 25,\n    \"ScannedCount\": 25,\n    \"ConsumedCapacity\": null\n}\n\nQuerying a Local Secondary Index\n\nNow that we have a table set up with a local secondary index, let's run a query against it. In our filter example, we looked for all of Daffy Duck's Orders that were over $100. We can now convert this directly to a Query without using a filter:\n\n$ aws dynamodb query \\\n    --table-name UserOrdersTable \\\n    --index-name UserAmountIndex \\\n    --key-condition-expression \"Username = :username AND Amount > :amount\" \\\n    --expression-attribute-values '{\n        \":username\": { \"S\": \"daffyduck\" },\n        \":amount\": { \"N\": \"100\" }\n    }' \\\n    $LOCAL\n\n\nNote that we've removed our --filter-expression and put the filter logic into the --key-condition-expression. We also specified the --index-name that we want to query rather than hitting the table directly.\n\nLet's look at the response:\n\n{\n    \"Count\": 1,\n    \"Items\": [\n        {\n            \"OrderId\": {\n                \"S\": \"20170609-25875\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"116.86\"\n            }\n        }\n    ],\n    \"ScannedCount\": 1,\n    \"ConsumedCapacity\": null\n}\n\n\nJust like the filter example, we received only one Item back that satisfied our conditions. However, look at the ScannedCount and Count. When we ran this query with a filter, we scanned 4 Items. These were all of Daffy's orders. It then returned a Count of 1 Item that satisfied the filter.\n\nWhen we queried the index, our ScannedCount is 1. This shows that we only retrieved 1 Item rather than all 4. This resulted in our query using a smaller number of read capacity units than with the filter example. When querying partitions with a large number of Items, this can make a huge difference in query speed and complexity.\n\nNow that we understand local secondary indexes, let's move on to global secondary indexes."
  },
  {
    "title": "Secondary Indexes | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/secondary-indexes/",
    "html": "Up to this point, most read operations have used a table's primary key directly, either through the GetItem call or the Query call. Using a table's primary key is the most efficient way to retrieve Items and avoids using the slow Scan operation.\n\nHowever, the requirement of using a primary key limits the access patterns of a table. In our Query example, we saw that we could include the order date in our RANGE key to enable fast lookups of a customer's orders by date. Yet this means we don't have a fast way of accessing customer orders by the amount of the order, which is a non-key attribute. To retrieve a subset of a customer's orders based on amount, we would need to use a filter which is only applied after all the customer's orders are retrieved.\n\nFortunately, DynamoDB has a concept of secondary indexes. Secondary indexes allow you to specify alternate key structures which can be used in Query or Scan operations (but not GetItem operations).\n\nIn this lesson, we'll discuss the two types of secondary indexes, and we'll cover some basic rules for working with secondary indexes.\n\nTypes of Secondary Indexes\n\nThere are two types of secondary indexes: local secondary indexes and global secondary indexes.\n\nLocal secondary indexes can be used on a table with a composite primary key to specify an index with the same HASH key but a different RANGE key for a table. This is useful for the scenario mentioned in the intro above -- we still want to partition our data by Username, but we want to retrieve Items by a different attribute (Amount).\n\nGlobal secondary indexes can be used to specify a completely different key structure for a table. If you had a table with a composite primary key, you could have a global secondary index with a simple key. Or, you could add a global secondary index with a completely different HASH key and RANGE key. If your table has a simple primary key, you could add a global secondary index with a composite key structure.\n\nSecondary indexes are very powerful and enable a much more flexible query structure.\n\nBasics of Secondary Indexes\n\nThere are a few basics of secondary indexes that are worth knowing:\n\nNo uniqueness requirement. Recall that for a table's primary key, every item is uniquely identified by its primary key. Thus, you can't have two Items with the same key in a table.\n\nThis requirement is not applicable to secondary indexes. You may have Items in your secondary index with the exact same key structure.\n\nSecondary index attributes aren't required. When writing an Item, you must specify the primary key elements. This isn't true with secondary indexes -- you may write an Item that doesn't include the attributes for secondary indexes. If you do this, the Item won't be written to the secondary index. This is known as a sparse index and can be a very useful pattern.\n\nIndex limits per table. You may create 20 global secondary indexes and 5 local secondary indexes per table.\n\nProjected Attributes\n\nWhen provisioning a secondary index, you specify which attributes you want to project into the index. This states which attributes will be available from the index directly without needing to make an additional call to the underlying table to retrieve attributes.\n\nYour options for attribute projections are:\n\nKEYS_ONLY: Your index will include only the keys for the index and the table's underlying partition and sort key values, but no other attributes.\n\nALL: The full Item is available in the secondary index with all attributes.\n\nINCLUDE: You may choose to name certain attributes that are projected into the secondary index.\n\nConsider your query patterns carefully when choosing your projections. DynamoDB charges based on the amount of data indexed, so projecting all of your attributes may result in excess charges. On the other hand, you want to avoid making two queries to answer a question -- one to query an index for relevant keys, and a second to retrieve the needed attributes on a table for the matching keys.\n\nNext, let's look deeper at local secondary indexes."
  },
  {
    "title": "Scans | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/scans/",
    "html": "In this lesson, we'll talk about using Scans with DynamoDB. The Scan call is the bluntest instrument in the DynamoDB toolset. By way of analogy, the GetItem call is like a pair of tweezers, deftly selecting the exact Item you want. The Query call is like a shovel -- grabbing a larger amount of Items but still small enough to avoid grabbing everything.\n\nThe Scan operation is like a payloader, grabbing everything in its path:\n\nThe Scan call, reporting for duty.\n\nBefore we dive too deeply into the Scan call, I want you to say the following words out loud:\n\nI will never use the Scan operation unless I know what I am doing.\n\nThe Scan operation operates on your entire table. For tables of real size, this can quickly use up all of your Read Capacity. If you're using it in your application's critical path, it will be very slow in returning a response to your users.\n\nThe Scan operation generally makes sense only in the following situations:\n\nyou have a very small table;\nyou're exporting all of your table's data to another storage system; or\nyou use global secondary indexes in a special way to set up a work queue (very advanced).\n\nWith these caveats out of the way, let's explore the Scan call.\n\nScan basics\n\nThe Scan call is likely the easiest of all DynamoDB calls. Simply provide a table name, and it will return all Items in the table (up to a 1MB limit):\n\n$ aws dynamodb scan \\\n    --table-name UserOrdersTable \\\n    $LOCAL\n\n\nThe response (truncated for brevity):\n\n{\n    \"Count\": 25,\n    \"Items\": [\n        {\n            \"OrderId\": {\n                \"S\": \"20160630-28176\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"88.3\"\n            }\n        },\n        ...\n        {\n            \"OrderId\": {\n                \"S\": \"20171129-28042\"\n            },\n            \"Username\": {\n                \"S\": \"alexdebrie\"\n            },\n            \"Amount\": {\n                \"N\": \"83.12\"\n            }\n        }\n    ],\n    \"ScannedCount\": 25,\n    \"ConsumedCapacity\": null\n}\n\n\nAs you can see, it returned all of our Items back to us.\n\nLike the GetItem and Query calls, you can use a --projection-expression to specify the particular attributes you want returned to you. I'll skip the example here as it's similar to the previously given examples.\n\nDynamoDB has a 1MB limit on the amount of data it will retrieve in a single request. Scans will often hit this 1MB limit if you're using your table for real use cases, which means you'll need to paginate through results.\n\nIf you hit the 1MB limit with a Scan, it will return a \"NextToken\" key in the response. You can use the value given with the --starting-token option to continue scanning from the location you previously ended.\n\nYou can test this behavior by passing a --max-items limit in our table. Let's make a Scan request with a max items limit of 1:\n\n$ aws dynamodb scan \\\n    --table-name UserOrdersTable \\\n    --max-items 1 \\\n    $LOCAL\n\n\nThe response includes a single Item, plus a NextToken to continue our Scan:\n\n{\n    \"Count\": 25,\n    \"Items\": [\n        {\n            \"OrderId\": {\n                \"S\": \"20160630-28176\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"88.3\"\n            }\n        }\n    ],\n    \"NextToken\": \"eyJFeGNsdXNpdmVTdGFydEtleSI6IG51bGwsICJib3RvX3RydW5jYXRlX2Ftb3VudCI6IDF9\",\n    \"ScannedCount\": 25,\n    \"ConsumedCapacity\": null\n}\n\nParallel Scans\n\nOne use case for Scans is to export the data into cold storage or for data analysis. If you have a large amount of data, scanning through a table with a single process can take quite a while.\n\nTo alleviate this, DynamoDB has the notion of Segments which allow for parallel scans. When making a Scan, a request can say how many Segments to divide the table into and which Segment number is claimed by the particular request. This allows you to spin up multiple threads or processes to scan the data in parallel.\n\nEven with our small amount of data, we can test this out. Let's say we want to segment our table into three segments to be processed separately. One process could say there are 3 total segments and that it wants the items for segment \"1\":\n\n$ aws dynamodb scan \\\n    --table-name UserOrdersTable \\\n    --total-segments 3 \\\n    --segment 1 \\\n    $LOCAL\n\n\nYou can see the response only has 11 items, rather than the full 25:\n\n{\n    \"Count\": 11,\n    \"Items\": [\n        {\n            \"OrderId\": {\n                \"S\": \"20160630-28176\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"88.3\"\n            }\n        },\n        ...\n        {\n            \"OrderId\": {\n                \"S\": \"20170609-9476\"\n            },\n            \"Username\": {\n                \"S\": \"yosemitesam\"\n            },\n            \"Amount\": {\n                \"N\": \"19.41\"\n            }\n        }\n    ],\n    \"ScannedCount\": 11,\n    \"ConsumedCapacity\": null\n}\n\n\nSegments are zero-indexed, though I had trouble when trying to use Segment \"0\" with DynamoDB Local -- it kept returning 0 elements.\n\nIn the next section, we'll learn about filtering your Query and Scan operations."
  },
  {
    "title": "Filtering | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/filtering/",
    "html": "In previous sections, we've covered key expressions, condition expressions, projection expressions, and update expressions. This lesson discusses the final kind of expression -- filter expressions.\n\nFilter expressions are used to apply server-side filters on Item attributes before they are returned to the client making the call. Before we dig too deeply into filters, let's first understand what's happening during a Query or Scan API call.\n\nBreakdown of a DynamoDB API Call\n\nFor the DynamoDB Query and Scan operations, there are three separate steps happening on the DynamoDB server:\n\nRetrieve the requested data. This step looks at Starting Token (if provided) for both types of operations, and the Key Expression in a Query operation.\n\n(Optionally) Filter the data retrieved in step 1. This includes applying filters as described in this section or projection expressions as discussed in previous lessons.\n\nReturn the data to the client.\n\nAn important note is that any limitations on reads are applied in Step 1, before a filter or projection expression is applied. If you retrieve 100KB of data in Step 1 but filter it down to 1KB of data in Step 2, you will consume the Read Capacity Units for 100KB of data, not the 1KB that it was filtered to. Further, there is a 1MB limit that is applied to all operations, regardless of the read capacity units on a table.\n\nFiltering and projection expressions aren't a magic bullet - they won't make it easy to quickly query your data in additional ways. However, they can save network transfer time by limiting the number and size of items transferred back to your network. They can also simplify application complexity by pre-filtering your results rather than requiring application-side filtering.\n\nFor more on filter expressions and when to use them, check out this post on When to use (and when not to use) DynamoDB Filter Expressions.\n\nUsing Filters\n\nFilter expressions are just like key expressions on Queries -- you specify an attribute to operate on and an expression to apply.\n\nLet's reuse our previous Query to find Daffy Duck's orders. This time, we're looking for the big ticket orders, so we'll add a filter expression to return Orders with Amounts over $100:\n\n$ aws dynamodb query \\\n    --table-name UserOrdersTable \\\n    --key-condition-expression \"Username = :username\" \\\n    --filter-expression \"Amount > :amount\" \\\n    --expression-attribute-values '{\n        \":username\": { \"S\": \"daffyduck\" },\n        \":amount\": { \"N\": \"100\" }\n    }' \\\n    $LOCAL\n\n\nThe response includes only one Order:\n\n{\n    \"Count\": 1,\n    \"Items\": [\n        {\n            \"OrderId\": {\n                \"S\": \"20170609-25875\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"116.86\"\n            }\n        }\n    ],\n    \"ScannedCount\": 4,\n    \"ConsumedCapacity\": null\n}\n\n\nAnother note -- look at the difference between \"ScannedCount\" and \"Count\". ScannedCount refers to the number of Items retrieved in Step 1 above. Count refers to the number of Items returned to the client.\n\nPreviously, these numbers were the same as we weren't filtering the retrieved items. Now that we've applied a filter, we see there were 4 total Orders for Daffy Duck but only 1 of them had an amount over $100. We consumed read capacity for those 4 scanned units, but only 1 was returned to us.\n\nOne final note: Filter expressions may not be used on primary key elements in a Query operation. This makes sense as you can just use the Key Expression if you want to apply an expression on the primary key elements. This limitation does not apply to Scan operations -- you may use filter expressions on primary keys with a Scan.\n\nThis concludes the chapter on working with multiple Items. In the next section, we'll look at advanced topics like secondary indexes and DynamoDB streams."
  },
  {
    "title": "Querying | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/querying/",
    "html": "Querying is a very powerful operation in DynamoDB. It allows you to select multiple Items that have the same partition (\"HASH\") key but different sort (\"RANGE\") keys.\n\nIn this lesson, we'll learn some basics around the Query operation including using Queries to:\n\nretrieve all Items with a given partition key;\nuse key expressions to limit Items based on the RANGE key; and\nuse projection expressions to narrow the response for your Query.\n\nBefore reading this section, you should understand DynamoDB expressions.\n\nRetrieving All Items with a Given Partition Key\n\nIn our previous chapter, we looked at working with individual Items at a time. That can be useful in some scenarios, such as working with Users. We're usually manipulating a single User at a time, whether retrieving a User's profile or changing a User's name.\n\nIt's less helpful in other situations, such as working with Orders. Occasionally we want to grab a specific Order, but we also may want to display all the Orders for a particular User. It would be inefficient to store the different partition keys for each User's Orders and query those Items separately.\n\nLet's see how we can satisfy the latter request using the Query API call. First, we'll retrieve all of the Orders for our daffyduck User. The --key-condition-expression option is the important thing to view here. It's how we define which items to select.\n\n$ aws dynamodb query \\\n    --table-name UserOrdersTable \\\n    --key-condition-expression \"Username = :username\" \\\n    --expression-attribute-values '{\n        \":username\": { \"S\": \"daffyduck\" }\n    }' \\\n    $LOCAL\n\n\nIt returns with all four of Daffy's Orders:\n\n{\n    \"Count\": 4,\n    \"Items\": [\n        {\n            \"OrderId\": {\n                \"S\": \"20160630-28176\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"88.3\"\n            }\n        },\n        {\n            \"OrderId\": {\n                \"S\": \"20170608-10171\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"18.95\"\n            }\n        },\n        {\n            \"OrderId\": {\n                \"S\": \"20170609-25875\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"116.86\"\n            }\n        },\n        {\n            \"OrderId\": {\n                \"S\": \"20171129-29970\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"6.98\"\n            }\n        }\n    ],\n    \"ScannedCount\": 4,\n    \"ConsumedCapacity\": null\n}\n\n\nThis is really useful. On an Orders overview page, we could show all of a User's Orders with the ability to drill into a particular Order if the User desired.\n\nUsing Key Expressions\n\nWhen querying to return Items, you might want to further limit the Items returned rather than returning all Items with a particular HASH key.\n\nFor example, when designing our table, we decided we wanted to answer the query:\n\nGive me all of the OrderIds for a particular Username.\n\nThis is useful generally, but we might want to add something like SQL's WHERE clause to the end:\n\nGive me all of the OrderIds for a particular Username where the Order was placed in the last 6 months.\n\nOR\n\nGive me all of the OrderIds for a particular Username where the Amount was greater than $50.\n\nThere are two different ways we can handle this further segmentation. The ideal way is to build the element we want to query into the RANGE key. This allows us to use Key Expressions to query our data, allowing DynamoDB to quickly find the Items that satisfy our Query.\n\nA second way to handle this is with filtering based on non-key attributes. This is less efficient than Key Expressions but can still be helpful in the right situations.\n\nIn this section, we'll see how to use Key Expressions to narrow our results. We've already used the --key-condition-expression option to indicate the HASH key we want to use with our Query. We can also include a RANGE key value or an expression to operate on that RANGE key.\n\nRecall that in our RANGE key of OrderId, we formatted it as <OrderDate>-<RandomInteger>. Starting with the OrderDate in our RANGE key allows us to query by order date using the expression syntax.\n\nFor example, if we wanted all Orders from 2017, we would make sure our OrderId was between \"20170101\" and \"20180101\":\n\naws dynamodb query \\\n    --table-name UserOrdersTable \\\n    --key-condition-expression \"Username = :username AND OrderId BETWEEN :startdate AND :enddate\" \\\n    --expression-attribute-values '{\n        \":username\": { \"S\": \"daffyduck\" },\n        \":startdate\": { \"S\": \"20170101\" },\n        \":enddate\": { \"S\": \"20180101\" }\n    }' \\\n    $LOCAL\n\n\nOur results return three Items rather than all four of Daffy's Orders:\n\n{\n    \"Count\": 3,\n    \"Items\": [\n        {\n            \"OrderId\": {\n                \"S\": \"20170608-10171\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"18.95\"\n            }\n        },\n        {\n            \"OrderId\": {\n                \"S\": \"20170609-25875\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"116.86\"\n            }\n        },\n        {\n            \"OrderId\": {\n                \"S\": \"20171129-29970\"\n            },\n            \"Username\": {\n                \"S\": \"daffyduck\"\n            },\n            \"Amount\": {\n                \"N\": \"6.98\"\n            }\n        }\n    ],\n    \"ScannedCount\": 3,\n    \"ConsumedCapacity\": null\n}\n\n\nDaffy's fourth order was in 2016 so it did not satisfy our Key Expression.\n\nThese Key Expressions are very useful for enabling more specific query patterns, but note the limitations. Because the Key Expression can only operate on the HASH and RANGE key, you need to build the relevant data into the keys directly. Further, it limits the number of query patterns you can enable. Choosing to start your RANGE key with the OrderDate means you can't do a Key Expression based on the Amount of the Order.\n\nIn future lessons, we'll see other ways to enable more advanced queries, including with filters or by using global secondary indexes and local secondary indexes.\n\nNarrowing Your Query Results\n\nIn the responses above, the Query result is returning full Item that satisfies our Query request. It's not so bad in our example above with small Items. With larger Items, it can increase your response size in undesirable ways.\n\nThe Query API call allows for a --projection-expression option similar to the GetItem call we explored previously. This allows you to limit the Items to return just the attributes you care about.\n\nFor example, if we just wanted to return the Amounts for Daffy's Orders, we could pass a projection expression accordingly:\n\n$ aws dynamodb query \\\n    --table-name UserOrdersTable \\\n    --key-condition-expression \"Username = :username\" \\\n    --expression-attribute-values '{\n        \":username\": { \"S\": \"daffyduck\" }\n    }' \\\n    --projection-expression 'Amount' \\\n    $LOCAL\n\n\nAnd the response contains only the Amount:\n\n{\n    \"Count\": 4,\n    \"Items\": [\n        {\n            \"Amount\": {\n                \"N\": \"88.3\"\n            }\n        },\n        {\n            \"Amount\": {\n                \"N\": \"18.95\"\n            }\n        },\n        {\n            \"Amount\": {\n                \"N\": \"116.86\"\n            }\n        },\n        {\n            \"Amount\": {\n                \"N\": \"6.98\"\n            }\n        }\n    ],\n    \"ScannedCount\": 4,\n    \"ConsumedCapacity\": null\n}\n\n\nNote that both responses so far have included a \"Count\" key that shows how many Items were returned in the response. If you want to get just the count of Items that satisfy a Query, you can use the --select option to return that:\n\n$ aws dynamodb query \\\n    --table-name UserOrdersTable \\\n    --key-condition-expression \"Username = :username\" \\\n    --expression-attribute-values '{\n        \":username\": { \"S\": \"daffyduck\" }\n    }' \\\n    --select COUNT \\\n    $LOCAL\n\n\nAnd the response:\n\n{\n    \"Count\": 4,\n    \"ScannedCount\": 4,\n    \"ConsumedCapacity\": null\n}\n\n\nIn this lesson, we covered the basics of the Query API call. I think it's the most powerful part of DynamoDB, but it requires careful data modeling to get full value. In the next lesson, we'll talk about Scans which is a much blunter instrument than the Query call."
  },
  {
    "title": "Working with Multiple Items | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/working-with-multiple-items/",
    "html": "In the previous chapter, we worked with a single Item at a time -- inserting, retrieving, updating, and deleting. In this chapter, we're going to work with multiple items at a time. We'll explore this in the context of a DynamoDB table that's using a composite primary key.\n\nA composite primary key is useful for using DynamoDB as more than a simple key-value store. It allows you to work with a group of related items with a single query and enables some powerful use cases.\n\nIn this lesson, we'll create a table with a composite primary key. Then, we'll seed it with some data using the BatchWriteItem API call. In the next lessons, we'll work with this data using the Query and Scan API calls.\n\nCreating a table\n\nCreating a table with a composite primary key is similar to creating a table with a simple primary key. You define the attributes and your key schema when creating the table. The main difference is that you'll need to define two attributes rather than one. You then have to specify which attribute is your HASH key and which is your RANGE key.\n\nThe HASH key is how your data is partitioned, while the RANGE key is how that data is sorted within a particular HASH key. The HASH key is particularly important -- you can only grab data for a single HASH key in a Query operation. The HASH and RANGE keys allow for a one-to-many like structure -- for a single HASH key, there can be multiple RANGE keys.\n\nWhen thinking about how to set up our data structure, think how you would fill in the blanks for the following query:\n\n\"Give me all of the ____ from a particular ___.\"\n\nThe element you put in the first blank should be your RANGE key, while the element you put in the second blank should be your HASH key.\n\nIn our example, we're going to make an Orders table. Each Order is placed by a User and is given a specific OrderId. Using our question from above, we would ask \"Give me all of the OrderIds for a particular Username\", but we wouldn't ask \"Give me all of the Usernames for a particular OrderId\", as an Order is placed by a particular User. Given that, Username is our HASH key and OrderId is our RANGE key.\n\nTo create the UserOrdersTable, we'll use the CreateTable API call:\n\n$ aws dynamodb create-table \\\n    --table-name UserOrdersTable \\\n    --attribute-definitions '[\n      {\n          \"AttributeName\": \"Username\",\n          \"AttributeType\": \"S\"\n      },\n      {\n          \"AttributeName\": \"OrderId\",\n          \"AttributeType\": \"S\"\n      }\n    ]' \\\n    --key-schema '[\n      {\n          \"AttributeName\": \"Username\",\n          \"KeyType\": \"HASH\"\n      },\n      {\n          \"AttributeName\": \"OrderId\",\n          \"KeyType\": \"RANGE\"\n      }\n    ]' \\\n    --provisioned-throughput '{\n      \"ReadCapacityUnits\": 1,\n      \"WriteCapacityUnits\": 1\n    }' \\\n    $LOCAL\n\n\nAnd the response shows your table description:\n\n{\n    \"TableDescription\": {\n        \"TableArn\": \"arn:aws:dynamodb:ddblocal:000000000000:table/UserOrdersTable\",\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"Username\",\n                \"AttributeType\": \"S\"\n            },\n            {\n                \"AttributeName\": \"OrderId\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"ProvisionedThroughput\": {\n            \"NumberOfDecreasesToday\": 0,\n            \"WriteCapacityUnits\": 1,\n            \"LastIncreaseDateTime\": 0.0,\n            \"ReadCapacityUnits\": 1,\n            \"LastDecreaseDateTime\": 0.0\n        },\n        \"TableSizeBytes\": 0,\n        \"TableName\": \"UserOrdersTable\",\n        \"TableStatus\": \"ACTIVE\",\n        \"KeySchema\": [\n            {\n                \"KeyType\": \"HASH\",\n                \"AttributeName\": \"Username\"\n            },\n            {\n                \"KeyType\": \"RANGE\",\n                \"AttributeName\": \"OrderId\"\n            }\n        ],\n        \"ItemCount\": 0,\n        \"CreationDateTime\": 1514657981.297\n    }\n}\n\nThis is very similar to when we created the UsersTable other than we've added a RANGE key in addition to a HASH key.\n\nBatch Write Item\n\nNow we have our new table. To get the full use of these multi-Item actions, we'll need to load a fair bit of data into it. One way to load a bunch of data is to use the BatchWriteItem API call. This call allows you to make multiple (up to 25) PutItem and/or DeleteItem requests in a single call rather than making separate calls. You can even make requests to different tables in a single call.\n\nThere are some limitations of the BatchWriteAPI. First, you cannot use the UpdateItem API call with a BatchWriteItem request. Updates must be done individually. Second, you cannot specify conditions for your Put and Delete operations -- they're all-or-nothing.\n\nWhen making a batch call, there are two different failure modes. First, the entire request could fail due to an error in the request, such as trying to write to a table that doesn't exist, trying to write more than 25 Items, or exceeding the size limits for an Item or a batch.\n\nAdditionally, you could have individual write requests that fail within the batch. This is most common when you exceed the write throughput for a given table, though it could also happen for AWS server-side errors. In this case, any unprocessed items will be returned in the response in an \"UnprocessedItems\" key.\n\nAt the bottom of this lesson is a (very long) BatchWriteItem request to paste into your terminal. It will insert 25 Items into our newly-created UserOrdersTable. Each Item has a Username, as required by the table's HASH key. It also has an OrderId to satisfy the table's RANGE key. The OrderId is a datestamp (e.g. 20171230) followed by a random integer. Finally, there is an Amount attribute that details the amount of the order.\n\nIn the next lesson, we'll learn about Querying our table for multiple Items.\n\n$ aws dynamodb batch-write-item \\\n    --request-items '{\n        \"UserOrdersTable\": [\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20160630-12928\"},\n                        \"Amount\": {\"N\": \"142.23\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"daffyduck\"},\n                        \"OrderId\": {\"S\": \"20170608-10171\"},\n                        \"Amount\": {\"N\": \"18.95\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"daffyduck\"},\n                        \"OrderId\": {\"S\": \"20170609-25875\"},\n                        \"Amount\": {\"N\": \"116.86\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"daffyduck\"},\n                        \"OrderId\": {\"S\": \"20160630-28176\"},\n                        \"Amount\": {\"N\": \"88.30\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"yosemitesam\"},\n                        \"OrderId\": {\"S\": \"20170609-18618\"},\n                        \"Amount\": {\"N\": \"122.45\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170609-4177\"},\n                        \"Amount\": {\"N\": \"27.89\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170608-24041\"},\n                        \"Amount\": {\"N\": \"142.02\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"yosemitesam\"},\n                        \"OrderId\": {\"S\": \"20170609-17146\"},\n                        \"Amount\": {\"N\": \"114.00\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"yosemitesam\"},\n                        \"OrderId\": {\"S\": \"20170609-9476\"},\n                        \"Amount\": {\"N\": \"19.41\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20160630-13286\"},\n                        \"Amount\": {\"N\": \"146.37\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170609-8718\"},\n                        \"Amount\": {\"N\": \"76.19\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"daffyduck\"},\n                        \"OrderId\": {\"S\": \"20171129-29970\"},\n                        \"Amount\": {\"N\": \"6.98\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170609-10699\"},\n                        \"Amount\": {\"N\": \"122.52\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20160630-25621\"},\n                        \"Amount\": {\"N\": \"141.78\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170330-29929\"},\n                        \"Amount\": {\"N\": \"80.36\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"yosemitesam\"},\n                        \"OrderId\": {\"S\": \"20160630-4350\"},\n                        \"Amount\": {\"N\": \"138.93\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170330-20659\"},\n                        \"Amount\": {\"N\": \"47.79\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170115-20782\"},\n                        \"Amount\": {\"N\": \"80.05\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"yosemitesam\"},\n                        \"OrderId\": {\"S\": \"20170330-18781\"},\n                        \"Amount\": {\"N\": \"98.40\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"yosemitesam\"},\n                        \"OrderId\": {\"S\": \"20170330-1645\"},\n                        \"Amount\": {\"N\": \"25.53\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170115-2268\"},\n                        \"Amount\": {\"N\": \"37.30\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170609-8267\"},\n                        \"Amount\": {\"N\": \"32.13\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20170330-3572\"},\n                        \"Amount\": {\"N\": \"126.17\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"alexdebrie\"},\n                        \"OrderId\": {\"S\": \"20171129-28042\"},\n                        \"Amount\": {\"N\": \"83.12\"}\n                    }\n                }\n            },\n            {\n                \"PutRequest\": {\n                    \"Item\": {\n                        \"Username\": {\"S\": \"yosemitesam\"},\n                        \"OrderId\": {\"S\": \"20170609-481\"},\n                        \"Amount\": {\"N\": \"136.68\"}\n                    }\n                }\n            }\n        ]\n    }' \\\n    $LOCAL\n\n\nAnd the response showing any unprocessed items:\n\n{\n    \"UnprocessedItems\": {}\n}"
  },
  {
    "title": "Updating & Deleting Items | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/updating-deleting-items/",
    "html": "In this lesson, we'll learn about updating and deleting Items. This is the final lesson on Single-Item Actions. The next chapter is on Multi-Item Actions where we'll use Queries & Scans to operate on multiple Items at a time.\n\nUpdating Items\n\nPreviously, we used the PutItem operation to insert Items into our DynamoDB table. We saw that this operation completely overwrites any existing Item in the table. To counteract this, we used a condition expression to only insert the Item if an Item with that primary key did not exist previously.\n\nAt other times, it is useful to update an existing Item by modifying one or two attributes but leaving the other attributes unchanged. DynamoDB has an UpdateItem operation which allows you to update an Item directly without first retrieving the Item, manipulating it as desired, then saving it back with a PutItem operation.\n\nWhen using the UpdateItem action, you need to specify an update expression. This describes the update actions you want to take and uses the expression syntax.\n\nWhen using the update expression, you must include one of four update clauses. These clauses are:\n\nSET: Used to add an attribute to an Item or modify an existing attribute;\nREMOVE: Used to delete attributes from an Item.\nADD: Used to increment/decrement a Number or insert elements into a Set.\nDELETE: Used to remove items from a Set.\n\nLet's check a few of these by example.\n\nUsing the SET update clause\n\nThe most common update clause is \"SET\", which is used to add an attribute to an Item if the attribute does not exist or to overwrite the existing attribute value if it does exist.\n\nReturning to our initial PutItem examples, perhaps we want to have a DateOfBirth attribute for our Users. Without the UpdateItem action, we would need to retrieve the Item with a GetItem call and then reinsert the Item with a DateOfBirth attribute via a PutItem call. With the UpdateItem call, we can just insert the DateOfBirth directly:\n\n$ aws dynamodb update-item \\\n    --table-name UsersTable \\\n    --key '{\n      \"Username\": {\"S\": \"daffyduck\"}\n    }' \\\n    --update-expression 'SET #dob = :dob' \\\n    --expression-attribute-names '{\n      \"#dob\": \"DateOfBirth\"\n    }' \\\n    --expression-attribute-values '{\n      \":dob\": {\"S\": \"1937-04-17\"}\n    }' \\\n    $LOCAL\n\n\nNote that we used the expression attribute names and expression attribute values from the previous lesson.\n\nIf we then retrieve our Item, we can see that the DateOfBirth attribute has been added but our previous attributes are still there:\n\n$ aws dynamodb get-item \\\n    --table-name UsersTable \\\n    --key '{\n      \"Username\": {\"S\": \"daffyduck\"}\n    }' \\\n    $LOCAL\n\n{\n    \"Item\": {\n        \"Username\": {\n            \"S\": \"daffyduck\"\n        },\n        \"DateOfBirth\": {\n            \"S\": \"1937-04-17\"\n        },\n        \"Age\": {\n            \"N\": \"81\"\n        },\n        \"Name\": {\n            \"S\": \"Daffy Duck\"\n        }\n    }\n}\n\nUsing the REMOVE update clause\n\nThe REMOVE clause is the opposite of the SET clause -- it deletes an attribute from an Item if it exists.\n\nLet's use it here to remove the \"DateOfBirth\" attribute we just added. We're also going to add a --return-values option to have DynamoDB return the Item to us after the update so we don't have to make a separate GetItem call. The --return-values option has different options, including to return the old Item before the changes or to return only the updated attributes before they were changed. Here, we'll just use the \"ALL_NEW\" option to show the Item as it exists after the operation:\n\n$ aws dynamodb update-item \\\n    --table-name UsersTable \\\n    --key '{\n      \"Username\": {\"S\": \"daffyduck\"}\n    }' \\\n    --update-expression 'REMOVE #dob' \\\n    --expression-attribute-names '{\n      \"#dob\": \"DateOfBirth\"\n    }' \\\n    --return-values 'ALL_NEW' \\\n    $LOCAL\n\n{\n    \"Attributes\": {\n        \"Username\": {\n            \"S\": \"daffyduck\"\n        },\n        \"Age\": {\n            \"N\": \"81\"\n        },\n        \"Name\": {\n            \"S\": \"Daffy Duck\"\n        }\n    }\n}\n\n\nWe can see that our Item no longer contains a DateOfBirth attribute.\n\nDeleting Items\n\nThe final single-item action to cover is DeleteItem. There will be times when you want to delete Items from your tables, and this is the action you'll use.\n\nThe DeleteItem action is pretty simple -- just provide the key of the Item you'd like to delete:\n\n$ aws dynamodb delete-item \\\n    --table-name UsersTable \\\n    --key '{\n      \"Username\": {\"S\": \"daffyduck\"}\n    }' \\\n    $LOCAL\n\n\nYour Item is deleted! If you try to retrieve your Item with a GetItem, you'll get an empty response.\n\nSimilar to the PutItem call, you can add a --condition-expression to only delete your Item under certain conditions. Let's say we want to delete Yosemite Sam, but only if he's younger than 21 years old:\n\n$ aws dynamodb delete-item \\\n    --table-name UsersTable \\\n    --key '{\n      \"Username\": {\"S\": \"yosemitesam\"}\n    }' \\\n    --condition-expression \"Age < :a\" \\\n    --expression-attribute-values '{\n      \":a\": {\"N\": \"21\"}\n    }' \\\n    $LOCAL\n\nAn error occurred (ConditionalCheckFailedException) when calling the DeleteItem operation: The conditional request failed\n\n\nBecause Yosemite Sam is 73 years old, the conditional check failed and the delete did not go through.\n\nConclusion\n\nThis concludes the single-item actions chapter of the DynamoDB guide. We learned about the basics of Items, including primary keys, attributes, and attribute types. We then covered inserting and retrieving Items. Then we looked at expression syntax, including expression attribute names and values. We did some conditional expressions, and wrapped up with update and delete actions.\n\nThe next chapter covers actions that operate on multiple items. This includes queries and scans, as well as how to use filter expressions. These actions take advantage of tables with composite primary keys and increase the utility of DynamoDB."
  },
  {
    "title": "Expression Basics | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/expression-basics/",
    "html": "In this lesson, we will cover using expressions with DynamoDB. Expressions are an integral part of using DynamoDB, and they are used in a few different areas:\n\nCondition expressions are used when manipulating individual items to only change an item when certain conditions are true.\nProjection expressions are used to specify a subset of attributes you want to receive when reading Items. We used these in our GetItem calls in the previous lesson.\nUpdate expressions are used to update a particular attribute in an existing Item.\nKey condition expressions are used when querying a table with a composite primary key to limit the items selected.\nFilter expressions allow you to filter the results of queries and scans to allow for more efficient responses.\n\nUnderstanding these expressions is key to getting the full value from DynamoDB. In this section, we'll look at the basics of expressions, including the use of expression attributes names and values. Then, we'll see how to use condition expressions in the context of our PutItem calls from the previous lesson.\n\nBasics of Expressions\n\nExpressions are strings that use DynamoDB's domain-specific expression logic to check for the validity of a described statement. With expressions, you can use comparator symbols, such as \"=\" (equals), \">\" (greater than), or \">=\" (greater than or equal to).\n\nFor example, a comparator symbol could be used as follows:\n\n\"Age >= 21\"\n\nto ensure that the Item being manipulated has an Age greater than or equal to 21.\n\nNote: this example wouldn't work as it wouldn't know the type of the value \"21\". You would need to use the expression attribute values discussed below.\n\nIn addition to comparators, you can also use certain functions in your expressions. This includes checking whether a particular attribute exists (attribute_exists() function) or does not exist (attribute_not_exists() function), or that an attribute begins with a particular substring (begins_with() function).\n\nFor example, you could use the attribute_not_exists() function as follows to ensure you're not manipulating an Order that already has a DateShipped attribute:\n\n\"attribute_not_exists(DateShipped)\"\n\nThe full list of available functions is:\n\nattribute_exists(): Check for existence of an attribute;\nattribute_not_exists(): Check for non-existence of an attribute;\nattribute_type(): Check if an attribute is of a certain type;\nbegins_with(): Check if an attribute begins with a particular substring;\ncontains(): Check if a String attribute contains a particular substring or a Set attribute contains a particular element; and\nsize(): Returns a number indicating the size of an attribute.\nExpression Placeholders\n\nFrom the previous section, we learned that expressions are strings that check for the validity of a particular statement. However, there are times when you cannot accurately represent your desired statement due to DynamoDB syntax limitations or when it's easier to use variable substitution to create your statement rather than building a string.\n\nDynamoDB's expression language lets you use expression attribute names and expression attribute values to get around these limitations. These allow you to define expression variables outside of the expression string itself, then use replacement syntax to use these variables in your expression.\n\nExpression Attribute Names\n\nLet's start with understanding expression attribute names. There are times when you want to write an expression for a particular attribute, but you can't properly represent that attribute name due to DynamoDB limitations. This could be because:\n\nYour attribute is a reserved word. DynamoDB has a huge list of reserved words, including words like \"Date\", \"Year\", and \"Name\". If you want to use those as attribute names, you'll need to use expression attribute name placeholders.\nYour attribute name contains a dot. DynamoDB uses dot syntax to access nested items in a document. If you used a dot in your top-level attribute name, you'll need to use a placeholder.\nYour attribute name begins with a number. DynamoDB won't let you use attribute names that begin with a number in your expression syntax.\n\nTo use expression attribute names, you pass in a map where the key is the placeholder to use in your expression and the value is the attribute name you want to expand to. For example, if I wanted to use a placeholder of \"#a\" for my attribute name of \"Age\", my expression attribute names map would look like:\n\n  --expression-attribute-names '{\n    \"#a\": \"Age\"\n  }'\n\nThen, I could use \"#a\" in my expression when I wanted to refer to the Age attribute.\n\nWhen using expression attribute names, the placeholder must begin with a pound sign (\"#\").\n\nIn the \"GetItem\" example from the previous lesson, we used the --projection-expression flag to return a subset of the item attributes. To alter it to use expression attribute names, the API call would look like:\n\n$ aws dynamodb get-item \\\n    --table-name UsersTable \\\n    --projection-expression \"#a, #u\" \\\n    --expression-attribute-names '{\n      \"#a\": \"Age\",\n      \"#u\": \"Username\"\n    }' \\\n    --key '{\n      \"Username\": {\"S\": \"daffyduck\"}\n    }' \\\n    $LOCAL\n\n{\n    \"Item\": {\n        \"Username\": {\n            \"S\": \"daffyduck\"\n        },\n        \"Age\": {\n            \"N\": \"81\"\n        }\n    }\n}\n\n\nNotice that we've replaced the \"Age\" and \"Username\" attributes with the expression attribute names of \"#a\" and \"#u\", respectively.\n\nOne final note: if you're modifying a nested attribute in a map, you'll need to use expression attribute names for each element in the attribute name.\n\nFor example, imagine you had an \"Address\" map attribute with keys of \"Street\", \"City\", and \"State\". You have a use case where you want to check if the \"State\" is equal to a particular value. To get the nested \"Address.State\" attribute, you would need to write it as:\n\n    --condition-expression \"#a.#st = 'Nebraska' \" \\\n    --expression-attribute-names '{\n      \"#a\": \"Address\",\n      \"#st\": \"State\"\n    }'\n\nNotice that both Address and State have been replaced with expression attribute names.\n\nExpression Attribute Values\n\nExpression attribute values are similar to expression attribute names except that they are used for the values used to compare to values of attributes on an Item, rather than the name of the attribute.\n\nThe syntax for expression attribute values is the same as expression attribute names with two changes:\n\nexpression attribute values must start with a colon (\":\") rather than a pound sign; and\nexpression attribute values must specify the type for the value they are referencing, e.g.: {\":agelimit\": {\"N\": 21} }\n\nFor examples of using expression attribute values, look at the next lesson where we use Update Expressions.\n\nCondition Expressions\n\nLet's close out this lesson by using an expression on one of our previous examples.\n\nRecall in the last lesson that we used the PutItem operation to insert Items into our table. The PutItem call overwrites any existing Item with the provided primary key.\n\nThis can be a destructive operation. With our Users example, imagine a new user tries to use a Username which has already been claimed by another user. If we did a simple PutItem operation, it would overwrite the existing User with that Username -- not a great experience!\n\nWe can use a condition expression to ensure there isn't a User with the requested Username before creating the new Item. We would adjust our call to look as follows:\n\n$ aws dynamodb put-item \\\n    --table-name UsersTable \\\n    --item '{\n      \"Username\": {\"S\": \"yosemitesam\"},\n      \"Name\": {\"S\": \"Yosemite Sam\"},\n      \"Age\": {\"N\": \"73\"}\n    }' \\\n    --condition-expression \"attribute_not_exists(#u)\" \\\n    --expression-attribute-names '{\n      \"#u\": \"Username\"\n    }' \\\n    $LOCAL\n\n\nNote that we've added a condition expression using the attribute_not_exists() function on the primary key of the table.\n\nOn first run, this Item is inserted successfully. If you try inserting the same Item again, you'll get an error:\n\nAn error occurred (ConditionalCheckFailedException) when calling the PutItem operation: The conditional request failed\n\n\nThe operation failed because the Username was already taken. We can return an error to the user and ask them to choose a different Username.\n\nIn the next lesson, we'll cover updating and deleting items, which will include a look at some additional expression examples."
  },
  {
    "title": "Inserting & Retrieving Items | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/inserting-retrieving-items/",
    "html": "Items are the key building block in DynamoDB. In this lesson, we're going to learn the basics of inserting and retrieving items with DynamoDB. We'll create a Users table with a simple primary key of Username. Then, we'll explore two basic API calls: PutItem and GetItem.\n\nThis lesson will only cover the basics of using these API calls. The next lesson will explain using expressions in these API calls for advanced functionality. Then we'll move on to updating and deleting items in the following lesson.\n\nTo follow along with the examples in this lesson, make sure you have your environment set up properly. Note that I'm appending a $LOCAL variable to the end of every command to facilitate those using DynamoDB Local.\n\nCreating a table\n\nThe first step is to create our table that we'll use throughout this example. We want to create a table to hold Users that will use a simple primary key of \"Username\", which is a string.\n\nWhen creating a table, you will need to provide AttributeDefinitions for each attribute you need to define. An attribute definition includes the name and type of the attribute. For us, this means we have an attribute with the name \"Username\" and of type \"S\", for String. You only need to define attributes which are used in your primary key or are used in secondary indexes.\n\nYou'll then need to provide the KeySchema of your table. This is where you define your primary key, including a HASH key and an optional RANGE key. In this example, we're using a simple primary key so we're just using Username as a HASH key.\n\nFinally, you'll need to specify a TableName and the ProvisionedThroughput for your table. We'll keep the Read and Write Capacity Units at 1 since this is just an example.\n\nWith these notes in mind, let's create our table:\n\n$ aws dynamodb create-table \\\n  --table-name UsersTable \\\n  --attribute-definitions '[\n    {\n        \"AttributeName\": \"Username\",\n        \"AttributeType\": \"S\"\n    }\n  ]' \\\n  --key-schema '[\n    {\n        \"AttributeName\": \"Username\",\n        \"KeyType\": \"HASH\"\n    }\n  ]' \\\n  --provisioned-throughput '{\n    \"ReadCapacityUnits\": 1,\n    \"WriteCapacityUnits\": 1\n  }' \\\n  $LOCAL\n\n\nIf your operation was successful, you should get a response with your table details:\n\n{\n    \"TableDescription\": {\n        \"TableArn\": \"arn:aws:dynamodb:ddblocal:000000000000:table/UsersTable\",\n        \"AttributeDefinitions\": [\n            {\n                \"AttributeName\": \"Username\",\n                \"AttributeType\": \"S\"\n            }\n        ],\n        \"ProvisionedThroughput\": {\n            \"NumberOfDecreasesToday\": 0,\n            \"WriteCapacityUnits\": 1,\n            \"LastIncreaseDateTime\": 0.0,\n            \"ReadCapacityUnits\": 1,\n            \"LastDecreaseDateTime\": 0.0\n        },\n        \"TableSizeBytes\": 0,\n        \"TableName\": \"UsersTable\",\n        \"TableStatus\": \"ACTIVE\",\n        \"KeySchema\": [\n            {\n                \"KeyType\": \"HASH\",\n                \"AttributeName\": \"Username\"\n            }\n        ],\n        \"ItemCount\": 0,\n        \"CreationDateTime\": 1514562957.925\n    }\n}\n\nAwesome! You have a table. You can view your tables with the list-tables command:\n\n$ aws dynamodb list-tables $LOCAL\n{\n    \"TableNames\": [\n        \"UsersTable\"\n    ]\n}\n\n\nYou can see details of your table with the describe-table command:\n\n$ aws dynamodb describe-table \\\n  --table-name UsersTable \\\n  $LOCAL\n\nPut Item\n\nNow that we have a table, let's throw some data into it. We'll be using the PutItem API call to DynamoDB.\n\nWith the PutItem call, you provide an entire Item to be placed into your DynamoDB table. This action will create a new Item if no Item with the given primary key exists, or it will overwrite an existing Item if an Item with that primary key already exists.\n\nIf you would like to prevent overwriting an existing Item, you'll need to use condition expressions. If you only want to update portions of an existing Item rather than overwriting the entire Item, you'll need to use the UpdateItem API call.\n\nLet's insert a simple Item into our table. Recall that you only need to provide the attributes of the table's primary key. Our primary key is a \"Username\" field. We'll create a User with the Username of \"alexdebrie\":\n\n$ aws dynamodb put-item \\\n    --table-name UsersTable \\\n    --item '{\n      \"Username\": {\"S\": \"alexdebrie\"}\n    }' \\\n    $LOCAL\n\n\nIf no error was returned, your write was successful!\n\nLet's add one more item to our table before moving on (don't worry - we'll read our items back soon enough!). Usually we want to include more information than just what's required by our primary key. Let's add a second User that also has Name and Age attributes:\n\n$ aws dynamodb put-item \\\n    --table-name UsersTable \\\n    --item '{\n      \"Username\": {\"S\": \"daffyduck\"},\n      \"Name\": {\"S\": \"Daffy Duck\"},\n      \"Age\": {\"N\": \"81\"}\n    }' \\\n    $LOCAL\n\n\nNote that we've still included our \"Username\" attribute as it's required by our table. We've also added Name and Age attributes, each with the proper attribute types. Unlike a relational database, we didn't need to define these additional attributes up front. We can add them freely as needed.\n\nDynamoDB's flexibility can be a blessing and a curse. The flexibility allows for a more dynamic data model that may fit your requirements. However, it won't provide the useful guardrails that a relational database includes to assist with data integrity.\n\nGet Item\n\nIn the previous section, we inserted two Items into our Users Table. Now let's see how we can retrieve those items for use in our application.\n\nRetrieving Items is done with the GetItem API call. This call requires the table name and the primary key of the Item you want to retrieve. Remember: each Item is uniquely identifiable by its primary key. Thus, the GetItem call is a way to get exactly the item you want.\n\nLet's retrieve our first User:\n\n$ aws dynamodb get-item \\\n    --table-name UsersTable \\\n    --key '{\n      \"Username\": {\"S\": \"alexdebrie\"}\n    }' \\\n    $LOCAL\n\n{\n    \"Item\": {\n        \"Username\": {\n            \"S\": \"alexdebrie\"\n        }\n    }\n}\n\n\nNotice that the command returned the Item requested in an \"Item\" key.\n\nIf we try to request our second User, we'll get the full Item with its additional Name and Age attributes:\n\n$ aws dynamodb get-item \\\n    --table-name UsersTable \\\n    --key '{\n      \"Username\": {\"S\": \"daffyduck\"}\n    }' \\\n    $LOCAL\n\n{\n    \"Item\": {\n        \"Username\": {\n            \"S\": \"daffyduck\"\n        },\n        \"Age\": {\n            \"N\": \"81\"\n        },\n        \"Name\": {\n            \"S\": \"Daffy Duck\"\n        }\n    }\n}\n\n\nSometimes you may want to retrieve only certain attributes when getting an Item. This can be particularly helpful for saving network bandwidth when working with large items.\n\nUse the --projection-expression option to return only particular elements from an item:\n\n$ aws dynamodb get-item \\\n    --table-name UsersTable \\\n    --projection-expression \"Age, Username\" \\\n    --key '{\n      \"Username\": {\"S\": \"daffyduck\"}\n    }' \\\n    $LOCAL\n\n{\n    \"Item\": {\n        \"Username\": {\n            \"S\": \"daffyduck\"\n        },\n        \"Age\": {\n            \"N\": \"81\"\n        }\n    }\n}\n\n\nIn the example above, we used the --projection-expression option to retrieve only the Age and Username of Daffy Duck. You can even use projection expressions to grab particular nested elements in a List attribute or Map attribute.\n\nThis lesson has covered the basics of inserting and retrieving items with DynamoDB. In the next lesson, we'll look at advanced functionality using expressions."
  },
  {
    "title": "Anatomy of an Item | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/anatomy-of-an-item/",
    "html": "An item is the core unit of data in DynamoDB. It is comparable to a row in a relational database, a document in MongoDB, or a simple object in a programming language. Each item is uniquely identifiable by a primary key that is set on a table level.\n\nAn item is composed of attributes, which are bits of data on the item. This could be the \"Name\" for a User, or the \"Year\" for a Car. Attributes have types -- e.g., strings, numbers, lists, sets, etc -- which must be provided when writing and querying Items.\n\nIn this section, we'll cover the core aspects of Items, including:\n\nPrimary keys,\nAttributes, and\nAttribute types.\nPrimary Keys\n\nEvery item in a table is uniquely identified by its primary key.\n\nWhen creating a new table, you will need to specify the primary key of that table. Every item in a table is uniquely identified by its primary key. Accordingly, the primary key must be included with every item that is written to a DynamoDB table.\n\nThere are two types of primary keys. A simple primary key uses a single attribute to identify an item, such as a Username or an OrderId. Using a DynamoDB table with a simple primary key is similar to using most simple key-value stores, such as Memcached.\n\nA composite primary key uses a combination of two attributes to identify a particular item. The first attribute is a partition key (also known as a \"hash key\") which is used to segment and distribute items across shards. The second attribute is a sort key (also known as a \"range key\") which is used to order items with the same partition key. A DynamoDB table with a composite primary key can use interesting query patterns beyond simple get / set operations.\n\nUnderstanding the primary key is a crucial part of planning your data model for a DynamoDB table. The primary key will be your main method of inserting and updating items in your table.\n\nAttributes\n\nAn item is made up of attributes, which are different elements of data on a particular item. For example, an item in the User table might have a Name attribute, an Age attribute, an Address attribute, and more. They are comparable to columns in a relational database.\n\nMost attributes in a DynamoDB table are not required for every item. DynamoDB is a NoSQL database which allows for a more flexible data model than the standard relational databases. You could store entirely different kinds of objects in a single DynamoDB table, such as a Car object with Make, Model, and Year attributes, and a Pet object with Type, Breed, Age, and Color attributes. This is a common practice, as you will often have multiple different entity types in a single table.\n\nThere is one exception to the flexible model of DynamoDB items -- each item must have the attribute(s) for the defined primary key of the table.\n\nAttribute types\n\nWhen setting an attribute for a DynamoDB item, you must specify the type of the attribute. Available types include simple types like strings and numbers as well as composite types like lists, maps, or sets.\n\nWhen manipulating an item, you'll provide each attribute you're trying to set with the type of the attribute. This will be provided in a map where the keys are the names of the attributes to set. The values for each attribute is a map with a single key indicating the type of value for the attribute and the value being the actual value of the attribute.\n\nFor example, if you want to store a User object with three attributes of Name, Age, and Roles, your API call to store the User would look like:\n\n{\n    \"Name\": { \"S\": \"Alex DeBrie\" },\n    \"Age\": { \"N\": \"29\" },\n    \"Roles\": { \"L\": [{ \"S\": \"Admin\" }, { \"S\": \"User\" }] }\n}\n\nIn this example, we've stored a string attribute of \"Name\" with the value \"Alex DeBrie\" using the string indicator of \"S\". There's also a number attribute of \"Age\" with the value \"29\" with the number indicator of \"N\". Finally, there's a list attribute of \"Roles\" with the value containing two items, \"Admin\" and \"User\" using the list indicator of \"L\".\n\nSimilarly, when you retrieve an item from DynamoDB, it will return the attributes in a map with the attribute names as the keys of the map. The values of the map will be a map containing a single key indicating the type of the attribute and the value containing the actual value of the attribute.\n\nFor example, if you're using the GetItem API call to retrieve the User from above, your response would look like:\n\n{\n    \"Item\": {\n        \"Name\": {\n            \"S\": \"Alex DeBrie\"\n        },\n        \"Age\": {\n            \"N\": \"29\"\n        },\n        \"Roles\": {\n            \"L\": [{ \"S\": \"Admin\" }, { \"S\": \"User\" }]\n        }\n    }\n}\n\nNote that the value for the Age attribute is a string -- \"29\" -- rather than the actual number 29. In your application, you'll need to do conversions from a string type to a number type if needed.\n\nWith the basics of attribute types in mind, let's look at the different types of attributes. Each type starts with the identifier used (e.g. S for strings, N for numbers, etc) as well as an example usage.\n\nString type\n\nIdentifier: \"S\"\n\nExample Usage:\n\n\"Name\": { \"S\": \"Alex DeBrie\" }\n\nThe string type is the most basic data type, representing a Unicode string with UTF-8 encoding.\n\nDynamoDB allows sorting and comparisons of string types according to the UTF-8 encoding. This can be helpful when sorting last names (\"Give me all last names, sorted alphabetically\") or when filtering ISO timestamps (\"Give me all orders between \"2017-07-01\" and \"2018-01-01\").\n\nNumber type\n\nIdentifier: \"N\"\n\nExample Usage:\n\n\"Age\": { \"N\": \"29\" }\n\nThe number type represents positive and negative numbers, or zero. It can be used for precision up to 38 digits.\n\nNote that you will send your number up as a string value. However, you may do numerical operations on your number attributes when working with condition expressions.\n\nBinary type\n\nIdentifier: \"B\"\n\nExample Usage:\n\n\"SecretMessage\": { \"B\": \"bXkgc3VwZXIgc2VjcmV0IHRleHQh\" }\n\nYou can use DynamoDB to store Binary data directly, such as an image or compressed data. Generally, larger binary blobs should be stored in something like Amazon S3 rather than DynamoDB to enable greater throughput, but you may use DynamoDB if you like.\n\nWhen using Binary data types, you must base64 encode your data before sending to DynamoDB.\n\nBoolean type\n\nIdentifier: \"BOOL\"\n\nExample Usage:\n\n\"IsActive\": { \"BOOL\": \"false\" }\n\nThe Boolean type stores either \"true\" or \"false\".\n\nNull type\n\nIdentifier: \"NULL\"\n\nExample Usage:\n\n\"OrderId\": { \"NULL\": \"true\" }\n\nThe Null type stores a boolean value of either \"true\" or \"false\". I would generally recommend against using it.\n\nList type\n\nIdentifier: \"L\"\n\nExample Usage:\n\n\"Roles\": { \"L\": [ \"Admin\", \"User\" ] }\n\nThe List type allows you to store a collection of values in a single attribute. The values are ordered and do not have to be of the same type (e.g. string or number).\n\nYou can operate directly on list elements using expressions.\n\nMap type\n\nIdentifier: \"M\"\n\nExample Usage:\n\n\"FamilyMembers\": {\n    \"M\": {\n        \"Bill Murray\": {\n            \"Relationship\": \"Spouse\",\n            \"Age\": 65\n        },\n        \"Tina Turner\": {\n            \"Relationship\": \"Daughter\",\n            \"Age\": 78,\n            \"Occupation\": \"Singer\"\n        }\n    }\n}\n\nLike the List type, the Map type allows you to store a collection of values in a single attribute. For a Map attribute, these values are stored in key-value pairs, similar to the map or dictionary objects in most programming languages.\n\nAlso like the List type, you can operate directly on map elements using expressions.\n\nString Set type\n\nIdentifier: \"SS\"\n\nExample Usage:\n\n\"Roles\": { \"SS\": [ \"Admin\", \"User\" ] }\n\nDynamoDB includes three different Set types which allow you to maintain a collection of unique items of the same type. The String Set is used to hold a set of strings.\n\nSets can be particularly useful with expressions. You can run update commands to add & remove elements to a set without fetching & inserting the whole object. You may also check for the existence of an element within a set when updating or retrieving items.\n\nNumber Set type\n\nIdentifier: \"NS\"\n\nExample Usage:\n\n\"RelatedUsers\": { \"NS\": [ \"123\", \"456\", \"789\" ] }\n\nDynamoDB includes three different Set types which allow you to maintain a collection of unique items of the same type. The Number Set is used to hold a set of numbers.\n\nSets can be particularly useful with expressions. You can run update commands to add & remove elements to a set without fetching & inserting the whole object. You may also check for the existence of an element within a set when updating or retrieving items.\n\nBinary Set type\n\nIdentifier: \"BS\"\n\nExample Usage:\n\n\"SecretCodes\": { \"BS\": [ \n\t\"c2VjcmV0IG1lc3NhZ2UgMQ==\", \n\t\"YW5vdGhlciBzZWNyZXQ=\", \n\t\"dGhpcmQgc2VjcmV0\" \n] }\n\nDynamoDB includes three different Set types which allow you to maintain a collection of unique items of the same type. The Binary Set is used to hold a set of binary values.\n\nSets can be particularly useful with expressions. You can run update commands to add & remove elements to a set without fetching & inserting the whole object. You may also check for the existence of an element within a set when updating or retrieving items.\n\nWith the basics of Items in mind, let's insert and retrieve our first items."
  },
  {
    "title": "Environment Setup | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/environment-setup/",
    "html": "In many of the subsequent lessons, we'll be directly interacting with the AWS DynamoDB APIs. To do this, we'll need to set up our environment.\n\nInstall the AWS CLI\n\nThe AWS CLI is a nice command line utility for interacting with AWS services.\n\n$ pip install awscli\n\nIf you have trouble installing it, check the install instructions here.\n\nGet IAM credentials\n\nIf you want to use a real AWS account, you'll need to set up your environment with the proper IAM credentials. You can read the AWS docs on doing that here.\n\nThe quickest route is to create an IAM profile with full DynamoDB permissions. The Policy statement would look like:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dynamodb:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n\n\nOnce you have the keys for your IAM user, you can add your profile with aws configure:\n\n$ aws configure\nAWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE\nAWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nDefault region name [None]: us-west-2\nDefault output format [None]: json\n\n(Optional) Use DynamoDB Local\n\nAWS has a downloadable version of DynamoDB that you can run locally. This is ideal if you don't want to configure a real AWS account or if you want to avoid any AWS charges.\n\nTo use it, download the zip file and unzip it:\n\n$ curl -O https://s3-us-west-2.amazonaws.com/dynamodb-local/dynamodb_local_latest.zip\n$ unzip dynamodb_local_latest.zip\n$ rm dynamodb_local_latest.zip\n\n\nThen start your DynamoDB local instance:\n\n$ java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb\n\nInitializing DynamoDB Local with the following configuration:\nPort:\t8000\nInMemory:\tfalse\nDbPath:\tnull\nSharedDb:\ttrue\nshouldDelayTransientStatuses:\tfalse\nCorsParams:\t*\n\n\nIf you see the initialization message in your terminal, you've successfully started the DynamoDB Local emulator. You're now ready to get started.\n\nThe $LOCAL variable\n\nIf you want to use the DynamoDB local emulator, you'll need to append the following flag to all commands given in the examples:\n\n--endpoint-url http://localhost:8000\n\n\nI don't like typing the full flag every time so I export it to a variable and use that shorthand:\n\n$ export LOCAL=\"--endpoint-url http://localhost:8000\"\n\n$ aws dynamodb list-tables $LOCAL\n\n\nAll examples will include the $LOCAL flag for easier copy-paste functionality. If you are using a real AWS account rather than the DynamoDB local emulator, make sure the $LOCAL variable is unset in your terminal."
  },
  {
    "title": "The Dynamo Paper | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/the-dynamo-paper/",
    "html": "In 2004, Amazon.com was growing rapidly and was starting to hit the upper scaling limits of its Oracle database. It started to consider building its own database in-house (note to readers: this is almost always a bad idea). Out of this experiment, the engineers created the Amazon Dynamo database which backed major internal infrastructure including the shopping cart on the Amazon.com website.\n\nA group of engineers behind the Amazon Dynamo database published the Dynamo Paper in 2007. It described the learnings from building an in-house, highly available key-value store designed to meet the demanding requirements of the Amazon.com website.\n\nThe paper was highly influential and inspired a number of NoSQL databases, including Apache Cassandra (originally developed at Facebook) and AWS offerings SimpleDB and DynamoDB. In 2012, Amazon Web Services launched DynamoDB, which was a managed database service modeled after the principles behind Dynamo.\n\nWant to know more about how DynamoDB scales? Check out this post on SQL, NoSQL, and Scale: How DynamoDB scales where relational databases don't.\n\nKey aspects of Dynamo\nNo Relational model\n\nThe relational data model is a useful way to model many types of data. Often, relational data is normalized to improve the integrity of the data. Rather than duplicating a particular piece of data in multiple rows, you can store it in one place and refer to it using a JOIN operation from one table to another. Now you can update that single place, and all items that refer to that data will gain the benefits of the update as well.\n\nYet one of the most interesting findings of the Amazon.com engineers while gathering their database requirements was how their engineers were using their relational databases:\n\nAbout 70 percent of operations were of the key-value kind, where only a primary key was used and a single row would be returned. About 20 percent would return a set of rows, but still operate on only a single table.\n\n- Werner Vogels, A Decade of Dynamo\n\nThis is a huge deal -- 90% of operations weren't using the JOIN functionality that is core to a relational database!\n\nThe JOIN operation is expensive. At a large enough scale, engineers often denormalize their data to avoid making expensive joins and slowing down response times. This decrease in response time comes with a trade-off of increased application complexity -- now you need to manage more of your data integrity issues in your code rather than your database.\n\nAmazon.com engineers were already making that trade-off of denormalization to improve response times. The realization that the relational model wasn't needed by Amazon engineers allowed the Dynamo designers to re-evaluate other aspects of a relational database.\n\nAvailability > Consistency\n\nMost relational databases use a strongly consistent model for their data. Briefly, this means all clients of the server will see the same data if querying at the same time.\n\nLet's use Twitter as an example. Imagine that Bob in Virginia tweets a cat picture at 2:30 PM. There are two users that view Bob's profile after he tweets his picture: his neighbor, Cheryl, and his uncle, Jeffrey, who lives in Singapore. If Twitter were using a strongly-consistent model, both Cheryl and Jeffrey should see Bob's most recent tweet as soon as it's committed to the database from Bob's action.\n\nThis might not be ideal, for a few reasons. First, think of the geography involved in this scenario. Twitter could choose to have a single database instance to enable this strong consistency. This database instance may be located in Virginia, close to Bob and Cheryl. This results in fast responses to Bob and Cheryl, but very slow responses to Jeffrey as each request must cross an ocean from Singapore to Virginia to request the data, then return from Virginia to Singapore to return it to Jeffrey. This results in slower read times to some users.\n\nInstead of maintaining a single database instance, perhaps Twitter wants to have two instances that are exact replicas -- one in Virginia and one in Singapore. If we still want to maintain strong consistency, this means a user must get the same answer if she queries the Virginia instance or the Singapore instance at the same time. This could be implemented by a more complex system on database writes -- before Bob's tweet is committed to the database, it has to be submitted to both the Virginia instance and the Singapore instance. Now Bob's request needs to make the hop across the ocean and back. This results in slower write times to some users.\n\nIn the Dynamo paper, Amazon noted that strong consistency isn't important in all scenarios. In our example, it would be fine if Jeffrey and Cheryl saw slightly different versions of my profile even if they queried at the same time. Sometimes you can settle for eventual consistency, meaning different users will eventually see the same view of the data. Jeffrey will eventually see Bob's tweet in Singapore, but it may be at 2:32 PM rather than 2:30.\n\nStrong consistency is important for certain use cases - think bank account balances - but less important for others, such as our Twitter example or the Amazon shopping cart, which was the impetus for Dynamo. For these use cases, speed and availability are more important than a consistent view of the world. By weakening the consistency model of a relational database, the Dynamo engineers were able to provide a database that better fit the needs of Amazon.com.\n\nNote: This section is a massive simplification of consistency, availability, and other concepts around databases and distributed systems. You should really look at this as a very simple primer rather than a definitive text.\n\nInfinitely Scalable\n\nThe final key aspect of Dynamo is that it is infinitely scalable without any negative performance impacts. This aspect is a result of the relaxing of relational and consistency constraints from prior databases.\n\nWhen scaling out a system, you can either vertically scale (use a larger server instance with more CPUs or RAM) or you can horizontally scale by splitting your data across multiple machines, each of which has a subset of your full dataset. Vertical scaling gets expensive and eventually hits limits based on available technology. Horizontal scaling is cheaper but more difficult to achieve.\n\nTo think about horizontal scaling, imagine you have a dataset of Users that you want to distribute across three machines. You could choose to split them across machines based on the last name of the Users -- A through H go on machine 1, I through Q go on machine 2, and R through Z go on machine 3.\n\nThis is nice if you're getting a single User -- a call to retrieve Linda Duffy can go directly to machine 1 -- but can be slow if your query spans multiple machines. A query to get all users older than 18 will have to hit all three machines, resulting in slower responses.\n\nSimilarly, we saw in the previous section how strong consistency requirements can make it difficult to scale out. We would introduce latency during writes to make sure the write is committed to all nodes before returning to the writing user.\n\nRelaxing these requirements makes it much easier for Dynamo to scale horizontally without sacrificing performance. DynamoDB uses consistent hashing to spread items across a number of nodes. As the amount of data in your DynamoDB table increases, AWS can add additional nodes behind the scenes to handle this data.\n\nDynamoDB avoids the multiple-machine problem by essentially requiring that all read operations use the primary key (other than Scans). From our Users example before, our primary key could be LastName, and Amazon would distribute the data accordingly. If you do need to query via Age, you would use a secondary index to apply the same distribution strategy via a different key.\n\nFinally, because DynamoDB allows for eventual consistency, it allows for easier replication strategies of your data. You can have your item copied onto three different machines and query any of them for increased throughput. It's possible one of the machines has a slightly different view of the item at different times due to the eventual consistency model, but this is a trade-off worth accepting for many use cases. Also, you may explicitly specify a strongly-consistent read if it is required for your application.\n\nThese changes make it possible for DynamoDB to provide query latencies in single-digit milliseconds for virtually unlimited amounts of data -- 100TB+.\n\nReady to dig in? Set up your environment then get started with some operations.\n\nReferences\nA Decade of Dynamo post on Werner Vogel's blog\nDynamo: Amazon's Highly Available Key-value Store\nCAP Theorem\nAmazon Takes Another Pass at NoSQL with DynamoDB."
  },
  {
    "title": "Key Concepts | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/key-concepts/",
    "html": "In this section, we'll cover the key concepts you need to know about DynamoDB. At the end of this section, you will understand:\n\ntables, items, and attributes;\nprimary keys;\nsecondary indexes;\nread and write capacity.\nTables, Items, and Attributes\n\nTables, items, and attributes are the core building blocks of DynamoDB.\n\nA table is a grouping of data records. For example, you might have a Users table to store data about your users, and an Orders table to store data about your users' orders. This concept is similar to a table in a relational database or a collection in MongoDB.\n\nAn item is a single data record in a table. Each item in a table is uniquely identified by the stated primary key of the table. In your Users table, an item would be a particular User. An item is similar to a row in a relational database or a document in MongoDB.\n\nAttributes are pieces of data attached to a single item. This could be a simple Age attribute that stores the age of a user. An attribute is comparable to a column in a relational database or a field in MongoDB. DynamoDB does not require attributes on items except for attributes that make up your primary key.\n\nPrimary Key\n\nEach item in a table is uniquely identified by a primary key. The primary key definition must be defined at the creation of the table, and the primary key must be provided when inserting a new item.\n\nThere are two types of primary key: a simple primary key made up of just a partition key, and a composite primary key made up of a partition key and a sort key.\n\nUsing a simple primary key is similar to standard key-value stores like Memcached or accessing rows in a SQL table by a primary key. One example would be a Users table with a Username primary key.\n\nThe composite primary key is more complex. With a composite primary key, you specify both a partition key and a sort key. The sort key is used to (wait for it) sort items with the same partition. One example could be an Orders tables for recording customer orders on an e-commerce site. The partition key would be the CustomerId, and the sort key would be the OrderId.\n\nRemember: each item in a table is uniquely identified by a primary key, even with the composite key. When using a table with a composite primary key, you may have multiple items with the same partition key but different sort keys. You can only have one item with a particular combination of partition key and sort key.\n\nThe composite primary key enables sophisticated query patterns, including grabbing all items with the given partition key or using the sort key to narrow the relevant items for a particular query.\n\nFor more on interacting with items, start with the lesson on the anatomy of an item.\n\nSecondary Indexes\n\nThe primary key uniquely identifies an item in a table, and you may make queries against the table using the primary key. However, sometimes you have additional access patterns that would be inefficient with your primary key. DynamoDB has the notion of secondary indexes to enable these additional access patterns.\n\nThe first kind of secondary index is a local secondary index. A local secondary index uses the same partition key as the underlying table but a different sort key. To take our Order table example from the previous section, imagine you wanted to quickly access a customer's orders in descending order of the amount they spent on the order. You could add a local secondary index with a partition key of CustomerId and a sort key of Amount, allowing for efficient queries on a customer's orders by amount.\n\nThe second kind of secondary index is a global secondary index. A global secondary index can define an entirely different primary key for a table. This could mean setting an index with just a partition key for a table with a composite primary key. It could also mean using completely different attributes to populate a partition key and sort key. With the Order example above, we could have a global secondary index with a partition key of OrderId so we could retrieve a particular order without knowing the CustomerId that placed the order.\n\nSecondary indexes are a complex topic but are extremely useful in getting the most out of DynamoDB. Check out the section on secondary indexes for a deeper dive.\n\nRead and Write Capacity\n\nWhen you use a database like MySQL, Postgres, or MongoDB, you provision a particular server to run your database. You'll need to choose your instance size -- how many CPUs do you need, how much RAM, how many GBs of storage, etc.\n\nNot so with DynamoDB. Instead, you provision read and write capacity units. These units allow a given number of operations per second. This is a fundamentally different pricing paradigm than the instance-based world -- pricing can more closely reflect actual usage.\n\nDynamoDB also has autoscaling of your read and write capacity units. This makes it much easier to scale your application up during peak times while saving money by scaling down when your users are asleep.\n\nNext steps\n\nIf you really want the nitty-gritty fundamentals of DynamoDB, go to the section on the Dynamo Paper. Otherwise, get your environment set up and then start the walkthrough with single-item actions."
  },
  {
    "title": "What is DynamoDB? | DynamoDB, explained.",
    "url": "https://www.dynamodbguide.com/what-is-dynamo-db/",
    "html": "DynamoDB is a hosted NoSQL database offered by Amazon Web Services (AWS). It offers:\n\nreliable performance even as it scales;\na managed experience, so you won't be SSH-ing into servers to upgrade the crypto libraries;\na small, simple API allowing for simple key-value access as well as more advanced query patterns.\n\nDynamoDB is a particularly good fit for the following use cases:\n\nApplications with large amounts of data and strict latency requirements. As your amount of data scales, JOINs and advanced SQL operations can slow down your queries. With DynamoDB, your queries have predictable latency up to any size, including over 100 TBs!\n\nServerless applications using AWS Lambda. AWS Lambda provides auto-scaling, stateless, ephemeral compute in response to event triggers. DynamoDB is accessible via an HTTP API and performs authentication & authorization via IAM roles, making it a perfect fit for building Serverless applications.\n\nData sets with simple, known access patterns. If you're generating recommendations and serving them to users, DynamoDB's simple key-value access patterns make it a fast, reliable choice.\n\nReady to learn more?\n\nStart with the key concepts to learn about tables, items, and other basic elements of DynamoDB. If you want the computer science background on DynamoDB, check out the section on the Dynamo Paper.\n\nIf you want to get your hands dirty, set up your environment then start with the section on working with single items. Then you can move on to working with multiple items using Queries and Scans.\n\nWant the advanced stuff? Power up your tables with secondary indexes and DynamoDB Streams.\n\nStill want more? Head to Additional Reading to find the best community resources on DynamoDB."
  }
]